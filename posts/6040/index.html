<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>动手学强化学习 | 马克图布</title><meta name="author" content="马克图布"><meta name="copyright" content="马克图布"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="强化学习简介强化学习里面一直以来就是value based和policy based两路方法，它们各有优劣。 Value based 方法强调让机器知道什么state或者state-action pair是好的，什么是坏的。例如Q-learning训练的优化目标是最小化一个TD error： 这个优化目标很清晰，就是让当前Q函数估计更准。但是这个优化目标并不对应任何策略的目标。 我们强化学习的总目">
<meta property="og:type" content="article">
<meta property="og:title" content="动手学强化学习">
<meta property="og:url" content="https://mr-maktoub.github.io/posts/6040/index.html">
<meta property="og:site_name" content="马克图布">
<meta property="og:description" content="强化学习简介强化学习里面一直以来就是value based和policy based两路方法，它们各有优劣。 Value based 方法强调让机器知道什么state或者state-action pair是好的，什么是坏的。例如Q-learning训练的优化目标是最小化一个TD error： 这个优化目标很清晰，就是让当前Q函数估计更准。但是这个优化目标并不对应任何策略的目标。 我们强化学习的总目">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://mr-maktoub.github.io/img/avatar1.png">
<meta property="article:published_time" content="2023-01-01T14:00:22.000Z">
<meta property="article:modified_time" content="2023-05-02T13:23:25.225Z">
<meta property="article:author" content="马克图布">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://mr-maktoub.github.io/img/avatar1.png"><link rel="shortcut icon" href="/img/log3.jfif"><link rel="canonical" href="https://mr-maktoub.github.io/posts/6040/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '动手学强化学习',
  isPost: true,
  isHome: false,
  isHighlightShrink: true,
  isToc: true,
  postUpdate: '2023-05-02 21:23:25'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"><style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/avatar1.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">66</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">22</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">13</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw iconfont icon-shouyex"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw iconfont icon-zuixinwenzhang_huaban"></i><span> 找文章</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw iconfont icon-fenlei1"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw iconfont icon-biaoqian1"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw iconfont icon-shijianzhou"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw iconfont icon-shenghuo"></i><span> 生活</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/messageboard/"><i class="fa-fw iconfont icon-liaotian-04"></i><span> 留言板</span></a></li><li><a class="site-page child" href="/link/"><i class="fa-fw iconfont icon-lianjie"></i><span> 友人帐</span></a></li><li><a class="site-page child" href="/share/"><i class="fa-fw iconfont icon-fenxiang"></i><span> 分享</span></a></li><li><a class="site-page child" href="/gallery/"><i class="fa-fw iconfont icon-xiangce"></i><span> 相册</span></a></li><li><a class="site-page child" href="/books/"><i class="fa-fw iconfont icon-shuji"></i><span> 书籍</span></a></li><li><a class="site-page child" href="/FilmAndTV/"><i class="fa-fw iconfont icon-yingshi1"></i><span> 影视</span></a></li><li><a class="site-page child" href="/daohang/"><i class="fa-fw iconfont icon-daohang"></i><span> 导航站</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw iconfont icon-gerenzhongxin"></i><span> 关于笔者</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/img/cover.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="马克图布"><img class="site-icon" src="/img/log3.jfif"/><span class="site-name">马克图布</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw iconfont icon-shouyex"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw iconfont icon-zuixinwenzhang_huaban"></i><span> 找文章</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw iconfont icon-fenlei1"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw iconfont icon-biaoqian1"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw iconfont icon-shijianzhou"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw iconfont icon-shenghuo"></i><span> 生活</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/messageboard/"><i class="fa-fw iconfont icon-liaotian-04"></i><span> 留言板</span></a></li><li><a class="site-page child" href="/link/"><i class="fa-fw iconfont icon-lianjie"></i><span> 友人帐</span></a></li><li><a class="site-page child" href="/share/"><i class="fa-fw iconfont icon-fenxiang"></i><span> 分享</span></a></li><li><a class="site-page child" href="/gallery/"><i class="fa-fw iconfont icon-xiangce"></i><span> 相册</span></a></li><li><a class="site-page child" href="/books/"><i class="fa-fw iconfont icon-shuji"></i><span> 书籍</span></a></li><li><a class="site-page child" href="/FilmAndTV/"><i class="fa-fw iconfont icon-yingshi1"></i><span> 影视</span></a></li><li><a class="site-page child" href="/daohang/"><i class="fa-fw iconfont icon-daohang"></i><span> 导航站</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw iconfont icon-gerenzhongxin"></i><span> 关于笔者</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">动手学强化学习</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-01-01T14:00:22.000Z" title="发表于 2023-01-01 22:00:22">2023-01-01</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-05-02T13:23:25.225Z" title="更新于 2023-05-02 21:23:25">2023-05-02</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">强化学习</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="动手学强化学习"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h2 id="强化学习简介"><a href="#强化学习简介" class="headerlink" title="强化学习简介"></a>强化学习简介</h2><p>强化学习里面一直以来就是value based和policy based两路方法，它们各有优劣。</p>
<p>Value based 方法强调让机器知道什么state或者state-action pair是好的，什么是坏的。例如Q-learning训练的优化目标是最小化一个TD error：</p>
<p>这个优化目标很清晰，就是让当前Q函数估计更准。但是这个优化目标并不对应任何策略的目标。</p>
<p>我们强化学习的总目标是给出一个policy，使之能在环境里面很好的完成序列决策任务。</p>
<p>Policy based 方法则正好直接朝着这么目标去优化策略的参数：</p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301040002870.png" alt="image-20230104000205843"></p>
<p>所以Berkeley和OpenAI的人（PPO和TRPO的作者）一般喜欢强调policy based 方法更加直接在优化最终的目标。</p>
<p>当然，我们也要知道value based 方法其实往往更方便学习，毕竟其优化目标就是一个TD error，相比policy based方法的目标要容易优化得多。</p>
<p>所以如果我们希望算法能尽快达到一个比较好的效果，可以直接用value based 方法。而如果有足够的时间和算力去训练，那么推荐使用 policy based 方法。</p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301032315632.png" alt="image-20230103231516559"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301032321541.png" alt="image-20230103232141470"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301032325880.png" alt="image-20230103232516797"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301032327286.png" alt="image-20230103232710218"></p>
<p>随机策略输出是条件概率分布</p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301032336777.png" alt="image-20230103233614708"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301032341993.png" alt="image-20230103234113921"></p>
<p>环境最重要得两个部分：状态转移概率，奖励函数</p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301032347810.png" alt="image-20230103234716734"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301032349567.png" alt="image-20230103234904499"></p>
<p>如下行走策略<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="1.29ex" height="1ex" role="img" focusable="false" viewBox="0 -431 570 442"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D70B" d="M132 -11Q98 -11 98 22V33L111 61Q186 219 220 334L228 358H196Q158 358 142 355T103 336Q92 329 81 318T62 297T53 285Q51 284 38 284Q19 284 19 294Q19 300 38 329T93 391T164 429Q171 431 389 431Q549 431 553 430Q573 423 573 402Q573 371 541 360Q535 358 472 358H408L405 341Q393 269 393 222Q393 170 402 129T421 65T431 37Q431 20 417 5T381 -10Q370 -10 363 -7T347 17T331 77Q330 86 330 121Q330 170 339 226T357 318T367 358H269L268 354Q268 351 249 275T206 114T175 17Q164 -11 132 -11Z"></path></g></g></g></svg></mjx-container></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301032349534.png" alt="image-20230103234918465"></p>
<p>对于上面给出的<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="1.29ex" height="1ex" role="img" focusable="false" viewBox="0 -431 570 442"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D70B" d="M132 -11Q98 -11 98 22V33L111 61Q186 219 220 334L228 358H196Q158 358 142 355T103 336Q92 329 81 318T62 297T53 285Q51 284 38 284Q19 284 19 294Q19 300 38 329T93 391T164 429Q171 431 389 431Q549 431 553 430Q573 423 573 402Q573 371 541 360Q535 358 472 358H408L405 341Q393 269 393 222Q393 170 402 129T421 65T431 37Q431 20 417 5T381 -10Q370 -10 363 -7T347 17T331 77Q330 86 330 121Q330 170 339 226T357 318T367 358H269L268 354Q268 351 249 275T206 114T175 17Q164 -11 132 -11Z"></path></g></g></g></svg></mjx-container>的状态价值函数如下</p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301032349551.png" alt="image-20230103234944473"></p>
<p>强化学习可分为基于model-base或者model-free的强化学习</p>
<p>model-base：知道环境信息，可以自己模拟</p>
<p>model-free：不知道环境信息，需要大量采样</p>
<p>Model-based RL最终效果还是会受到环境model本身精度不够的影响，导致最终效果往往不如model-free RL。</p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301032355659.png" alt="image-20230103235510598"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301032356206.png" alt="image-20230103235642128"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301032359779.png" alt="image-20230103235922705"></p>
<h2 id="探索与利用"><a href="#探索与利用" class="headerlink" title="探索与利用"></a>探索与利用</h2><p>Epsilon greedy在RL里面是做探索的方法。其实RL领域也有不少其他的做探索的方法，只是专门在做探索，所以就不仅仅是UCB或者TS这种MAB的经典方法了。这个你搜索RL exploration methods就有不少工作。</p>
<p>例如，在树结构博弈环境里面UCB叫UCT (Upper Confidence bounds applied to Trees)。这方面很有名。David Silver的论文还拿了ICML的10年最佳论文奖：</p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301041349733.jpeg" alt="img"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301041041946.png" alt="image-20230104104147805"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301041049700.png" alt="image-20230104104933586"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301041053433.png" alt="image-20230104105322310"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301041056949.png" alt="image-20230104105657814"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301041059742.png" alt="image-20230104105931625"><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301041100701.png" alt="image-20230104110053600"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301041104163.png" alt="image-20230104110416048"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301041112051.png" alt="image-20230104111211907"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301041114673.png" alt="image-20230104111400561"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301041115004.png" alt="image-20230104111543909"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301041117013.png" alt="image-20230104111701915"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301041118334.png" alt="image-20230104111821206"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301041120836.png" alt="image-20230104112028715"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301041148818.png" alt="image-20230104114815700"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301041150269.png" alt="image-20230104115043142"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301041154799.png" alt="image-20230104115412662"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301041157600.png" alt="image-20230104115707475"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301041158232.png" alt="image-20230104115803096"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301041205261.png" alt="image-20230104120523146"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301041208972.png" alt="image-20230104120823873"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301041344120.png" alt="image-20230104134424027"></p>
<h2 id="马尔科夫决策过程"><a href="#马尔科夫决策过程" class="headerlink" title="马尔科夫决策过程"></a>马尔科夫决策过程</h2><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301102340804.png" alt="image-20230110234040676"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301102341613.png" alt="image-20230110234139543"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301102346219.png" alt="image-20230110234619149"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301102348920.png" alt="image-20230110234829841"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301102350117.png" alt="image-20230110235006050"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301102352978.png" alt="image-20230110235222914"></p>
<h2 id="基于动态规划的强化学习"><a href="#基于动态规划的强化学习" class="headerlink" title="基于动态规划的强化学习"></a>基于动态规划的强化学习</h2><p>更新价值函数很耗时，对于空间较小的MDP，策略迭代通常更快收敛；而对于空间较大的价值迭代效率更高。</p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301121221549.jpeg" alt="img"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301121056397.png" alt="image-20230112105637325"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301121059032.png" alt="image-20230112105905971"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301121102540.png" alt="image-20230112110217474"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301121130616.png" alt="image-20230112113046551"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301121132358.png" alt="image-20230112113225284"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301121135609.png" alt="image-20230112113529531"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301121137212.png" alt="image-20230112113755143"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301121139119.png" alt="image-20230112113925041"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301121144690.png" alt="image-20230112114448618"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301121152307.png" alt="image-20230112115231216"></p>
<h2 id="蒙特卡洛方法"><a href="#蒙特卡洛方法" class="headerlink" title="蒙特卡洛方法"></a>蒙特卡洛方法</h2><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303012036999.png" alt="image-20230301203558857"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303012038695.png" alt="image-20230301203841613"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303012040496.png" alt="image-20230301204026411"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303012041831.png" alt="image-20230301204121753"></p>
<h2 id="蒙特卡洛价值预测"><a href="#蒙特卡洛价值预测" class="headerlink" title="蒙特卡洛价值预测"></a>蒙特卡洛价值预测</h2><p>蒙特卡洛策略评估使用经验平均累计奖励而不是期望累计奖励</p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303012049355.png" alt="image-20230301204929269"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303012057947.png" alt="image-20230301205707869"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303012059328.png" alt="image-20230301205943246"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303012102508.png" alt="image-20230301210234424"></p>
<h2 id="规划与学习"><a href="#规划与学习" class="headerlink" title="规划与学习"></a>规划与学习</h2><h3 id="入门"><a href="#入门" class="headerlink" title="入门"></a>入门</h3><p>Dyna-Q中规划会受到模型偏差的影响，经验需求量比较小，规划可以使得策略能够收敛的更快, Dyna-Q+可以在环境改变时能够快速反应得到更好的策略</p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303041105835.png" alt="image-20230304110550708"></p>
<p>环境本身去建模，称为环境模型。</p>
<p>分布模型可以看做白盒，样本模型可以视作为黑盒</p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303041108284.png" alt="image-20230304110818215"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303041110474.png" alt="image-20230304111058402"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303041116930.png" alt="image-20230304111646848"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303041119648.png" alt="image-20230304111936568"></p>
<p>只有<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex;" xmlns="http://www.w3.org/2000/svg" width="4.162ex" height="2.059ex" role="img" focusable="false" viewBox="0 -716 1839.7 910"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D446" d="M308 24Q367 24 416 76T466 197Q466 260 414 284Q308 311 278 321T236 341Q176 383 176 462Q176 523 208 573T273 648Q302 673 343 688T407 704H418H425Q521 704 564 640Q565 640 577 653T603 682T623 704Q624 704 627 704T632 705Q645 705 645 698T617 577T585 459T569 456Q549 456 549 465Q549 471 550 475Q550 478 551 494T553 520Q553 554 544 579T526 616T501 641Q465 662 419 662Q362 662 313 616T263 510Q263 480 278 458T319 427Q323 425 389 408T456 390Q490 379 522 342T554 242Q554 216 546 186Q541 164 528 137T492 78T426 18T332 -20Q320 -22 298 -22Q199 -22 144 33L134 44L106 13Q83 -14 78 -18T65 -22Q52 -22 52 -14Q52 -11 110 221Q112 227 130 227H143Q149 221 149 216Q149 214 148 207T144 186T142 153Q144 114 160 87T203 47T255 29T308 24Z"></path></g><g data-mml-node="mo" transform="translate(645,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(1089.7,0)"><path data-c="1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path></g></g></g></svg></mjx-container>是真实的，<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex;" xmlns="http://www.w3.org/2000/svg" width="4.927ex" height="2.156ex" role="img" focusable="false" viewBox="0 -759 2177.7 953"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D445" d="M230 637Q203 637 198 638T193 649Q193 676 204 682Q206 683 378 683Q550 682 564 680Q620 672 658 652T712 606T733 563T739 529Q739 484 710 445T643 385T576 351T538 338L545 333Q612 295 612 223Q612 212 607 162T602 80V71Q602 53 603 43T614 25T640 16Q668 16 686 38T712 85Q717 99 720 102T735 105Q755 105 755 93Q755 75 731 36Q693 -21 641 -21H632Q571 -21 531 4T487 82Q487 109 502 166T517 239Q517 290 474 313Q459 320 449 321T378 323H309L277 193Q244 61 244 59Q244 55 245 54T252 50T269 48T302 46H333Q339 38 339 37T336 19Q332 6 326 0H311Q275 2 180 2Q146 2 117 2T71 2T50 1Q33 1 33 10Q33 12 36 24Q41 43 46 45Q50 46 61 46H67Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628Q287 635 230 637ZM630 554Q630 586 609 608T523 636Q521 636 500 636T462 637H440Q393 637 386 627Q385 624 352 494T319 361Q319 360 388 360Q466 361 492 367Q556 377 592 426Q608 449 619 486T630 554Z"></path></g><g data-mml-node="mo" transform="translate(759,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msup" transform="translate(1203.7,0)"><g data-mml-node="mi"><path data-c="1D446" d="M308 24Q367 24 416 76T466 197Q466 260 414 284Q308 311 278 321T236 341Q176 383 176 462Q176 523 208 573T273 648Q302 673 343 688T407 704H418H425Q521 704 564 640Q565 640 577 653T603 682T623 704Q624 704 627 704T632 705Q645 705 645 698T617 577T585 459T569 456Q549 456 549 465Q549 471 550 475Q550 478 551 494T553 520Q553 554 544 579T526 616T501 641Q465 662 419 662Q362 662 313 616T263 510Q263 480 278 458T319 427Q323 425 389 408T456 390Q490 379 522 342T554 242Q554 216 546 186Q541 164 528 137T492 78T426 18T332 -20Q320 -22 298 -22Q199 -22 144 33L134 44L106 13Q83 -14 78 -18T65 -22Q52 -22 52 -14Q52 -11 110 221Q112 227 130 227H143Q149 221 149 216Q149 214 148 207T144 186T142 153Q144 114 160 87T203 47T255 29T308 24Z"></path></g><g data-mml-node="mo" transform="translate(729.6,363) scale(0.707)"><path data-c="2032" d="M79 43Q73 43 52 49T30 61Q30 68 85 293T146 528Q161 560 198 560Q218 560 240 545T262 501Q262 496 260 486Q259 479 173 263T84 45T79 43Z"></path></g></g></g></g></svg></mjx-container>是模型给出的，被称为模拟经验。然后进行Q-planning更新<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="7.712ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 3408.7 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D444" d="M399 -80Q399 -47 400 -30T402 -11V-7L387 -11Q341 -22 303 -22Q208 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435Q740 255 592 107Q529 47 461 16L444 8V3Q444 2 449 -24T470 -66T516 -82Q551 -82 583 -60T625 -3Q631 11 638 11Q647 11 649 2Q649 -6 639 -34T611 -100T557 -165T481 -194Q399 -194 399 -87V-80ZM636 468Q636 523 621 564T580 625T530 655T477 665Q429 665 379 640Q277 591 215 464T153 216Q153 110 207 59Q231 38 236 38V46Q236 86 269 120T347 155Q372 155 390 144T417 114T429 82T435 55L448 64Q512 108 557 185T619 334T636 468ZM314 18Q362 18 404 39L403 49Q399 104 366 115Q354 117 347 117Q344 117 341 117T337 118Q317 118 296 98T274 52Q274 18 314 18Z"></path></g><g data-mml-node="mo" transform="translate(791,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1180,0)"><path data-c="1D446" d="M308 24Q367 24 416 76T466 197Q466 260 414 284Q308 311 278 321T236 341Q176 383 176 462Q176 523 208 573T273 648Q302 673 343 688T407 704H418H425Q521 704 564 640Q565 640 577 653T603 682T623 704Q624 704 627 704T632 705Q645 705 645 698T617 577T585 459T569 456Q549 456 549 465Q549 471 550 475Q550 478 551 494T553 520Q553 554 544 579T526 616T501 641Q465 662 419 662Q362 662 313 616T263 510Q263 480 278 458T319 427Q323 425 389 408T456 390Q490 379 522 342T554 242Q554 216 546 186Q541 164 528 137T492 78T426 18T332 -20Q320 -22 298 -22Q199 -22 144 33L134 44L106 13Q83 -14 78 -18T65 -22Q52 -22 52 -14Q52 -11 110 221Q112 227 130 227H143Q149 221 149 216Q149 214 148 207T144 186T142 153Q144 114 160 87T203 47T255 29T308 24Z"></path></g><g data-mml-node="mo" transform="translate(1825,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(2269.7,0)"><path data-c="1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path></g><g data-mml-node="mo" transform="translate(3019.7,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303041120537.png" alt="image-20230304112006456"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303041126053.png" alt="image-20230304112618984"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303041126612.png" alt="image-20230304112628543"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303041130961.png" alt="image-20230304113035871"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303041135556.png" alt="image-20230304113518460"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303041138702.png" alt="image-20230304113825621"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303041139561.png" alt="image-20230304113945484"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303052313121.png" alt="image-20230305231258009"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303052324969.png" alt="image-20230305232419885"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303052326868.png" alt="image-20230305232643794"><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303052327937.png" alt="image-20230305232707876"></p>
<h3 id="采样方法"><a href="#采样方法" class="headerlink" title="采样方法"></a>采样方法</h3><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303081747318.jpeg" alt="img"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303081716623.png" alt="image-20230308171629511"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303081719339.png" alt="image-20230308171940261"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303081729824.png" alt="image-20230308172909721"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303081730371.png" alt="image-20230308173016281"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303081739448.png" alt="image-20230308173939343"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303081742411.png" alt="image-20230308174227334"></p>
<p>此处策略变为随机</p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303081747753.png" alt="image-20230308174727646"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303081750721.png" alt="image-20230308175018618"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303081751212.png" alt="image-20230308175107127"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303081752943.png" alt="image-20230308175231867"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303081753617.png" alt="image-20230308175338531"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303081757362.png" alt="image-20230308175707276"></p>
<h2 id="模型无关控制方法"><a href="#模型无关控制方法" class="headerlink" title="模型无关控制方法"></a>模型无关控制方法</h2><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303012211824.jpeg" alt="img"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303012135403.png" alt="image-20230301213537315"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303012136419.png" alt="image-20230301213616350"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303012141403.png" alt="image-20230301214151333"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303012143818.png" alt="image-20230301214328756"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303012149178.png" alt="image-20230301214955106"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303012153732.png" alt="image-20230301215359652"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303012155350.png" alt="image-20230301215527271"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303012159576.png" alt="image-20230301215932498"></p>
<p><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="0.919ex" height="1ex" role="img" focusable="false" viewBox="0 -431 406 442"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D716" d="M227 -11Q149 -11 95 41T40 174Q40 262 87 322Q121 367 173 396T287 430Q289 431 329 431H367Q382 426 382 411Q382 385 341 385H325H312Q191 385 154 277L150 265H327Q340 256 340 246Q340 228 320 219H138V217Q128 187 128 143Q128 77 160 52T231 26Q258 26 284 36T326 57T343 68Q350 68 354 58T358 39Q358 36 357 35Q354 31 337 21T289 0T227 -11Z"></path></g></g></g></svg></mjx-container>-贪心策略探索中，以<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="0.919ex" height="1ex" role="img" focusable="false" viewBox="0 -431 406 442"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D716" d="M227 -11Q149 -11 95 41T40 174Q40 262 87 322Q121 367 173 396T287 430Q289 431 329 431H367Q382 426 382 411Q382 385 341 385H325H312Q191 385 154 277L150 265H327Q340 256 340 246Q340 228 320 219H138V217Q128 187 128 143Q128 77 160 52T231 26Q258 26 284 36T326 57T343 68Q350 68 354 58T358 39Q358 36 357 35Q354 31 337 21T289 0T227 -11Z"></path></g></g></g></svg></mjx-container>概率随机选择一个动作，而以<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.186ex;" xmlns="http://www.w3.org/2000/svg" width="4.815ex" height="1.692ex" role="img" focusable="false" viewBox="0 -666 2128.4 748"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mn"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mo" transform="translate(722.2,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mi" transform="translate(1722.4,0)"><path data-c="1D716" d="M227 -11Q149 -11 95 41T40 174Q40 262 87 322Q121 367 173 396T287 430Q289 431 329 431H367Q382 426 382 411Q382 385 341 385H325H312Q191 385 154 277L150 265H327Q340 256 340 246Q340 228 320 219H138V217Q128 187 128 143Q128 77 160 52T231 26Q258 26 284 36T326 57T343 68Q350 68 354 58T358 39Q358 36 357 35Q354 31 337 21T289 0T227 -11Z"></path></g></g></g></svg></mjx-container>的概率选择贪心动作</p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303012200380.png" alt="image-20230301220035290"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303012202923.png" alt="image-20230301220253855"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303012205973.png" alt="image-20230301220517903"><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303012207471.png" alt="image-20230301220726404"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303012210250.png" alt="image-20230301221009177"></p>
<h2 id="策略梯度（Policy-Gradient）"><a href="#策略梯度（Policy-Gradient）" class="headerlink" title="策略梯度（Policy Gradient）"></a>策略梯度（Policy Gradient）</h2><p>很好的<a target="_blank" rel="noopener" href="https://www.jianshu.com/p/7853fb0fbab9">参考文献</a></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301092113150.jpeg" alt="img"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301091927054.png" alt="image-20230109192733983"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301091931492.png" alt="image-20230109193145420"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301091937429.png" alt="image-20230109193723361"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301091941347.png" alt="image-20230109194148272"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301091945816.png" alt="image-20230109194537747"></p>
<p>小trick</p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301091948215.png" alt="image-20230109194824127"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301092006624.png" alt="image-20230109200651544"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301092033470.png" alt="image-20230109203329402"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301092035694.png" alt="image-20230109203519618"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301092049596.png" alt="image-20230109204910523"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301092051203.png" alt="image-20230109205133139"></p>
<h2 id="深度Q网络"><a href="#深度Q网络" class="headerlink" title="深度Q网络"></a>深度Q网络</h2><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202302152226096.jpeg" alt="img"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202302152008607.png" alt="image-20230215200815467"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202302152008030.png" alt="image-20230215200850937"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202302152015096.png" alt="image-20230215201552016"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202302152018215.png" alt="image-20230215201842132"></p>
<p>原本的DQN中是根据均匀分布采样数据，而优先经验回放机制根据p_t的大小来采样，就会带来data bias的问题；于是引入重要性采样就是先以每个样本p_t的大小确定选择样本的概率，再在更新Q的学习率上把采样到这个样本的概率除回去，这样就能把采样到数据的分布修改为原来的均匀分布。</p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202302152023373.png" alt="image-20230215202330291"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202302152027605.png" alt="image-20230215202738507"></p>
<p>双网络结构指的是使用评估网络和目标网络，其目的是缓解拟合的Q函数的频繁更新。</p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202302152029315.png" alt="image-20230215202921234"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202302152035149.png" alt="image-20230215203515034"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202302152035571.png" alt="image-20230215203544502"></p>
<p>过估计问题是由Target值计算中的max⁡max操作导致的。</p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202302152038945.png" alt="image-20230215203840859"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202302152208384.png" alt="image-20230215220815293"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202302152210888.png" alt="image-20230215221052784"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202302152227781.png" alt="image-20230215222745688"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202302152213768.png" alt="image-20230215221351663">、<img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202302152228526.png" alt="image-20230215222806426"></p>
<p>dueling dqn能够通过advantage函数把方向感建模出来，这句话我不是非常理解。在我看来，单纯的Q(s,a)也是能够建模出方向感的，因为不同action对应的q函数的差值和advantage函数的差值应该是一样的？</p>
<p>首先，当reward都是正数的时候，Q(s,a)也肯定只会是正数，也就是都是正向的。</p>
<p>Advantage函数重点在建模 Q(s,a) 相对于 V(s) （或者其他baseline function）的residual，它的学习目标就是那个residual。当residual比较小而V Q值比较大的时候，直接学Q(s,a)就不容易捕捉到这些微小的residual信号从而使得Q网络估计不精确。而Q(s,a) = V(s) + A(s,a)，所以argmax_a Q(s,a) 其实就是 argmax_a A(s,a)，所以对residual的学习至关重要。</p>
<p>对Q的建模不如A有方向感，这其实更加体现在policy gradient（actor critic）上面。在上述reward总是正数的情况下，Q(s,a)都是正数，所以policy gradient时，每个action其实都是加分的，最后被softmax给normalize一下。这时就好比action们在赛跑，谁跑的更快，谁接下来的pi(a|s)概率就越大。而对A(s,a)建模，即使在上述reward总是正数的情况下，也各有正负，这时A(s,a)为负数的就对应着pi(a|s)的概率下降，为正数的对应着概率上升，就很合理，对学习算法很友好。</p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202302152215112.png" alt="image-20230215221505035"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202302152217594.png" alt="image-20230215221709513"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202302152218464.png" alt="image-20230215221819381"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202302152228902.png" alt="image-20230215222827736"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202302152223660.png" alt="image-20230215222309556"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202302152224699.png" alt="image-20230215222415598"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202302152225686.png" alt="image-20230215222501585"></p>
<h2 id="Actor-Critic方法"><a href="#Actor-Critic方法" class="headerlink" title="Actor-Critic方法"></a>Actor-Critic方法</h2><p>在PG算法中，计算Q(s,a)可以用REINFORE算法，也可以用AC算法</p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301092147834.jpeg" alt="img"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301092117016.png" alt="image-20230109211732946"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301092128940.png" alt="image-20230109212859824"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301092133149.png" alt="image-20230109213325082"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301092136848.png" alt="image-20230109213605773"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301092141874.png" alt="image-20230109214119827"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202302271236431.png" alt="image-20230227123629349"></p>
<h2 id="重要性采样"><a href="#重要性采样" class="headerlink" title="重要性采样"></a>重要性采样</h2><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301031020762.jpeg" alt="img"></p>
<p>TRPO（PPO同理）的objective给出过程中也用到了importance sampling的思想，可以参考TRPO论文。但它们的确是on-policy算法。</p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301031035361.png" alt="image-20230103103549296"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301031042929.png" alt="image-20230103104250852"></p>
<p>无偏估计，但是方差可能会出现很大的情况</p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301031048067.png" alt="image-20230103104849984"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301031053310.png" alt="image-20230103105350225"></p>
<h2 id="时序差分学习"><a href="#时序差分学习" class="headerlink" title="时序差分学习"></a>时序差分学习</h2><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303021914731.jpeg" alt="img"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303021712508.png" alt="image-20230302171233380"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303021721469.png" alt="image-20230302172128389"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303021726498.png" alt="image-20230302172658391"></p>
<p> <img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303021731514.png" alt="image-20230302173111441"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303021731364.png" alt="image-20230302173127286"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303021906247.png" alt="image-20230302190657161"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303021911122.png" alt="image-20230302191107033"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303021917769.png" alt="image-20230302191705700"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303021917626.png" alt="image-20230302191727553"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303021919773.png" alt="image-20230302191937697"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303021921138.png" alt="image-20230302192105064"></p>
<h2 id="A3C"><a href="#A3C" class="headerlink" title="A3C"></a>A3C</h2><p>A3C的创新和厉害之处在于：A3C算法为了提升训练速度采用异步训练的思想，利用多个线程。每个线程相当于一个智能体在随机探索，多个智能体共同探索，并行计算策略梯度，对参数进行更新。或者说同时启动多个训练环境，同时进行采样，并直接使用采集的样本进行训练，这里的异步得到数据，相比DQN算法，A3C算法不需要使用经验池来存储历史样本并随机抽取训练来打乱数据相关性，节约了存储空间，并且采用异步训练，大大加倍了数据的采样速度，也因此提升了训练速度。与此同时，采用多个不同训练环境采集样本，样本的分布更加均匀，更有利于神经网络的训练。</p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202302271249000.jpeg" alt="img"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202302271222633.png" alt="image-20230227122204454"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202302271226677.png" alt="image-20230227122620599"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202302271231394.png" alt="image-20230227123127314"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202302271235650.png" alt="image-20230227123556564"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202302271245372.png" alt="image-20230227124513286"></p>
<h2 id="确定性策略梯度-DPG"><a href="#确定性策略梯度-DPG" class="headerlink" title="确定性策略梯度(DPG)"></a>确定性策略梯度(DPG)</h2><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303191957319.png" alt="image-20230319195737186"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303192000896.png" alt="image-20230319200011830"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303192006800.png" alt="image-20230319200649728"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303192014446.png" alt="image-20230319201443349"></p>
<h2 id="深度确定性策略梯度-DDPG"><a href="#深度确定性策略梯度-DDPG" class="headerlink" title="深度确定性策略梯度(DDPG)"></a>深度确定性策略梯度(DDPG)</h2><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303192016411.png" alt="image-20230319201604326"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303192022347.png" alt="image-20230319202237251"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303192030988.png" alt="image-20230319203008874"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303192030832.png" alt="image-20230319203048753"></p>
<h2 id="信任区域策略优化TRPO"><a href="#信任区域策略优化TRPO" class="headerlink" title="信任区域策略优化TRPO"></a>信任区域策略优化TRPO</h2><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301012201212.jpeg" alt="img"></p>
<p>TRPO（以及PPO）算法都是on-policy的算法。On-policy是指，采样使用的policy就是要更新的policy。在TRPO中，用来更新policy的样本都是由当前policy生成的，更新完就丢弃了，所以是on policy的。TRPO的思想是通过局部优化一个近似<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="5.846ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 2584 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mtext" fill="red" stroke="red"><path data-c="5C" d="M56 731Q56 740 62 745T75 750Q85 750 92 740Q96 733 270 255T444 -231Q444 -239 438 -244T424 -250Q414 -250 407 -240Q404 -236 230 242T56 731Z"></path><path data-c="52" d="M130 622Q123 629 119 631T103 634T60 637H27V683H202H236H300Q376 683 417 677T500 648Q595 600 609 517Q610 512 610 501Q610 468 594 439T556 392T511 361T472 343L456 338Q459 335 467 332Q497 316 516 298T545 254T559 211T568 155T578 94Q588 46 602 31T640 16H645Q660 16 674 32T692 87Q692 98 696 101T712 105T728 103T732 90Q732 59 716 27T672 -16Q656 -22 630 -22Q481 -16 458 90Q456 101 456 163T449 246Q430 304 373 320L363 322L297 323H231V192L232 61Q238 51 249 49T301 46H334V0H323Q302 3 181 3Q59 3 38 0H27V46H60Q102 47 111 49T130 61V622ZM491 499V509Q491 527 490 539T481 570T462 601T424 623T362 636Q360 636 340 636T304 637H283Q238 637 234 628Q231 624 231 492V360H289Q390 360 434 378T489 456Q491 467 491 499Z" transform="translate(500,0)"></path></g><g data-mml-node="mo" transform="translate(1236,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1625,0)"><path data-c="1D70B" d="M132 -11Q98 -11 98 22V33L111 61Q186 219 220 334L228 358H196Q158 358 142 355T103 336Q92 329 81 318T62 297T53 285Q51 284 38 284Q19 284 19 294Q19 300 38 329T93 391T164 429Q171 431 389 431Q549 431 553 430Q573 423 573 402Q573 371 541 360Q535 358 472 358H408L405 341Q393 269 393 222Q393 170 402 129T421 65T431 37Q431 20 417 5T381 -10Q370 -10 363 -7T347 17T331 77Q330 86 330 121Q330 170 339 226T357 318T367 358H269L268 354Q268 351 249 275T206 114T175 17Q164 -11 132 -11Z"></path></g><g data-mml-node="mo" transform="translate(2195,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container>的下界函数，从而保证每次策略的改进并最终得到最优策略。</p>
<p>策略梯度算法回顾</p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301012202777.png" alt="image-20230101220237693"></p>
<p>策略梯度算法的缺点</p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301012224178.png" alt="image-20230101222404108"></p>
<p>TRPO</p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301012233141.png" alt="image-20230101223314061"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301012242620.png" alt="image-20230101224221536"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301012247024.png" alt="image-20230101224722953"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301012300304.png" alt="image-20230101230021213"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301012309931.png" alt="image-20230101230946859"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301012315983.png" alt="image-20230101231516833"></p>
<p>通过不断改进<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="14.034ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 6202.9 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g><g data-mml-node="mo" transform="translate(681,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1070,0)"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"></path></g><g data-mml-node="mo" transform="translate(1539,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(2150.2,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mi" transform="translate(3150.4,0)"><path data-c="1D436" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z"></path></g><g data-mml-node="mo" transform="translate(4132.7,0)"><path data-c="22C5" d="M78 250Q78 274 95 292T138 310Q162 310 180 294T199 251Q199 226 182 208T139 190T96 207T78 250Z"></path></g><g data-mml-node="mi" transform="translate(4632.9,0)"><path data-c="1D43E" d="M285 628Q285 635 228 637Q205 637 198 638T191 647Q191 649 193 661Q199 681 203 682Q205 683 214 683H219Q260 681 355 681Q389 681 418 681T463 682T483 682Q500 682 500 674Q500 669 497 660Q496 658 496 654T495 648T493 644T490 641T486 639T479 638T470 637T456 637Q416 636 405 634T387 623L306 305Q307 305 490 449T678 597Q692 611 692 620Q692 635 667 637Q651 637 651 648Q651 650 654 662T659 677Q662 682 676 682Q680 682 711 681T791 680Q814 680 839 681T869 682Q889 682 889 672Q889 650 881 642Q878 637 862 637Q787 632 726 586Q710 576 656 534T556 455L509 418L518 396Q527 374 546 329T581 244Q656 67 661 61Q663 59 666 57Q680 47 717 46H738Q744 38 744 37T741 19Q737 6 731 0H720Q680 3 625 3Q503 3 488 0H478Q472 6 472 9T474 27Q478 40 480 43T491 46H494Q544 46 544 71Q544 75 517 141T485 216L427 354L359 301L291 248L268 155Q245 63 245 58Q245 51 253 49T303 46H334Q340 37 340 35Q340 19 333 5Q328 0 317 0Q314 0 280 1T180 2Q118 2 85 2T49 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Z"></path></g><g data-mml-node="mi" transform="translate(5521.9,0)"><path data-c="1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g></g></g></svg></mjx-container>,走到其曲线最顶端<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.023ex;" xmlns="http://www.w3.org/2000/svg" width="1.689ex" height="1.74ex" role="img" focusable="false" viewBox="0 -759 746.5 769"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"></path></g><g data-mml-node="mo" transform="translate(502,363) scale(0.707)"><path data-c="2032" d="M79 43Q73 43 52 49T30 61Q30 68 85 293T146 528Q161 560 198 560Q218 560 240 545T262 501Q262 496 260 486Q259 479 173 263T84 45T79 43Z"></path></g></g></g></g></svg></mjx-container>,然后在从<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.023ex;" xmlns="http://www.w3.org/2000/svg" width="1.689ex" height="1.74ex" role="img" focusable="false" viewBox="0 -759 746.5 769"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"></path></g><g data-mml-node="mo" transform="translate(502,363) scale(0.707)"><path data-c="2032" d="M79 43Q73 43 52 49T30 61Q30 68 85 293T146 528Q161 560 198 560Q218 560 240 545T262 501Q262 496 260 486Q259 479 173 263T84 45T79 43Z"></path></g></g></g></g></svg></mjx-container>所在横轴坐标处在<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="4.881ex" height="2.283ex" role="img" focusable="false" viewBox="0 -759 2157.5 1009"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D43D" d="M447 625Q447 637 354 637H329Q323 642 323 645T325 664Q329 677 335 683H352Q393 681 498 681Q541 681 568 681T605 682T619 682Q633 682 633 672Q633 670 630 658Q626 642 623 640T604 637Q552 637 545 623Q541 610 483 376Q420 128 419 127Q397 64 333 21T195 -22Q137 -22 97 8T57 88Q57 130 80 152T132 174Q177 174 182 130Q182 98 164 80T123 56Q115 54 115 53T122 44Q148 15 197 15Q235 15 271 47T324 130Q328 142 387 380T447 625Z"></path></g><g data-mml-node="mo" transform="translate(633,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msup" transform="translate(1022,0)"><g data-mml-node="mi"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"></path></g><g data-mml-node="mo" transform="translate(502,363) scale(0.707)"><path data-c="2032" d="M79 43Q73 43 52 49T30 61Q30 68 85 293T146 528Q161 560 198 560Q218 560 240 545T262 501Q262 496 260 486Q259 479 173 263T84 45T79 43Z"></path></g></g><g data-mml-node="mo" transform="translate(1768.5,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container>上构建新的<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="4.362ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 1928 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g><g data-mml-node="mo" transform="translate(681,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1070,0)"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"></path></g><g data-mml-node="mo" transform="translate(1539,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container>,然后不断更新。</p>
<p>这里C是等于where后面的那个值，D_KL表示KL散度。这里相当是是给出<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="4.881ex" height="2.283ex" role="img" focusable="false" viewBox="0 -759 2157.5 1009"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D43D" d="M447 625Q447 637 354 637H329Q323 642 323 645T325 664Q329 677 335 683H352Q393 681 498 681Q541 681 568 681T605 682T619 682Q633 682 633 672Q633 670 630 658Q626 642 623 640T604 637Q552 637 545 623Q541 610 483 376Q420 128 419 127Q397 64 333 21T195 -22Q137 -22 97 8T57 88Q57 130 80 152T132 174Q177 174 182 130Q182 98 164 80T123 56Q115 54 115 53T122 44Q148 15 197 15Q235 15 271 47T324 130Q328 142 387 380T447 625Z"></path></g><g data-mml-node="mo" transform="translate(633,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msup" transform="translate(1022,0)"><g data-mml-node="mi"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"></path></g><g data-mml-node="mo" transform="translate(502,363) scale(0.707)"><path data-c="2032" d="M79 43Q73 43 52 49T30 61Q30 68 85 293T146 528Q161 560 198 560Q218 560 240 545T262 501Q262 496 260 486Q259 479 173 263T84 45T79 43Z"></path></g></g><g data-mml-node="mo" transform="translate(1768.5,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container>的下界，表示我们策略是一直在往好的地方改进的</p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301012320121.png" alt="image-20230101232039041"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301012328232.png" alt="image-20230101232817159"></p>
<h2 id="PPO"><a href="#PPO" class="headerlink" title="PPO"></a>PPO</h2><p>PPO算法按照同策略（on policy）模式更新时，每次更新参数<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.023ex;" xmlns="http://www.w3.org/2000/svg" width="1.061ex" height="1.618ex" role="img" focusable="false" viewBox="0 -705 469 715"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"></path></g></g></g></svg></mjx-container>的目的是在旧参数$\theta<em>{old}<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.452ex;" xmlns="http://www.w3.org/2000/svg" width="2.262ex" height="2.149ex" role="img" focusable="false" viewBox="0 -750 1000 950"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">的</text></g></g></g></svg></mjx-container>\epsilon<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.452ex;" xmlns="http://www.w3.org/2000/svg" width="56.561ex" height="2.149ex" role="img" focusable="false" viewBox="0 -750 25000 950"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">领</text></g><g data-mml-node="mi" transform="translate(1000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">域</text></g><g data-mml-node="mi" transform="translate(2000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">中</text></g><g data-mml-node="mi" transform="translate(3000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">找</text></g><g data-mml-node="mi" transform="translate(4000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">到</text></g><g data-mml-node="mi" transform="translate(5000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">尽</text></g><g data-mml-node="mi" transform="translate(6000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">可</text></g><g data-mml-node="mi" transform="translate(7000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">能</text></g><g data-mml-node="mi" transform="translate(8000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">接</text></g><g data-mml-node="mi" transform="translate(9000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">近</text></g><g data-mml-node="mi" transform="translate(10000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">最</text></g><g data-mml-node="mi" transform="translate(11000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">优</text></g><g data-mml-node="mi" transform="translate(12000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">解</text></g><g data-mml-node="mi" transform="translate(13000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">的</text></g><g data-mml-node="mi" transform="translate(14000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">参</text></g><g data-mml-node="mi" transform="translate(15000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">数</text></g><g data-mml-node="mi" transform="translate(16000,0)"><text data-variant="italic" transform="scale(1,-1)" font-size="884px" font-family="serif" font-style="italic">，</text></g><g data-mml-node="mi" transform="translate(17000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">所</text></g><g data-mml-node="mi" transform="translate(18000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">以</text></g><g data-mml-node="mi" transform="translate(19000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">每</text></g><g data-mml-node="mi" transform="translate(20000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">轮</text></g><g data-mml-node="mi" transform="translate(21000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">更</text></g><g data-mml-node="mi" transform="translate(22000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">新</text></g><g data-mml-node="mi" transform="translate(23000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">参</text></g><g data-mml-node="mi" transform="translate(24000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">数</text></g></g></g></svg></mjx-container>\theta<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.452ex;" xmlns="http://www.w3.org/2000/svg" width="40.724ex" height="2.149ex" role="img" focusable="false" viewBox="0 -750 18000 950"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">时</text></g><g data-mml-node="mi" transform="translate(1000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">会</text></g><g data-mml-node="mi" transform="translate(2000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">用</text></g><g data-mml-node="mi" transform="translate(3000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">同</text></g><g data-mml-node="mi" transform="translate(4000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">一</text></g><g data-mml-node="mi" transform="translate(5000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">批</text></g><g data-mml-node="mi" transform="translate(6000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">数</text></g><g data-mml-node="mi" transform="translate(7000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">据</text></g><g data-mml-node="mi" transform="translate(8000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">更</text></g><g data-mml-node="mi" transform="translate(9000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">新</text></g><g data-mml-node="mi" transform="translate(10000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">多</text></g><g data-mml-node="mi" transform="translate(11000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">次</text></g><g data-mml-node="mi" transform="translate(12000,0)"><text data-variant="italic" transform="scale(1,-1)" font-size="884px" font-family="serif" font-style="italic">，</text></g><g data-mml-node="mi" transform="translate(13000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">然</text></g><g data-mml-node="mi" transform="translate(14000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">后</text></g><g data-mml-node="mi" transform="translate(15000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">赋</text></g><g data-mml-node="mi" transform="translate(16000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">值</text></g><g data-mml-node="mi" transform="translate(17000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">给</text></g></g></g></svg></mjx-container>\theta</em>{old}$，详见PPO算法论文或本课程的代码实现。</p>
<p>截断式优化目标意在用截断操作达到限制新旧策略的差异程度。</p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301021044447.jpeg" alt="img"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301021043932.png" alt="image-20230102104350782"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301021053111.png" alt="image-20230102105310029"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301021055541.png" alt="image-20230102105517465"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301021057047.png" alt="image-20230102105709971"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301021059447.png" alt="image-20230102105912366"></p>
<p>代码实现：</p>
<h2 id="模仿学习"><a href="#模仿学习" class="headerlink" title="模仿学习"></a>模仿学习</h2><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303081906491.png" alt="image-20230308190631407"></p>
<p>强化学习中的模仿学习和机器学习中的监督学习有什么相同与区别：</p>
<p>强化学习中的模仿学习（Imitation Learning）和机器学习中的监督学习（Supervised Learning）有一些相同点和区别。</p>
<p>相同点：</p>
<ol>
<li>数据驱动：它们都是基于数据的学习方法，需要使用大量的数据样本进行训练。</li>
<li>目标导向：它们都有一个明确的目标，即通过学习从输入到输出的映射来实现某种预期的目标。</li>
</ol>
<p>区别：</p>
<ol>
<li>数据来源不同：监督学习的数据来自于已有的标注数据集，而模仿学习的数据则来自于专家演示的行为轨迹。</li>
<li>预测结果不同：监督学习是从输入到输出的映射，即输入数据与输出结果之间的对应关系。而模仿学习是从状态到动作的映射，即给定一个状态，预测出在这个状态下应该采取的动作。</li>
<li>应用场景不同：监督学习主要应用于分类和回归等问题，而模仿学习主要应用于控制问题，如机器人控制、自动驾驶等。</li>
<li>误差来源不同：监督学习的误差来自于标签与预测值之间的差异，而模仿学习的误差来自于专家演示的行为与模型生成的行为之间的差异。</li>
</ol>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303081913255.png" alt="image-20230308191337145"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303081925448.png" alt="image-20230308192549357"></p>
<h3 id="行为克隆"><a href="#行为克隆" class="headerlink" title="行为克隆"></a>行为克隆</h3><p>行为克隆可以理解为以状态-动作对为特征值，以下一个状态值为标签的监督学习过程</p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303081939409.png" alt="image-20230308193945311"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303081957241.png" alt="image-20230308195704137"></p>
<p>强化学习中多模态的行为指的是什么？</p>
<p>在强化学习中，多模态的行为是指智能体在学习和执行任务时，能够利用多种不同类型的输入信号，从而实现更加丰富和复杂的行为。</p>
<p>在传统的强化学习中，智能体通常只能通过单一的输入信号（例如状态向量）来感知环境，并通过一个单一的输出信号（例如动作向量）来执行动作。但在现实世界中，智能体通常需要同时感知多种不同类型的信息，例如图像、声音、文本等。这些不同类型的信息可以提供不同的视角和丰富的信息，帮助智能体更好地理解环境和任务，并做出更加准确和高效的决策。</p>
<p>因此，利用多模态的行为可以提高智能体的学习能力和任务执行能力。例如，在视觉任务中，智能体可以同时感知图像和语音信号，从而更好地理解环境和执行任务；在语音交互任务中，智能体可以同时感知语音和文本信号，从而更好地理解用户的意图和需求。同时，多模态的行为也带来了更多的挑战和复杂性，例如如何有效地融合不同类型的输入信号和如何处理不同类型的输出信号等。</p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303081959331.png" alt="image-20230308195921217"></p>
<h3 id="逆强化学习"><a href="#逆强化学习" class="headerlink" title="逆强化学习"></a>逆强化学习</h3><p>逆强化学习找到一个代价函数使得专家策略产生较低的代价，而其他策略产生较高的代价，然后通过使用该代价函数进行正向强化学习，尝试恢复专家策略</p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303082015869.png" alt="image-20230308201530748"></p>
<p>奖励函数的相反数可以看做代价函数</p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303082021827.png" alt="image-20230308202128744"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303082036705.png" alt="image-20230308203613599"></p>
<h2 id="多智能体强化学习"><a href="#多智能体强化学习" class="headerlink" title="多智能体强化学习"></a>多智能体强化学习</h2><h3 id="简介-1"><a href="#简介-1" class="headerlink" title="简介"></a>简介</h3><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303031024768.png" alt="image-20230303102431618"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303031025452.png" alt="image-20230303102540354"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303031028941.png" alt="image-20230303102817840"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303031028584.png" alt="image-20230303102826502"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303031028982.png" alt="image-20230303102839788"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303031030288.png" alt="image-20230303103036203"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303031031194.png" alt="image-20230303103103984"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303031031408.png" alt="image-20230303103149326"></p>
<p>多智能体的环境是非稳态的，智能体不仅和环境交互，智能体之间还要交互，假如把其他智能体当成环境的一部分从而使用单智能体的学习算法，结果不可行，原因是该方法环境不是稳态的，任何一个智能体策略的改变会影响其它智能体的策略，学习不收敛</p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303031038560.png" alt="image-20230303103853467"></p>
<h3 id="入门-1"><a href="#入门-1" class="headerlink" title="入门"></a>入门</h3><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303031041775.png" alt="image-20230303104109698"></p>
<p>多智能体环境是非稳态的[稳态的意思是说条件概率分布是不能改变的]</p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303031043204.png" alt="image-20230303104336095"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303031048575.png" alt="image-20230303104853485"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303031100334.png" alt="image-20230303110023247"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303031105881.png" alt="image-20230303110503803"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303031106531.png" alt="image-20230303110653452"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303031111360.png" alt="image-20230303111145257"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303031114244.png" alt="image-20230303111444172"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303031115570.png" alt="image-20230303111500485"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303031736832.png" alt="image-20230303173606728"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303031744560.png" alt="image-20230303174442467"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303031911579.png" alt="image-20230303191154495"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303031913977.png" alt="image-20230303191345884"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303031920151.png" alt="image-20230303192014078"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303031920941.png" alt="image-20230303192034842"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303031924089.png" alt="image-20230303192419991"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303031930222.png" alt="image-20230303193032127"></p>
<h3 id="进阶"><a href="#进阶" class="headerlink" title="进阶"></a>进阶</h3><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303031932036.png" alt="image-20230303193257923"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303031935733.png" alt="image-20230303193520635"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303031938882.png" alt="image-20230303193844766"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303031944040.png" alt="image-20230303194427944"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303031948006.png" alt="image-20230303194834911"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303031951799.png" alt="image-20230303195149697"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202303031958413.png" alt="image-20230303195840324"></p>
<h2 id="Offline-RL-离线强化学习"><a href="#Offline-RL-离线强化学习" class="headerlink" title="Offline RL(离线强化学习)"></a>Offline RL(离线强化学习)</h2><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301041604042.png" alt="image-20230104160406673"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301041608715.png" alt="image-20230104160839619"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301041612897.png" alt="image-20230104161235815"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301041625962.png" alt="image-20230104162528790"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301041712658.png" alt="image-20230104171225562"></p>
<p>BCQ</p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301041718081.png" alt="image-20230104171818998"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301041723893.png" alt="image-20230104172337711"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301041726755.png" alt="image-20230104172635675"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301041739631.png" alt="image-20230104173925537"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301041746336.png" alt="image-20230104174639232"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301041747049.png" alt="image-20230104174755920"></p>
<p>==CQL==</p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301041800832.png" alt="image-20230104180059729"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301041802959.png" alt="image-20230104180214860"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202301041802492.png" alt="image-20230104180232377"></p>
<h2 id="知识Tips"><a href="#知识Tips" class="headerlink" title="知识Tips"></a>知识Tips</h2><ul>
<li>深度强化学习可以分类为基于价值的方法、基于随机策略的方法和基于确定性策略的方法，其中基于确定性策略的方法包括了确定性策略梯度（DPG）和深度确定性策略梯度（DDPG）</li>
<li>第一，reward设置成[-1, 1]是normalized之后的结果，一般reward的设置是根据reward function或是根据一些经验值，比如在一些经典的迷宫场景中，reward的设置一般是一步-1的reward，作用是鞭策agent加快学习过程，在一些悬崖的地方会设置很大的负reward比如-200，这是因为这些地方会直接导致游戏结束，所以reward会很大，有点类似于自动驾驶中的撞车。不过总体来说reward的设置多数是根据经验并结合reward function。第二，强化学习本身也是一个搜索和优化的过程，肯定存在一些局部最优点，agent在学习的过程中很可能会收敛到局部最优点，解决方法主要是通过exploration来扩大搜索范围，防止agent因为没有见过相关的state而收敛到现有的state，目前也有一种方法是学习anti-goal，可以<a target="_blank" rel="noopener" href="https://papers.nips.cc/paper/9225-keeping-your-distance-solving-sparse-reward-tasks-using-self-balancing-shaped-rewards.pdf">参考</a></li>
</ul>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="https://mr-maktoub.github.io">马克图布</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://mr-maktoub.github.io/posts/6040/">https://mr-maktoub.github.io/posts/6040/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://mr-maktoub.github.io" target="_blank">马克图布</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="/img/avatar1.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/posts/96ac/" title="Tensorflow2学习笔记"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Tensorflow2学习笔记</div></div></a></div><div class="next-post pull-right"><a href="/posts/7280/" title="What is Mutual Information?"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">What is Mutual Information?</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/avatar1.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">马克图布</div><div class="author-info__description">记录我的遗忘</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">66</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">22</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">13</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B"><span class="toc-number">1.</span> <span class="toc-text">强化学习简介</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8E%A2%E7%B4%A2%E4%B8%8E%E5%88%A9%E7%94%A8"><span class="toc-number">2.</span> <span class="toc-text">探索与利用</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B"><span class="toc-number">3.</span> <span class="toc-text">马尔科夫决策过程</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E7%9A%84%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0"><span class="toc-number">4.</span> <span class="toc-text">基于动态规划的强化学习</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E6%96%B9%E6%B3%95"><span class="toc-number">5.</span> <span class="toc-text">蒙特卡洛方法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E4%BB%B7%E5%80%BC%E9%A2%84%E6%B5%8B"><span class="toc-number">6.</span> <span class="toc-text">蒙特卡洛价值预测</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%A7%84%E5%88%92%E4%B8%8E%E5%AD%A6%E4%B9%A0"><span class="toc-number">7.</span> <span class="toc-text">规划与学习</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%A5%E9%97%A8"><span class="toc-number">7.1.</span> <span class="toc-text">入门</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%87%87%E6%A0%B7%E6%96%B9%E6%B3%95"><span class="toc-number">7.2.</span> <span class="toc-text">采样方法</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E6%97%A0%E5%85%B3%E6%8E%A7%E5%88%B6%E6%96%B9%E6%B3%95"><span class="toc-number">8.</span> <span class="toc-text">模型无关控制方法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6%EF%BC%88Policy-Gradient%EF%BC%89"><span class="toc-number">9.</span> <span class="toc-text">策略梯度（Policy Gradient）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B7%B1%E5%BA%A6Q%E7%BD%91%E7%BB%9C"><span class="toc-number">10.</span> <span class="toc-text">深度Q网络</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Actor-Critic%E6%96%B9%E6%B3%95"><span class="toc-number">11.</span> <span class="toc-text">Actor-Critic方法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%87%8D%E8%A6%81%E6%80%A7%E9%87%87%E6%A0%B7"><span class="toc-number">12.</span> <span class="toc-text">重要性采样</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%97%B6%E5%BA%8F%E5%B7%AE%E5%88%86%E5%AD%A6%E4%B9%A0"><span class="toc-number">13.</span> <span class="toc-text">时序差分学习</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#A3C"><span class="toc-number">14.</span> <span class="toc-text">A3C</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%A1%AE%E5%AE%9A%E6%80%A7%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6-DPG"><span class="toc-number">15.</span> <span class="toc-text">确定性策略梯度(DPG)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B7%B1%E5%BA%A6%E7%A1%AE%E5%AE%9A%E6%80%A7%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6-DDPG"><span class="toc-number">16.</span> <span class="toc-text">深度确定性策略梯度(DDPG)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BF%A1%E4%BB%BB%E5%8C%BA%E5%9F%9F%E7%AD%96%E7%95%A5%E4%BC%98%E5%8C%96TRPO"><span class="toc-number">17.</span> <span class="toc-text">信任区域策略优化TRPO</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#PPO"><span class="toc-number">18.</span> <span class="toc-text">PPO</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E4%BB%BF%E5%AD%A6%E4%B9%A0"><span class="toc-number">19.</span> <span class="toc-text">模仿学习</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AE%80%E4%BB%8B"><span class="toc-number">19.1.</span> <span class="toc-text">简介</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A1%8C%E4%B8%BA%E5%85%8B%E9%9A%86"><span class="toc-number">19.2.</span> <span class="toc-text">行为克隆</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%80%86%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0"><span class="toc-number">19.3.</span> <span class="toc-text">逆强化学习</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%9A%E6%99%BA%E8%83%BD%E4%BD%93%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0"><span class="toc-number">20.</span> <span class="toc-text">多智能体强化学习</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AE%80%E4%BB%8B-1"><span class="toc-number">20.1.</span> <span class="toc-text">简介</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%A5%E9%97%A8-1"><span class="toc-number">20.2.</span> <span class="toc-text">入门</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BF%9B%E9%98%B6"><span class="toc-number">20.3.</span> <span class="toc-text">进阶</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Offline-RL-%E7%A6%BB%E7%BA%BF%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0"><span class="toc-number">21.</span> <span class="toc-text">Offline RL(离线强化学习)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%9F%A5%E8%AF%86Tips"><span class="toc-number">22.</span> <span class="toc-text">知识Tips</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/3eeb/" title="Hello World">Hello World</a><time datetime="2023-06-01T07:29:35.968Z" title="发表于 2023-06-01 15:29:35">2023-06-01</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/6f25/" title="我的云音乐APP开发课程笔记">我的云音乐APP开发课程笔记</a><time datetime="2023-06-01T07:29:35.962Z" title="发表于 2023-06-01 15:29:35">2023-06-01</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/8462/" title="重新梳理Android权限管理">重新梳理Android权限管理</a><time datetime="2023-06-01T07:29:35.959Z" title="发表于 2023-06-01 15:29:35">2023-06-01</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/9958/" title="《吴恩达机器学习笔记》">《吴恩达机器学习笔记》</a><time datetime="2023-06-01T07:29:35.949Z" title="发表于 2023-06-01 15:29:35">2023-06-01</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/935d/" title="常见不等式、等式与基础理论">常见不等式、等式与基础理论</a><time datetime="2023-06-01T07:29:35.914Z" title="发表于 2023-06-01 15:29:35">2023-06-01</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2023 By 马克图布</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container').forEach(node => {
            if (node.hasAttribute('display')) {
              btf.wrap(node, 'div', { class: 'mathjax-overflow' })
            } else {
              btf.wrap(node, 'span', { class: 'mathjax-overflow' })
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>