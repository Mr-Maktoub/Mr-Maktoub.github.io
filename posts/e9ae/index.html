<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>精读论文系列 | 马克图布</title><meta name="author" content="马克图布"><meta name="copyright" content="马克图布"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="如何读论文？视频链接 绝大部分文章结构 1.title -&gt; 2.abstract -&gt; 3.introduction -&gt; 4.method -&gt; 5. experiment -&gt; 6.conclusion 读论文是要快速找到适合自己的文章，然后进行精读 方法：怎么样花三遍读一篇论文。 第一遍：（海选） 需要关注论文的标题和摘要，读完摘要之后直接跳到论文结论部分。也">
<meta property="og:type" content="article">
<meta property="og:title" content="精读论文系列">
<meta property="og:url" content="https://mr-maktoub.github.io/posts/e9ae/index.html">
<meta property="og:site_name" content="马克图布">
<meta property="og:description" content="如何读论文？视频链接 绝大部分文章结构 1.title -&gt; 2.abstract -&gt; 3.introduction -&gt; 4.method -&gt; 5. experiment -&gt; 6.conclusion 读论文是要快速找到适合自己的文章，然后进行精读 方法：怎么样花三遍读一篇论文。 第一遍：（海选） 需要关注论文的标题和摘要，读完摘要之后直接跳到论文结论部分。也">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://mr-maktoub.github.io/img/avatar1.png">
<meta property="article:published_time" content="2022-09-12T10:35:50.000Z">
<meta property="article:modified_time" content="2023-05-30T01:43:09.017Z">
<meta property="article:author" content="马克图布">
<meta property="article:tag" content="论文">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://mr-maktoub.github.io/img/avatar1.png"><link rel="shortcut icon" href="/img/log3.jfif"><link rel="canonical" href="https://mr-maktoub.github.io/posts/e9ae/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '精读论文系列',
  isPost: true,
  isHome: false,
  isHighlightShrink: true,
  isToc: true,
  postUpdate: '2023-05-30 09:43:09'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"><style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/avatar1.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">66</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">22</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">13</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw iconfont icon-shouyex"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw iconfont icon-zuixinwenzhang_huaban"></i><span> 找文章</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw iconfont icon-fenlei1"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw iconfont icon-biaoqian1"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw iconfont icon-shijianzhou"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw iconfont icon-shenghuo"></i><span> 生活</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/messageboard/"><i class="fa-fw iconfont icon-liaotian-04"></i><span> 留言板</span></a></li><li><a class="site-page child" href="/link/"><i class="fa-fw iconfont icon-lianjie"></i><span> 友人帐</span></a></li><li><a class="site-page child" href="/share/"><i class="fa-fw iconfont icon-fenxiang"></i><span> 分享</span></a></li><li><a class="site-page child" href="/gallery/"><i class="fa-fw iconfont icon-xiangce"></i><span> 相册</span></a></li><li><a class="site-page child" href="/books/"><i class="fa-fw iconfont icon-shuji"></i><span> 书籍</span></a></li><li><a class="site-page child" href="/FilmAndTV/"><i class="fa-fw iconfont icon-yingshi1"></i><span> 影视</span></a></li><li><a class="site-page child" href="/daohang/"><i class="fa-fw iconfont icon-daohang"></i><span> 导航站</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw iconfont icon-gerenzhongxin"></i><span> 关于笔者</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/img/cover.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="马克图布"><img class="site-icon" src="/img/log3.jfif"/><span class="site-name">马克图布</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw iconfont icon-shouyex"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw iconfont icon-zuixinwenzhang_huaban"></i><span> 找文章</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw iconfont icon-fenlei1"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw iconfont icon-biaoqian1"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw iconfont icon-shijianzhou"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw iconfont icon-shenghuo"></i><span> 生活</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/messageboard/"><i class="fa-fw iconfont icon-liaotian-04"></i><span> 留言板</span></a></li><li><a class="site-page child" href="/link/"><i class="fa-fw iconfont icon-lianjie"></i><span> 友人帐</span></a></li><li><a class="site-page child" href="/share/"><i class="fa-fw iconfont icon-fenxiang"></i><span> 分享</span></a></li><li><a class="site-page child" href="/gallery/"><i class="fa-fw iconfont icon-xiangce"></i><span> 相册</span></a></li><li><a class="site-page child" href="/books/"><i class="fa-fw iconfont icon-shuji"></i><span> 书籍</span></a></li><li><a class="site-page child" href="/FilmAndTV/"><i class="fa-fw iconfont icon-yingshi1"></i><span> 影视</span></a></li><li><a class="site-page child" href="/daohang/"><i class="fa-fw iconfont icon-daohang"></i><span> 导航站</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw iconfont icon-gerenzhongxin"></i><span> 关于笔者</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">精读论文系列</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-09-12T10:35:50.000Z" title="发表于 2022-09-12 18:35:50">2022-09-12</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-05-30T01:43:09.017Z" title="更新于 2023-05-30 09:43:09">2023-05-30</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E5%AD%A6%E6%9C%AF/">学术</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="精读论文系列"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h2 id="如何读论文？"><a href="#如何读论文？" class="headerlink" title="如何读论文？"></a>如何读论文？</h2><p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1H44y1t75x/?spm_id_from=333.788&amp;vd_source=2ffed29a08d7c0e5407d996c2c4915a8">视频链接</a></p>
<p>绝大部分文章结构</p>
<p>1.title -&gt; 2.abstract -&gt; 3.introduction -&gt; 4.method -&gt; 5. experiment -&gt; 6.conclusion</p>
<p>读论文是要快速找到适合自己的文章，然后进行精读</p>
<p>方法：怎么样花三遍读一篇论文。</p>
<p>第一遍：（海选）</p>
<p>需要关注论文的标题和摘要，读完摘要之后直接跳到论文结论部分。也可以看一下文章中的图和表，知道论文的工作是在做什么，方法看上去怎么样，结果怎么样。整个过程大概十几分钟的时间，看适不适合自己，决定是不是要继续往下读。</p>
<p>第二遍：（精选）</p>
<p>对整个文章过一遍，知道文章具体在做什么东西，不用太过关注文章的细节，比如公式证明什么的，可以先忽略掉。但是对于每一张图和表，它的每一个字你都要知道他是在做什么，它的x轴，y轴你都要知道是什么意思；明白作者提出的方法和别人的方法是怎么对比的，之间的差距有多大。对于一些引用的重要文献可以圈出来（比如是在哪篇论文的基础上）。决定要不要继续往下精读，如果你感觉文章太难，可以去读一下这边论文引用的一些文章。如果是不需要了解那么深，不需要完全搞懂论文，可以不读第三遍。</p>
<p>第三遍：（精读）</p>
<p>这一遍要知道每一句话在说什么，每一段在说什么。多思考，多脑补。</p>
<blockquote>
<p>NOTE:</p>
<p>可以边读，边记录，同时做ppt将自己读到的内容尽量的浓缩</p>
</blockquote>
<h2 id="RL"><a href="#RL" class="headerlink" title="RL"></a>RL</h2><h3 id="1-Playing-Atari-with-Deep-Reinforcement-Learning"><a href="#1-Playing-Atari-with-Deep-Reinforcement-Learning" class="headerlink" title="1 Playing Atari with Deep Reinforcement Learning"></a>1 <em>Playing Atari with Deep Reinforcement Learning</em></h3><p><code>tag:dqn</code></p>
<p><strong>Abstract</strong></p>
<p>用dqn训练Arcade 2600种游戏，在其中6个游戏中超越之前所有的算法，在其中三个游戏中超过人类专家。</p>
<p><strong>Introduction</strong></p>
<p>网络结构，卷积神经网络提取原始像素特征</p>
<p>应用算法，Q-learning变体————DQN</p>
<p>优化器，SGD</p>
<p>技巧，经验回放(experience replay mechanism)</p>
<p><strong>Background</strong></p>
<p>目标是让智能体选择动作与模拟器交互从而最大化未来的奖励，做了一个假设是未来的奖励会以每步<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.489ex;" xmlns="http://www.w3.org/2000/svg" width="1.229ex" height="1.486ex" role="img" focusable="false" viewBox="0 -441 543 657"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D6FE" d="M31 249Q11 249 11 258Q11 275 26 304T66 365T129 418T206 441Q233 441 239 440Q287 429 318 386T371 255Q385 195 385 170Q385 166 386 166L398 193Q418 244 443 300T486 391T508 430Q510 431 524 431H537Q543 425 543 422Q543 418 522 378T463 251T391 71Q385 55 378 6T357 -100Q341 -165 330 -190T303 -216Q286 -216 286 -188Q286 -138 340 32L346 51L347 69Q348 79 348 100Q348 257 291 317Q251 355 196 355Q148 355 108 329T51 260Q49 251 47 251Q45 249 31 249Z"></path></g></g></g></svg></mjx-container> 进行折扣，并定义时间t的回报为</p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202209180929917.png" alt="image-20220916130409930"></p>
<p>其中<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0;" xmlns="http://www.w3.org/2000/svg" width="1.593ex" height="1.532ex" role="img" focusable="false" viewBox="0 -677 704 677"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g></g></g></svg></mjx-container>是游戏终止的时长</p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202209180011033.png" alt="image-20220918001109987"></p>
<p><strong><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="0.919ex" height="1ex" role="img" focusable="false" viewBox="0 -431 406 442"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D716" d="M227 -11Q149 -11 95 41T40 174Q40 262 87 322Q121 367 173 396T287 430Q289 431 329 431H367Q382 426 382 411Q382 385 341 385H325H312Q191 385 154 277L150 265H327Q340 256 340 246Q340 228 320 219H138V217Q128 187 128 143Q128 77 160 52T231 26Q258 26 284 36T326 57T343 68Q350 68 354 58T358 39Q358 36 357 35Q354 31 337 21T289 0T227 -11Z"></path></g></g></g></svg></mjx-container>-greedy strategy</strong>:follows the greedy strategy with probability 1 − <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="0.919ex" height="1ex" role="img" focusable="false" viewBox="0 -431 406 442"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D716" d="M227 -11Q149 -11 95 41T40 174Q40 262 87 322Q121 367 173 396T287 430Q289 431 329 431H367Q382 426 382 411Q382 385 341 385H325H312Q191 385 154 277L150 265H327Q340 256 340 246Q340 228 320 219H138V217Q128 187 128 143Q128 77 160 52T231 26Q258 26 284 36T326 57T343 68Q350 68 354 58T358 39Q358 36 357 35Q354 31 337 21T289 0T227 -11Z"></path></g></g></g></svg></mjx-container> and selects a<br>random action with probability  <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="0.919ex" height="1ex" role="img" focusable="false" viewBox="0 -431 406 442"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D716" d="M227 -11Q149 -11 95 41T40 174Q40 262 87 322Q121 367 173 396T287 430Q289 431 329 431H367Q382 426 382 411Q382 385 341 385H325H312Q191 385 154 277L150 265H327Q340 256 340 246Q340 228 320 219H138V217Q128 187 128 143Q128 77 160 52T231 26Q258 26 284 36T326 57T343 68Q350 68 354 58T358 39Q358 36 357 35Q354 31 337 21T289 0T227 -11Z"></path></g></g></g></svg></mjx-container> .</p>
<p><strong>Deep Reinforcement Learning</strong></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202209181713009.png" alt="image-20220918171315928"></p>
<p>相比Q-Learning算法的优点：</p>
<p>(1)每一步经验可能更新许多权重，这增加了数据利用率</p>
<p>(2打断了数据之间的关联性，减小方差，获得更好的训练效果</p>
<p>(3)第三，在学习策略时，当前参数决定了训练参数的下一个数据样本。 例如，如果最大化动作是向左移动，那么训练样本将由左侧的样本支配,如果最大化动作然后切换到右侧，那么训练分布也将切换。</p>
<p><strong>Preprocessing and Model Architecture</strong></p>
<h3 id="2-CONTINUOUS-CONTROL-WITH-DEEP-REINFORCEMENT-LEARNING"><a href="#2-CONTINUOUS-CONTROL-WITH-DEEP-REINFORCEMENT-LEARNING" class="headerlink" title="2 CONTINUOUS CONTROL WITH DEEP REINFORCEMENT LEARNING"></a>2 CONTINUOUS CONTROL WITH DEEP REINFORCEMENT LEARNING</h3><p>tag:  <code>ddpg，model-free, off-policy，actor-critic</code></p>
<h4 id="1-INTRODUCTION"><a href="#1-INTRODUCTION" class="headerlink" title="1 INTRODUCTION"></a><strong>1 INTRODUCTION</strong></h4><p>DQN的出现时强化学习领域一个大的进展，它用深度神经网络来近似最优价值动作函数，通过动作价值函数来寻找价值最大的动作，但是它的缺点是不适合应用在连续控制上，尽管可以通过将动作离散化，但是也会面临高维灾难：即动作的数量会随着自由度的增加呈现指数型增长。</p>
<p>这篇论文是基于 deterministic policy gradient (DPG) algorithm，DPG中对于具有挑战性的问题，这种带有神经函数近似器的actor-critic朴素应用是不稳定的。本文将actor-critic算法与DQN中的一些方法结合起来使用，</p>
<p>dqn能够stable and robust（稳定和鲁棒）的训练价值网络，归因于两个创新：</p>
<p>1.经验回放。建立一个缓冲区：replay buffer ，从replay buffer中采样用off-policy方式训练网络会减小样本间的相关性</p>
<p>2.为了缓解高估问题（TD算法中），在用TD算法训练网络不是用Q Network，而是用一个target Q network来训练。</p>
<p>本文同样使用了Batch Normalization    </p>
<h4 id="2-BACKGROUND"><a href="#2-BACKGROUND" class="headerlink" title="2 BACKGROUND"></a><strong>2 BACKGROUND</strong></h4><p><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.65ex;" xmlns="http://www.w3.org/2000/svg" width="19.886ex" height="2.347ex" role="img" focusable="false" viewBox="0 -750 8789.6 1037.2"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D43D" d="M447 625Q447 637 354 637H329Q323 642 323 645T325 664Q329 677 335 683H352Q393 681 498 681Q541 681 568 681T605 682T619 682Q633 682 633 672Q633 670 630 658Q626 642 623 640T604 637Q552 637 545 623Q541 610 483 376Q420 128 419 127Q397 64 333 21T195 -22Q137 -22 97 8T57 88Q57 130 80 152T132 174Q177 174 182 130Q182 98 164 80T123 56Q115 54 115 53T122 44Q148 15 197 15Q235 15 271 47T324 130Q328 142 387 380T447 625Z"></path></g><g data-mml-node="mo" transform="translate(910.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="msub" transform="translate(1966.6,0)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D53C" d="M12 666Q12 675 24 683H582Q590 680 593 672V588Q593 514 591 502T575 490Q567 490 563 495T555 517Q552 556 517 590Q486 623 445 634T340 648H282Q266 636 264 620T260 492V370H277Q329 375 358 391T404 439Q420 480 420 506Q420 529 436 529Q445 529 451 521Q455 517 455 361Q455 333 455 298T456 253Q456 217 453 207T437 197Q420 196 420 217Q420 240 406 270Q377 328 284 335H260V201Q261 174 261 134Q262 73 264 61T278 38Q281 36 282 35H331Q400 35 449 50Q571 93 602 179Q605 203 622 203Q629 203 634 197T640 183Q638 181 624 95T604 3L600 -1H24Q12 5 12 16Q12 35 51 35Q92 38 97 52Q102 60 102 341T97 632Q91 645 51 648Q12 648 12 666ZM137 341Q137 131 136 89T130 37Q129 36 129 35H235Q233 41 231 48L226 61V623L231 635L235 648H129Q132 641 133 638T135 603T137 517T137 341ZM557 603V648H504Q504 646 515 639Q527 634 542 619L557 603ZM420 317V397L406 383Q394 370 380 363L366 355Q373 350 382 346Q400 333 409 328L420 317ZM582 61L586 88Q585 88 582 83Q557 61 526 46L511 37L542 35H577Q577 36 578 39T580 49T582 61Z"></path></g></g><g data-mml-node="TeXAtom" transform="translate(700,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(484,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(778,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msub" transform="translate(1056,0)"><g data-mml-node="mi"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path></g><g data-mml-node="mi" transform="translate(502,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(1851.9,0)"><path data-c="223C" d="M55 166Q55 241 101 304T222 367Q260 367 296 349T362 304T421 252T484 208T554 189Q616 189 655 236T694 338Q694 350 698 358T708 367Q722 367 722 334Q722 260 677 197T562 134H554Q517 134 481 152T414 196T355 248T292 293T223 311Q179 311 145 286Q109 257 96 218T80 156T69 133Q55 133 55 166Z"></path></g><g data-mml-node="mi" transform="translate(2629.9,0)"><path data-c="1D438" d="M492 213Q472 213 472 226Q472 230 477 250T482 285Q482 316 461 323T364 330H312Q311 328 277 192T243 52Q243 48 254 48T334 46Q428 46 458 48T518 61Q567 77 599 117T670 248Q680 270 683 272Q690 274 698 274Q718 274 718 261Q613 7 608 2Q605 0 322 0H133Q31 0 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q146 66 215 342T285 622Q285 629 281 629Q273 632 228 634H197Q191 640 191 642T193 659Q197 676 203 680H757Q764 676 764 669Q764 664 751 557T737 447Q735 440 717 440H705Q698 445 698 453L701 476Q704 500 704 528Q704 558 697 578T678 609T643 625T596 632T532 634H485Q397 633 392 631Q388 629 386 622Q385 619 355 499T324 377Q347 376 372 376H398Q464 376 489 391T534 472Q538 488 540 490T557 493Q562 493 565 493T570 492T572 491T574 487T577 483L544 351Q511 218 508 216Q505 213 492 213Z"></path></g><g data-mml-node="mo" transform="translate(3393.9,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msub" transform="translate(3671.9,0)"><g data-mml-node="mi"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="mi" transform="translate(562,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(4527.9,0)"><path data-c="223C" d="M55 166Q55 241 101 304T222 367Q260 367 296 349T362 304T421 252T484 208T554 189Q616 189 655 236T694 338Q694 350 698 358T708 367Q722 367 722 334Q722 260 677 197T562 134H554Q517 134 481 152T414 196T355 248T292 293T223 311Q179 311 145 286Q109 257 96 218T80 156T69 133Q55 133 55 166Z"></path></g><g data-mml-node="mi" transform="translate(5305.9,0)"><path data-c="1D70B" d="M132 -11Q98 -11 98 22V33L111 61Q186 219 220 334L228 358H196Q158 358 142 355T103 336Q92 329 81 318T62 297T53 285Q51 284 38 284Q19 284 19 294Q19 300 38 329T93 391T164 429Q171 431 389 431Q549 431 553 430Q573 423 573 402Q573 371 541 360Q535 358 472 358H408L405 341Q393 269 393 222Q393 170 402 129T421 65T431 37Q431 20 417 5T381 -10Q370 -10 363 -7T347 17T331 77Q330 86 330 121Q330 170 339 226T357 318T367 358H269L268 354Q268 351 249 275T206 114T175 17Q164 -11 132 -11Z"></path></g></g></g><g data-mml-node="mrow" transform="translate(7038.1,0)"><g data-mml-node="mo"><path data-c="5B" d="M118 -250V750H255V710H158V-210H255V-250H118Z"></path></g><g data-mml-node="msub" transform="translate(278,0)"><g data-mml-node="mi"><path data-c="1D445" d="M230 637Q203 637 198 638T193 649Q193 676 204 682Q206 683 378 683Q550 682 564 680Q620 672 658 652T712 606T733 563T739 529Q739 484 710 445T643 385T576 351T538 338L545 333Q612 295 612 223Q612 212 607 162T602 80V71Q602 53 603 43T614 25T640 16Q668 16 686 38T712 85Q717 99 720 102T735 105Q755 105 755 93Q755 75 731 36Q693 -21 641 -21H632Q571 -21 531 4T487 82Q487 109 502 166T517 239Q517 290 474 313Q459 320 449 321T378 323H309L277 193Q244 61 244 59Q244 55 245 54T252 50T269 48T302 46H333Q339 38 339 37T336 19Q332 6 326 0H311Q275 2 180 2Q146 2 117 2T71 2T50 1Q33 1 33 10Q33 12 36 24Q41 43 46 45Q50 46 61 46H67Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628Q287 635 230 637ZM630 554Q630 586 609 608T523 636Q521 636 500 636T462 637H440Q393 637 386 627Q385 624 352 494T319 361Q319 360 388 360Q466 361 492 367Q556 377 592 426Q608 449 619 486T630 554Z"></path></g><g data-mml-node="mn" transform="translate(792,-150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g><g data-mml-node="mo" transform="translate(1473.6,0)"><path data-c="5D" d="M22 710V750H159V-250H22V-210H119V710H22Z"></path></g></g></g></g></svg></mjx-container></p>
<p>$R<em>t=\sum</em>{i=t}^T \gamma^{(i-t)} r\left(s_i, a_i\right)$</p>
<p>$Q^\pi\left(s<em>t, a_t\right)=\mathbb{E}</em>{r<em>{i \geq t}, s</em>{i&gt;t} \sim E, a_{i&gt;t} \sim \pi}\left[R_t \mid s_t, a_t\right]$</p>
<p>$\left(s<em>t, a_t, r_t, s</em>{t+1}\right)$</p>
<p>序列:At each timestep t the agent receives an observation xt, takes an action at and receives a scalar reward rt.</p>
<p>Here, we assumed the environment is fully-observed so st = xt.</p>
<p>An agent’s behavior is defined by a policy, π, which maps states to a probability distribution over the actions π : S → P(A).We model it as a ==Markov decision process（MDP）== with a state space S, action space A = <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0;" xmlns="http://www.w3.org/2000/svg" width="3.242ex" height="1.914ex" role="img" focusable="false" viewBox="0 -846 1432.9 846"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="211D" d="M17 665Q17 672 28 683H221Q415 681 439 677Q461 673 481 667T516 654T544 639T566 623T584 607T597 592T607 578T614 565T618 554L621 548Q626 530 626 497Q626 447 613 419Q578 348 473 326L455 321Q462 310 473 292T517 226T578 141T637 72T686 35Q705 30 705 16Q705 7 693 -1H510Q503 6 404 159L306 310H268V183Q270 67 271 59Q274 42 291 38Q295 37 319 35Q344 35 353 28Q362 17 353 3L346 -1H28Q16 5 16 16Q16 35 55 35Q96 38 101 52Q106 60 106 341T101 632Q95 645 55 648Q17 648 17 665ZM241 35Q238 42 237 45T235 78T233 163T233 337V621L237 635L244 648H133Q136 641 137 638T139 603T141 517T141 341Q141 131 140 89T134 37Q133 36 133 35H241ZM457 496Q457 540 449 570T425 615T400 634T377 643Q374 643 339 648Q300 648 281 635Q271 628 270 610T268 481V346H284Q327 346 375 352Q421 364 439 392T457 496ZM492 537T492 496T488 427T478 389T469 371T464 361Q464 360 465 360Q469 360 497 370Q593 400 593 495Q593 592 477 630L457 637L461 626Q474 611 488 561Q492 537 492 496ZM464 243Q411 317 410 317Q404 317 401 315Q384 315 370 312H346L526 35H619L606 50Q553 109 464 243Z"></path></g></g><g data-mml-node="mi" transform="translate(755,363) scale(0.707)"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"></path></g></g></g></g></svg></mjx-container>, an initial state distribution p(s1), transition dynamics p(st+1|st,at), and reward function r(st, at).</p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202212072147716.png" alt="image-20221207214740667"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202212072151123.png" alt="image-20221207215157080"></p>
<p>Bellman equation:</p>
<p>$Q^\pi\left(s<em>t, a_t\right)=\mathbb{E}</em>{r<em>t, s</em>{t+1} \sim E}\left[r\left(s<em>t, a_t\right)+\gamma \mathbb{E}</em>{a<em>{t+1} \sim \pi}\left[Q^\pi\left(s</em>{t+1}, a_{t+1}\right)\right]\right]$</p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202212072202325.png" alt="image-20221207220245275"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202212072252446.png" alt="image-20221207225239409"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202212072300945.png" alt="image-20221207230017892"></p>
<h4 id="3-ALGORITHM"><a href="#3-ALGORITHM" class="headerlink" title="3 ALGORITHM"></a><strong>3 ALGORITHM</strong></h4><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202212072332222.png" alt="image-20221207233245160"></p>
<p>遇到挑战：</p>
<p>1.假设 独立同分布。when using neural networks for reinforcement learning is that most optimization algorithms assume that the samples are independently and identically distributed。Obviously, when the samples are generated from exploring sequentially in an environment this assumption no longer holds（显然，当样本是在一个环境中连续探索产生的时，这个假设就不再成立了）</p>
<p>Additionally, to make efficient use of hardware optimizations, it is essential to learn in mini-<br>batches, rather than online.</p>
<p>解决方法：经验回放。设置一个replay buffer（replay buffer是一个容量为<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.048ex;" xmlns="http://www.w3.org/2000/svg" width="1.717ex" height="1.593ex" role="img" focusable="false" viewBox="0 -683 759 704"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D445" d="M230 637Q203 637 198 638T193 649Q193 676 204 682Q206 683 378 683Q550 682 564 680Q620 672 658 652T712 606T733 563T739 529Q739 484 710 445T643 385T576 351T538 338L545 333Q612 295 612 223Q612 212 607 162T602 80V71Q602 53 603 43T614 25T640 16Q668 16 686 38T712 85Q717 99 720 102T735 105Q755 105 755 93Q755 75 731 36Q693 -21 641 -21H632Q571 -21 531 4T487 82Q487 109 502 166T517 239Q517 290 474 313Q459 320 449 321T378 323H309L277 193Q244 61 244 59Q244 55 245 54T252 50T269 48T302 46H333Q339 38 339 37T336 19Q332 6 326 0H311Q275 2 180 2Q146 2 117 2T71 2T50 1Q33 1 33 10Q33 12 36 24Q41 43 46 45Q50 46 61 46H67Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628Q287 635 230 637ZM630 554Q630 586 609 608T523 636Q521 636 500 636T462 637H440Q393 637 386 627Q385 624 352 494T319 361Q319 360 388 360Q466 361 492 367Q556 377 592 426Q608 449 619 486T630 554Z"></path></g></g></g></svg></mjx-container>的高速缓存）。 根据探索策略从环境中采样转换，并将元组$\left(s<em>t, a_t, r_t, s</em>{t+1}\right)$存储在replay buffer 中。当buffer容量满时，就丢弃最    旧的样本，At each timestep the actor and critic are updated by sampling a minibatch ==uniformly==from the buffer.</p>
<p>Because DDPG is an off-policy algorithm, the replay buffer can be large, allowing the algorithm to benefit from learning across a set of uncorrelated transitions.<strong>（？）</strong></p>
<p>2.高估问题。用神经网络直接实现Q学习（方程4）在许多环境中被证明是不稳定的。使用target network，不过改进是用soft target network。</p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202212081113762.png" alt="image-20221208111329689"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202212081113692.png" alt="image-20221208111342650"></p>
<p><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="18.72ex" height="2.283ex" role="img" focusable="false" viewBox="0 -759 8274.4 1009"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"></path></g><g data-mml-node="TeXAtom" transform="translate(502,363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="2032" d="M79 43Q73 43 52 49T30 61Q30 68 85 293T146 528Q161 560 198 560Q218 560 240 545T262 501Q262 496 260 486Q259 479 173 263T84 45T79 43Z"></path></g></g></g><g data-mml-node="mo" transform="translate(1024.2,0)"><path data-c="2190" d="M944 261T944 250T929 230H165Q167 228 182 216T211 189T244 152T277 96T303 25Q308 7 308 0Q308 -11 288 -11Q281 -11 278 -11T272 -7T267 2T263 21Q245 94 195 151T73 236Q58 242 55 247Q55 254 59 257T73 264Q121 283 158 314T215 375T247 434T264 480L267 497Q269 503 270 505T275 509T288 511Q308 511 308 500Q308 493 303 475Q293 438 278 406T246 352T215 315T185 287T165 270H929Q944 261 944 250Z"></path></g><g data-mml-node="mi" transform="translate(2302,0)"><path data-c="1D70F" d="M39 284Q18 284 18 294Q18 301 45 338T99 398Q134 425 164 429Q170 431 332 431Q492 431 497 429Q517 424 517 402Q517 388 508 376T485 360Q479 358 389 358T299 356Q298 355 283 274T251 109T233 20Q228 5 215 -4T186 -13Q153 -13 153 20V30L203 192Q214 228 227 272T248 336L254 357Q254 358 208 358Q206 358 197 358T183 359Q105 359 61 295Q56 287 53 286T39 284Z"></path></g><g data-mml-node="mi" transform="translate(2819,0)"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"></path></g><g data-mml-node="mo" transform="translate(3510.2,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mo" transform="translate(4510.5,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mn" transform="translate(4899.5,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mo" transform="translate(5621.7,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mi" transform="translate(6621.9,0)"><path data-c="1D70F" d="M39 284Q18 284 18 294Q18 301 45 338T99 398Q134 425 164 429Q170 431 332 431Q492 431 497 429Q517 424 517 402Q517 388 508 376T485 360Q479 358 389 358T299 356Q298 355 283 274T251 109T233 20Q228 5 215 -4T186 -13Q153 -13 153 20V30L203 192Q214 228 227 272T248 336L254 357Q254 358 208 358Q206 358 197 358T183 359Q105 359 61 295Q56 287 53 286T39 284Z"></path></g><g data-mml-node="mo" transform="translate(7138.9,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="msup" transform="translate(7527.9,0)"><g data-mml-node="mi"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"></path></g><g data-mml-node="TeXAtom" transform="translate(502,363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="2032" d="M79 43Q73 43 52 49T30 61Q30 68 85 293T146 528Q161 560 198 560Q218 560 240 545T262 501Q262 496 260 486Q259 479 173 263T84 45T79 43Z"></path></g></g></g></g></g></svg></mjx-container></p>
<p>采用部分更新和软更新的方式能大大提高训练稳定性，但是也会因此产生训练缓慢的问题，不过在实际中训练的稳定性的重要性可以忽略掉这一点。</p>
<p>3.不同特征含义有不同的含义，变化范围不一样，很难找到一个泛化的超参数。解决方法，BN。将每个维度分别就行归一化，用均值和方差。在测试的时候，是维护一个均值和方差的平均。</p>
<p>4.A major challenge of learning in continuous action spaces is <code>exploration</code>.An advantage of <code>off-
policies</code> algorithms such as DDPG is that we can treat the problem of exploration <code>independently</code><br>from the learning algorithm.</p>
<p>We constructed an exploration policy µ by adding noise sampled from a noise process N to our actor policy <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.791ex;" xmlns="http://www.w3.org/2000/svg" width="23.693ex" height="2.713ex" role="img" focusable="false" viewBox="0 -849.5 10472.3 1199"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><path data-c="1D707" d="M58 -216Q44 -216 34 -208T23 -186Q23 -176 96 116T173 414Q186 442 219 442Q231 441 239 435T249 423T251 413Q251 401 220 279T187 142Q185 131 185 107V99Q185 26 252 26Q261 26 270 27T287 31T302 38T315 45T327 55T338 65T348 77T356 88T365 100L372 110L408 253Q444 395 448 404Q461 431 491 431Q504 431 512 424T523 412T525 402L449 84Q448 79 448 68Q448 43 455 35T476 26Q485 27 496 35Q517 55 537 131Q543 151 547 152Q549 153 557 153H561Q580 153 580 144Q580 138 575 117T555 63T523 13Q510 0 491 -8Q483 -10 467 -10Q446 -10 429 -4T402 11T385 29T376 44T374 51L368 45Q362 39 350 30T324 12T288 -4T246 -11Q199 -11 153 12L129 -85Q108 -167 104 -180T92 -202Q76 -216 58 -216Z"></path></g><g data-mml-node="TeXAtom" transform="translate(636,363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="2032" d="M79 43Q73 43 52 49T30 61Q30 68 85 293T146 528Q161 560 198 560Q218 560 240 545T262 501Q262 496 260 486Q259 479 173 263T84 45T79 43Z"></path></g></g></g><g data-mml-node="mrow" transform="translate(1047.1,0)"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msub" transform="translate(389,0)"><g data-mml-node="mi"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path></g><g data-mml-node="mi" transform="translate(502,-150) scale(0.707)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g></g><g data-mml-node="mo" transform="translate(1196.3,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g><g data-mml-node="mo" transform="translate(2910.2,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mi" transform="translate(3965.9,0)"><path data-c="1D707" d="M58 -216Q44 -216 34 -208T23 -186Q23 -176 96 116T173 414Q186 442 219 442Q231 441 239 435T249 423T251 413Q251 401 220 279T187 142Q185 131 185 107V99Q185 26 252 26Q261 26 270 27T287 31T302 38T315 45T327 55T338 65T348 77T356 88T365 100L372 110L408 253Q444 395 448 404Q461 431 491 431Q504 431 512 424T523 412T525 402L449 84Q448 79 448 68Q448 43 455 35T476 26Q485 27 496 35Q517 55 537 131Q543 151 547 152Q549 153 557 153H561Q580 153 580 144Q580 138 575 117T555 63T523 13Q510 0 491 -8Q483 -10 467 -10Q446 -10 429 -4T402 11T385 29T376 44T374 51L368 45Q362 39 350 30T324 12T288 -4T246 -11Q199 -11 153 12L129 -85Q108 -167 104 -180T92 -202Q76 -216 58 -216Z"></path></g><g data-mml-node="mrow" transform="translate(4735.6,0)"><g data-mml-node="mo" transform="translate(0 -0.5)"><path data-c="28" d="M152 251Q152 646 388 850H416Q422 844 422 841Q422 837 403 816T357 753T302 649T255 482T236 250Q236 124 255 19T301 -147T356 -251T403 -315T422 -340Q422 -343 416 -349H388Q359 -325 332 -296T271 -213T212 -97T170 56T152 251Z"></path></g><g data-mml-node="msub" transform="translate(458,0)"><g data-mml-node="mi"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path></g><g data-mml-node="mi" transform="translate(502,-150) scale(0.707)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g></g><g data-mml-node="mo" transform="translate(1543,0)"><path data-c="2223" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="msubsup" transform="translate(2098.8,0)"><g data-mml-node="mi"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"></path></g><g data-mml-node="mi" transform="translate(502,506.3) scale(0.707)"><path data-c="1D707" d="M58 -216Q44 -216 34 -208T23 -186Q23 -176 96 116T173 414Q186 442 219 442Q231 441 239 435T249 423T251 413Q251 401 220 279T187 142Q185 131 185 107V99Q185 26 252 26Q261 26 270 27T287 31T302 38T315 45T327 55T338 65T348 77T356 88T365 100L372 110L408 253Q444 395 448 404Q461 431 491 431Q504 431 512 424T523 412T525 402L449 84Q448 79 448 68Q448 43 455 35T476 26Q485 27 496 35Q517 55 537 131Q543 151 547 152Q549 153 557 153H561Q580 153 580 144Q580 138 575 117T555 63T523 13Q510 0 491 -8Q483 -10 467 -10Q446 -10 429 -4T402 11T385 29T376 44T374 51L368 45Q362 39 350 30T324 12T288 -4T246 -11Q199 -11 153 12L129 -85Q108 -167 104 -180T92 -202Q76 -216 58 -216Z"></path></g><g data-mml-node="mi" transform="translate(502,-269) scale(0.707)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g></g><g data-mml-node="mo" transform="translate(3077.2,0) translate(0 -0.5)"><path data-c="29" d="M305 251Q305 -145 69 -349H56Q43 -349 39 -347T35 -338Q37 -333 60 -307T108 -239T160 -136T204 27T221 250T204 473T160 636T108 740T60 807T35 839Q35 850 50 850H56H69Q197 743 256 566Q305 425 305 251Z"></path></g></g><g data-mml-node="mo" transform="translate(8493,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(9493.3,0)"><g data-mml-node="mi"><path data-c="4E" d="M343 705Q358 705 358 698Q360 696 370 658T411 524T484 319Q536 174 590 82L595 73L615 152Q646 274 683 407Q729 571 752 637T799 727Q852 780 937 788Q939 788 947 788T958 789H962Q979 789 979 765Q979 722 951 692Q942 683 924 683Q888 681 859 672T818 654T803 639Q784 608 708 322T631 15Q631 14 630 15Q630 17 629 15Q628 14 628 12Q621 -4 601 -17T560 -31Q550 -31 546 -28T530 -7Q484 67 458 123T398 272Q352 392 314 514L306 535V534Q306 533 296 488T272 379T234 239T185 100T127 -7T61 -50Q34 -50 4 -34T-27 8Q-27 33 -12 61T18 90Q21 90 36 77T87 57H92Q109 57 123 78T162 173Q206 299 232 417T265 599T276 667Q284 681 304 693T343 705Z"></path></g></g></g></g></svg></mjx-container></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202212081527001.png" alt="image-20221208152734908"></p>
<h4 id="4-RESULTS"><a href="#4-RESULTS" class="headerlink" title="4 RESULTS"></a>4 RESULTS</h4><p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202212081531924.png" alt="image-20221208153123842"></p>
<p>BN（浅灰色）的原始DPG算法（Minibatch NFQCA）</p>
<p>带target network（深灰色）的原始DPG算法</p>
<p>以target network和BN（绿色）、</p>
<p>target network仅像素输入（蓝色）</p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202212081654036.png" alt="image-20221208165445923"></p>
<h4 id="6-CONCLUSION"><a href="#6-CONCLUSION" class="headerlink" title="6 CONCLUSION"></a>6 CONCLUSION</h4><p>The work combines insights from recent advances in deep learning and reinforcement learning, resulting in an algorithm that robustly solves challenging problems across a variety of domains with continuous action spaces, even when using raw pixels for observations</p>
<h3 id="3-Cross-Domain-Adaptive-Transfer-Reinforcement-Learning-Based-on-State-Action-Correspondence"><a href="#3-Cross-Domain-Adaptive-Transfer-Reinforcement-Learning-Based-on-State-Action-Correspondence" class="headerlink" title="3 Cross-Domain Adaptive Transfer Reinforcement Learning Based on State-Action Correspondence"></a>3 Cross-Domain Adaptive Transfer Reinforcement Learning Based on State-Action Correspondence</h3><p>tag:<code>transfer RL</code></p>
<h4 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h4><p>以往的工作大多考虑具有相同状态-动作空间的任务之间的TL，而在具有不同状态-动作空间的领域之间的迁移则相对较少。此外，这种现有的跨域传输方法只允许从单个源策略进行传输，留下了如何从多个源策略进行最佳传输的重要问题。本文提出了一种新的框架<code>Cross-domain Adaptive Transfer (CAT)</code>来加速DRL。CAT学习每个源任务到目标任务的状态-动作对应关系，并自适应地将多个源任务策略的知识转移到目标策略。实验结果表明，在多个连续动作控制任务上，CAT算法能显著地加快学习速度，优于其他跨域迁移算法。项目的代码发布在<a target="_blank" rel="noopener" href="https://github.com/tju-drl-lab/transfer-andmulti-task-restruction-learning的项目页面下。">https://github.com/tju-drl-lab/transfer-andmulti-task-restruction-learning的项目页面下。</a></p>
<h4 id="1-INTRODUCTION-1"><a href="#1-INTRODUCTION-1" class="headerlink" title="1 INTRODUCTION"></a>1 INTRODUCTION</h4><p>尽管DRL在很多领域都获得了成功，但是仍然面临着采样效率低下的问题，需要与环境进行大量交互。迁移学习(TL)作为一种利用先验知识加速学习过程的技术，已成为显著降低样本复杂度的研究方向之一。</p>
<p>RL中迁移的一个主要分支侧重于利用来自预先训练的源任务策略的外部知识，我们称之为策略转移（policy transfer）。这些方法要么通过模仿学习从源策略中提取知识，或者基于对目标环境的源策略的评估重用源策略进行探索。然而，所有这些方法都需要相同的假设，即源任务与目标任务共享相同的状态-动作空间，以便可以直接模仿或重用源策略。</p>
<p>训练state encoder</p>
<p>本文解决了学习从具有不同状态-动作空间的多个任务转移的更困难的情况。</p>
<p>我们提出了一种新的跨域自适应传输框架(CAT)，它可以自适应地传输具有不同状态-动作空间的多个源策略</p>
<p>与以往的工作不同，我们不需要配对数据来学习状态-动作对应关系，也不需要学习训练不足的状态对应关系，相反，CAT通过使用源策略轨迹的状态编码器、动作编码器和反向状态编码器来学习从每个源域到目标域的状态-动作对应关系。由于无法访问源环境以获取更多信息，因此我们不需要反向操作编码器来获取源环境上的操作。此外，CAT评估目标任务上的每个源策略，并了解每个源策略对目标策略的帮助程度，然后使用性能作为度量来确定何时以及哪些源策略应该被转移</p>
<p>主要贡献：</p>
<ul>
<li>本文提出的转移框架CAT由Agent模块、自适应模块和修正模块三个主要部分组成，以解决具有不同状态-动作空间的多源策略的自适应知识转移问题。</li>
<li>CAT使用修正模块和代理模块学习更充分训练的状态嵌入和动作嵌入，作为后续传输过程的基础。</li>
<li>CAT使用自适应模块生成的自适应加权因子将来自源策略网络的知识与目标策略网络相结合。</li>
<li>CAT算法与已有的DRL算法很容易结合，实验结果表明，在不同状态-动作空间的连续控制任务中，CAT算法有效地加速了RL算法，并优于其他相关的转移算法。</li>
</ul>
<h4 id="2-BACKGROUND-1"><a href="#2-BACKGROUND-1" class="headerlink" title="2 BACKGROUND"></a>2 BACKGROUND</h4><p>策略梯度(PG)算法。 策略梯度法被广泛应用于直接优化以θ为参数的策略π。 近程政策优化(PPO，Schulman et al.[2017])是目前最高效的PG方法之一。</p>
<p>同域迁移学习问题，</p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202212241343202.png" alt="image-20221224134341111"></p>
<p>跨域迁移学习</p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202212241344801.png" alt="image-20221224134432769"></p>
<p>这篇论文考虑了多个源任务与一个目标任务之间的跨域转移问题。</p>
<p>我们通常假设MDP之间有一些高层次的共性（例如，四足、六足和八足机器人可能有质量相似的步态）</p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202212241348866.png" alt="image-20221224134846837"></p>
<h4 id="3-METHODOLOGY"><a href="#3-METHODOLOGY" class="headerlink" title="3 METHODOLOGY"></a>3 METHODOLOGY</h4><p>在本节中，我们首先介绍我们的整个框架和每个组件。 然后，我们描述了如何学习状态和动作嵌入，以及如何自适应地将多个跨域源策略转移到目标任务。 最后，我们详细描述了CAT与一种特定的DRL算法PPO[Schulman et al.，2017]的结合</p>
<p><strong>3.1 FRAMEWORK OVERVIEW</strong></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202212241542598.png" alt="image-20221224154210522"></p>
<p>Correction Module</p>
<p>The goal of the correction module is to learn embeddings to distill knowledge<br>from multiple source policies into the target task.</p>
<p>Self-Adaptation Module</p>
<p><strong>3.2 LEARNING STATE-ACTION CORRESPONDENCE</strong></p>
<p><strong>3.3 ADAPTIVE POLICY TRANSFER</strong></p>
<p><strong>3.4 CAT-PPO</strong></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202212291200593.png" alt="image-20221229120025531"></p>
<h4 id="4-EXPERIMENTS"><a href="#4-EXPERIMENTS" class="headerlink" title="4 EXPERIMENTS"></a>4 EXPERIMENTS</h4><p>()</p>
<h3 id="4-Adversarially-Trained-Actor-Critic-for-Offline-Reinforcement-Learning"><a href="#4-Adversarially-Trained-Actor-Critic-for-Offline-Reinforcement-Learning" class="headerlink" title="4 Adversarially Trained Actor Critic for Offline Reinforcement Learning"></a>4 Adversarially Trained Actor Critic for Offline Reinforcement Learning</h3><p><strong>Prior Work</strong></p>
<p>To address such a challenge, the recent works <em>(Fujimoto et al., 2019; Laroche et al., 2019; Jaques et al., 2019; Wu et al., 2019; Kumar et al., 2019, 2020; Agarwal et al., 2020b; Yu et al., 2020; Kidambi et al., 2020; Wang et al., 2020c; Siegel et al., 2020; Nair et al., 2020; Liu et al., 2020)</em> demonstrate the empirical success of various algorithms, which fall into two (possibly overlapping) categories: ==(i) regularized policy-based approaches==and ==(ii) pessimistic value-based approaches==. Specifically, (i) regularizes (or equivalently, constrains) the policy to avoid visiting the states and actions that are less covered by the dataset, while (ii) penalizes the (action- or state-) value function on such states and actions.</p>
<p>In offline/batch reinforcement learning (RL), the predominant class of approaches with most      success have been “==support constraint==” methods, ==where trained policies are encouraged to remain    within the support of the provided offline dataset==.However, support constraints correspond to an overly <code>pessimistic assumption that actions outside the provided data may lead to worst-case outcomes.</code></p>
<p>Offline RL的解决思路无外乎就是“悲观”二字，也就要想办法对OOD action进行低估，不要让策略走到没见到过的动作上去。为了实现这种悲观，实际的算法大体分为两种思路：一种是把OOD action的值直接拉低，代表性的方法就是CQL；另一种是让被训练的策略和采样策略不要差得太远，这类方法有AWAC，TD3+BC等。</p>
<p>Stackelberg Game，即斯塔克伯格博弈，是一个两阶段的完全信息动态博弈，博弈的time是序贯的。<code>主要思想是双方都是根据对方可能的策略来选择自己的策略以保证自己在对方策略下的利益最大化，从而达到纳什均衡</code>。在该博弈模型中，先作出决策的一方被称为leader，在leader之后，剩余的players根据leader的决策进行决策，被称为followers，然后leader再根据followers的决策对自己的决策进行调整，如此往复，直到达到纳什均衡。(斯塔克伯格博弈（Stackelberg game）是一种博弈理论中的概念，是一种多人博弈的形式，在这种博弈中，有一个明确的领导者（称为领导者）和若干个追随者（称为追随者）。领导者先决策，然后追随者决策。领导者考虑追随者的反应，试图通过最优决策来最大化自己的利益。追随者则根据领导者的决策来决策，试图最大化自己的利益。</p>
<p>斯塔克伯格博弈的应用广泛，如市场竞争、广告定价、生产决策等。在实际应用中，斯塔克伯格博弈可以帮助我们了解决决策者之间的博弈关系，从而帮助决策者做出更好的决策。)</p>
<p>零和博弈是一种博弈理论的概念，它指的是一种博弈模型，在这种模型中，双方的利益是对立的，每个人的目的是赢得比对手多的利益，换句话说，没有公共利益可以分享。因此，在零和博弈中，一个人的胜利必须是另一个人的失败。零和博弈是研究博弈策略和决策理论的重要工具，广泛应用于经济学、政治学、军事学和生物学等领域。</p>
<p>深度离线强化学习（deep offline RL）可以通过利用深度神经网络和巨大的离线数据集，在没有任何环境交互的情况下训练强大的agent，但是训练得到的offline RL agents可能是次优的，因为offline datasets可能是次优的，另外，agent部署的环境可能与生成offline datasets的环境不同，这就需要一个在线微调（online fine-tuning）过程，agent通过在线收集更多的样本来改进。</p>
<p>但是使用传统的off-policy RL算法微调offline RL agent比较困难，因为存在distribution shift，agent会遇见offline dataset中没有的state-action，即out-of-distribution (OOD) online samples，Q函数在这样的state-action上无法给出准确的估计，导致严重的bootstrap error，从而使策略在任意方向上更新，破坏了offline RL获得的良好初始策略</p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202302141237228.png" alt="image-20230214123740154"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202302141237759.png" alt="image-20230214123753685"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202302141239948.png" alt="image-20230214123951879"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202302141421428.png" alt="image-20230214142157358"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202302141424525.png" alt="image-20230214142417458"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202302141426452.png" alt="image-20230214142656394"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202302141430334.png" alt="image-20230214143053270"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202302141431590.png" alt="image-20230214143134519"></p>
<p>Bi-level Actor-Critic for Multi-agent Coordination<br><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1909.03510">https://arxiv.org/abs/1909.03510</a></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202302141441780.png" alt="image-20230214144122705"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202302141444075.png" alt="image-20230214144419961"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202302141445122.png" alt="image-20230214144502035"></p>
<h4 id="Abstract-1"><a href="#Abstract-1" class="headerlink" title="Abstract"></a>Abstract</h4><p>mark：3.2. Relative Pessimism and Robust Policy Improvement</p>
<h3 id="5-Constrained-Variational-Policy-Optimization-for-Safe-Reinforcement-Learning"><a href="#5-Constrained-Variational-Policy-Optimization-for-Safe-Reinforcement-Learning" class="headerlink" title="5 Constrained Variational Policy Optimization for Safe Reinforcement Learning"></a>5 Constrained Variational Policy Optimization for Safe Reinforcement Learning</h3><h3 id="6-Multi-Agent-Deep-Reinforcement-Learning-for-Joint-Decoupled-User-Association-and-Trajectory-Design"><a href="#6-Multi-Agent-Deep-Reinforcement-Learning-for-Joint-Decoupled-User-Association-and-Trajectory-Design" class="headerlink" title="6 Multi-Agent Deep Reinforcement Learning for Joint Decoupled User Association and Trajectory Design"></a>6 Multi-Agent Deep Reinforcement Learning for Joint Decoupled User Association and Trajectory Design</h3><p><strong>摘要</strong>在多无人机网络中，无人机和用户设备（UE）之间的下行链路（DL）和上行链路（UL）关联通常是耦合的，这限制了每个UE在DL和UL中都与同一个无人机关联。然而，这种模式可能并不高效，因为无人机网络可能是异构的（例如，多层无人机网络），并且由于无人机的移动性，可能会出现高链路不确定性。在多无人机网络中引入全双工通信会使无人机联盟进一步复杂化。出于这个原因，本工作中引入了DL-UL解耦（DUDe）的想法，允许每个UE与不同的UAV联合进行UL和DL传输。此外，UE-UAV的关联取决于UAV的飞行轨迹，这使得DUDe的设计具有挑战性。在本文中，我们研究了全双工多无人机网络的联合解耦UL-DL关联和轨迹设计问题。本文提出了一个联合优化问题，目标是使UE在UL和DL的总速率最大化。由于该问题是具有复杂状态的非凸性问题，而且单个无人机可能不知道其他无人机的奖励函数，因此提出了一个稳健的部分可观察马尔科夫决策过程（POMDP）模型来描述模型的不确定性。提出了一种多代理深度强化学习（MADRL）方法，使每个无人机能够以分布式方式选择其政策。为了训练MADRL方法中的行为批评神经网络，开发了一种改进的基于片段和计数的近似策略优化（PPO）算法。特别是，设计了一个修正的片段分布来处理当前和旧政策之间的硬性限制，并引入了内在的奖励来增强探索能力。仿真结果表明，与基准相比，我们提出的方案具有优越性。这些代码在GitHub（<a target="_blank" rel="noopener" href="https://github.com/isdai/MADRLPPO）上公开提供。">https://github.com/isdai/MADRLPPO）上公开提供。</a></p>
<p><strong>Index Terms</strong>-Unmanned aerial vehicle (UAV), in-band fullduplex communication, decoupled UL-DL association, trajectory design, multi-agent deep reinforcement learning, proximal policy optimization.</p>
<p>I. INTRODUCTION</p>
<p>该优化问题的目标是在给定的有限时间内最大化 UE 的平均总和速率。然而，由于非凸目标函数和 DL-UL 解耦以及带内 FD 通信产生的约束，利用集中优化技术为联合问题得出最优解是一项艰巨的任务。</p>
<p>最近，强化学习 (RL) [18] 框架已成为解决无人机网络中传统优化问题的流行方法 [19]-[21]。 RL 背后的基本思想是通过与动态环境交互来迭代学习和做出决策，以最大化代理的累积奖励。然而，大多数用于解决此类问题的基于 RL 的方法仅考虑单代理系统，这不适用于本工作中制定的联合问题。这是因为，存在大量可能的轨迹会导致计算复杂度高，并且学习过程会变得棘手。此外，由于单个无人机可能无法全局了解其他无人机的奖励函数（即由于估计的不确定性），因此这种单代理方法将面临不稳定的风险。为此，我们构想了一个鲁棒的部分可观察马尔可夫决策过程（鲁棒 POMDP）模型来表征不确定性，并采用多代理深度强化学习（MADRL）方法使每个代理（即无人机）能够在分布式方式。具体来说，利用深度神经网络 (DNN) 的强大学习和预测特性来指导代理的探索以提高学习率，并且在考虑多个代理时可以利用更多信息。为了获得接近最优的策略，开发了一种改进的基于剪辑和计数的近端策略优化 (PPO) 算法来训练 MADRL 模型中的 ActorCritic 神经网络。</p>
<p>The major novelty and contributions of the paper can be summarized as follows:</p>
<ul>
<li>To the best of our knowledge, this is the first work to devise a framework for DL-UL decoupling association mode for FD multi-UAV networks (referred to as DFA). An optimization problem is formulated for achieving the optimal sum-rate for UEs in both UL and DL by jointly considering the association between UEs and BSs and designing the UAVs’ trajectories. </li>
<li>Due to its non-convexity and the presence of reward uncertainty, the optimization problem is converted to a robust POMDP. In particular, each UAV is specified as an agent which can adapt its associating UEs and flight trajectory, without obtaining complete information from the time-varying multi-UAV environment.  </li>
<li>A distributed multi-agent DRL-based approach is designed to solve the robust POMDP problem, and an improved clip-and-count based PPO algorithm is developed to obtain a near-optimal policy. Specifically, a modified clip distribution is designed to deal with the hard restrictions between current and old policies, and an intrinsic reward is introduced to enhance the exploration capability. •</li>
<li>The practical implementation issues for the proposed approach are discussed, and extensive simulation results are provided to illustrate the superiority of the proposed scheme.</li>
</ul>
<p>in Full-Duplex Multi-UAV Networks<br>downlink (DL) uplink (UL)  multi-UAV networks  user equipment(UE) DL-UL decoupling(DUDe)  robust partially observable Markov decision process<br>(robust POMDP) model full-duplex (FD)<br>communication macro base station (MBS)</p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202302131129442.png" alt="image-20230213112927370"></p>
<p>Network Model and BSs Deployment</p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202302131232312.png" alt="image-20230213123238260"></p>
<p>Channel Model</p>
<h3 id="7-Reinforcement-Learning-Fast-and-Slow-Review"><a href="#7-Reinforcement-Learning-Fast-and-Slow-Review" class="headerlink" title="7 Reinforcement Learning, Fast and Slow[Review ]"></a>7 Reinforcement Learning, Fast and Slow[Review ]</h3><p>分析了RL采样效率低的两个原因：</p>
<ul>
<li><p>incremental parameter adjustment </p>
<p>一般学习率低下，是采样效率低的第一个原因，如果单存的增大梯度很容易造成对学习策略的过程造成灾难性的干扰</p>
</li>
<li><p>weak inductive biasexplain</p>
<p>神经网络具有弱归纳偏置，导致假设空间很大，这是RL采样效率低下的另一个原因。</p>
</li>
</ul>
<p>对应上述问题，本文给出两个解决方案</p>
<ul>
<li>Episodic Deep RL: Fast Learning through Episodic Memory</li>
<li>Meta-RL: Speeding up Deep RL by Learning to Learn</li>
</ul>
<p>最后</p>
<p>这两个方法的结合——Episodic Meta-RL。</p>
<p>总结：</p>
<p>学习一开始的冷启动总是很慢的，两个方案相同点都是利用过去经验，然后由慢而快，也就是说快速学习需要建立在缓慢学习的基础上。</p>
<h3 id="8-Benchmarking-Safe-Exploration-in-Deep-Reinforcement-Learning"><a href="#8-Benchmarking-Safe-Exploration-in-Deep-Reinforcement-Learning" class="headerlink" title="8 Benchmarking Safe Exploration in Deep Reinforcement Learning"></a>8 Benchmarking Safe Exploration in Deep Reinforcement Learning</h3><h4 id="Abstract-2"><a href="#Abstract-2" class="headerlink" title="Abstract"></a>Abstract</h4><p>强化学习 (RL) 代理需要探索其环境，以便通过反复试验学习最佳策略。在许多环境中，安全是一个关键问题，某些错误是不可接受的：例如，与人类互动的机器人系统在探索时绝不能对人类造成伤害。虽然目前通常主要或完全在模拟中训练 RL 代理，其中安全问题最小，但我们预计模拟现实世界的复杂性（例如人机交互）的挑战将导致直接训练 RL 代理的转变在安全问题至高无上的现实世界中。因此，我们认为安全探索应被视为 RL 研究的关键重点领域，在这项工作中，我们为推进安全探索研究做出了三项贡献。首先，基于之前关于安全强化学习的广泛工作，我们建议将受限强化学习标准化为安全探索的主要形式。其次，我们介绍了 Safety Gym 基准套件，这是一种新的高维连续控制环境，用于衡量受限 RL 的研究进展。最后，我们在 Safety Gym 环境中对几种受约束的深度 RL 算法进行基准测试，以建立未来工作可以建立的基线</p>
<h4 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h4><p>RL 的基本原理是代理，即 AI 系统，试图通过反复试验来最大化奖励信号。 RL 适用于评估行为（通过计算奖励函数）比生成最佳行为（例如通过分析或数值方法）更容易的任何问题。</p>
<p>The safe exploration problem is a natural consequence of the trial-and-error nature of RL: agents will sometimes try dangerous or harmful behaviors in the course of learning.</p>
<p>When all training occurs in a simulator, this is usually not concerning, but exploration of this kind in the real world could produce unacceptable catastrophes. To illustrate safety concerns in a few domains where RL might plausibly be applied:</p>
<p>• Robots and autonomous vehicles should not cause physical harm to humans. </p>
<p>• AI systems that manage power grids should not damage critical infrastructure. </p>
<p>• Question-answering systems should not provide false or misleading answers for questions about medical emergencies [Bickmore et al., 2018].</p>
<p> • Recommender systems should not expose users to psychologically harmful or extremist content [Vendrov and Nixon, 2019].</p>
<p>A central question for the field of RL is therefore:</p>
<p> How do we formulate safety specifications to incorporate them into RL, and how do we ensure that these specifications are robustly satisfied throughout exploration?</p>
<p>The goal of our work is to facilitate progress on this question on several fronts.</p>
<ul>
<li><p>Towards standardizing safety specifications:</p>
</li>
<li><p>Towards measuring progress</p>
<p>. To address the gap, we present Safety Gym: a set of tools for accelerating safe exploration research. Safety Gym includes a benchmark suite of 18 high-dimensional continuous control environments for safe exploration, plus 9 additional environments for debugging task performance separately from safety requirements, and tools for building additional environments.</p>
</li>
<li><p>Towards providing useful baselines:</p>
</li>
</ul>
<p>We recommend a protocol for evaluating constrained RL algorithms on Safety Gym environments based on <strong>three metrics:</strong></p>
<ul>
<li>task performance of the final policy,</li>
<li>constraint satisfaction of the final policy</li>
<li>and average regret with respect to safety costs throughout training.</li>
</ul>
<p>We highlight three particularly desirable features of Safety Gym: </p>
<ol>
<li>There is a gradient of difficulty across benchmark environments. This allows practitioners to quickly iterate on the simplest tasks before proceeding to the hardest ones.</li>
<li>In all Safety Gym benchmark environments, the layout of environment elements is randomized at the start of each episode. Each distribution over layouts is continuous and minimally restricted, allowing for essentially infinite variations within each environment. This prevents RL algorithms from learning trivial solutions that memorize particular trajectories, and requires agents to learn more-general behaviors to succeed.</li>
<li>Safety Gym is highly extensible. The tools used to build Safety Gym allow the easy creation of new environments with different layout distributions, including combinations of constraints not present in our standard benchmark environments.</li>
</ol>
<p><strong>Surprisingly, we find that CPO performs poorly on Safety Gym environments by comparison to Lagrangian methods.</strong></p>
<h4 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2 Related Work"></a>2 Related Work</h4><h4 id="3-Safe-Reinforcement-Learning-Via-Constraints"><a href="#3-Safe-Reinforcement-Learning-Via-Constraints" class="headerlink" title="3 Safe Reinforcement Learning Via Constraints"></a>3 Safe Reinforcement Learning Via Constraints</h4><p>3.1 What is Constrained RL?</p>
<p>Let <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.375ex;" xmlns="http://www.w3.org/2000/svg" width="3.1ex" height="1.913ex" role="img" focusable="false" viewBox="0 -680 1370.4 845.6"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="3A0" d="M128 619Q121 626 117 628T101 631T58 634H25V680H724V634H691Q651 633 640 631T622 619V61Q628 51 639 49T691 46H724V0H713Q692 3 569 3Q434 3 425 0H414V46H447Q489 47 498 49T517 61V634H232V348L233 61Q239 51 250 49T302 46H335V0H324Q303 3 180 3Q45 3 36 0H25V46H58Q100 47 109 49T128 61V619Z"></path></g><g data-mml-node="mi" transform="translate(783,-150) scale(0.707)"><path data-c="1D436" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z"></path></g></g></g></g></svg></mjx-container> denote a feasible set of constraint-satisfying policies, and for the moment, put aside the question of how it is constructed. An optimal policy in constrained RL is given by:</p>
<script type="math/tex; mode=display">
\pi^*=\arg\max\limits_{\pi\in\Pi_C}J_r(\pi),</script><p>where <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="5.215ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 2304.9 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D43D" d="M447 625Q447 637 354 637H329Q323 642 323 645T325 664Q329 677 335 683H352Q393 681 498 681Q541 681 568 681T605 682T619 682Q633 682 633 672Q633 670 630 658Q626 642 623 640T604 637Q552 637 545 623Q541 610 483 376Q420 128 419 127Q397 64 333 21T195 -22Q137 -22 97 8T57 88Q57 130 80 152T132 174Q177 174 182 130Q182 98 164 80T123 56Q115 54 115 53T122 44Q148 15 197 15Q235 15 271 47T324 130Q328 142 387 380T447 625Z"></path></g><g data-mml-node="mi" transform="translate(588,-150) scale(0.707)"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(956.9,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1345.9,0)"><path data-c="1D70B" d="M132 -11Q98 -11 98 22V33L111 61Q186 219 220 334L228 358H196Q158 358 142 355T103 336Q92 329 81 318T62 297T53 285Q51 284 38 284Q19 284 19 294Q19 300 38 329T93 391T164 429Q171 431 389 431Q549 431 553 430Q573 423 573 402Q573 371 541 360Q535 358 472 358H408L405 341Q393 269 393 222Q393 170 402 129T421 65T431 37Q431 20 417 5T381 -10Q370 -10 363 -7T347 17T331 77Q330 86 330 121Q330 170 339 226T357 318T367 358H269L268 354Q268 351 249 275T206 114T175 17Q164 -11 132 -11Z"></path></g><g data-mml-node="mo" transform="translate(1915.9,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container> is a reward-based objective function. As in standard RL, the objective is usually either the infinite-horizon discounted return, the finite-horizon undiscounted return, or the infinite-horizon average reward.</p>
<p>The framework of constrained Markov Decision Processes (CMDPs) [Altman, 1999] is the de-facto standard for describing feasible sets in constrained RL. CMDPs are equipped with a set of cost functions, c1, …, ck, separate from the reward function. The feasible set in a CMDP is given by</p>
<script type="math/tex; mode=display">
\Pi_C=\{\pi:J_{c_i}(\pi)\le d_i,i=1,...,k\},</script><p>where each <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.592ex;" xmlns="http://www.w3.org/2000/svg" width="2.659ex" height="2.137ex" role="img" focusable="false" viewBox="0 -683 1175.4 944.6"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D43D" d="M447 625Q447 637 354 637H329Q323 642 323 645T325 664Q329 677 335 683H352Q393 681 498 681Q541 681 568 681T605 682T619 682Q633 682 633 672Q633 670 630 658Q626 642 623 640T604 637Q552 637 545 623Q541 610 483 376Q420 128 419 127Q397 64 333 21T195 -22Q137 -22 97 8T57 88Q57 130 80 152T132 174Q177 174 182 130Q182 98 164 80T123 56Q115 54 115 53T122 44Q148 15 197 15Q235 15 271 47T324 130Q328 142 387 380T447 625Z"></path></g><g data-mml-node="TeXAtom" transform="translate(588,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"></path></g><g data-mml-node="mi" transform="translate(466,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g></g></g></g></svg></mjx-container> is a cost-based constraint function defined the same way as an expected return or average return metric (using ci instead of the reward r), and each di is a threshold (a human-selected hyperparameter)</p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202305061039279.png" alt="image-20230506103913161"></p>
<p>Later, when describing our evaluation protocol for benchmarking constrained RL algorithms in Safety Gym, we will make the case that cost rate (the average cost over the entirety of training) is a suitable regret measure.</p>
<p>3.2 Constrained RL and Safe Exploration </p>
<p>Contrast this with standard reinforcement learning, which just maximizes a reward function. In order to design hazard-avoiding behavior into an agent through a scalar reward function, a designer would have to carefully select a trade-off between a reward for task-solving behavior and a penalty for proximity to hazards.(在奖励函数里面加一个惩罚项，但是也会带来一些问题，如下)</p>
<p>There are two problems with this: 1) There is no invertible map between “desired safety specification” and “correct trade-off parameter” that can be checked before running an RL algorithm. If the designer selects a penalty that is too small, the agent will learn unsafe behavior, and if the penalty is too severe, the agent may fail to learn anything. 2) A fixed trade-off, even one that results in a hazard-avoiding policy at optimum, does not account for a requirement to satisfy safety requirements throughout training. Both of these problems have been observed in practice, for example by Achiam et al. [2017], Pham et al. [2018], and Dalal et al. [2018].</p>
<p>The choice to formulate safety requirements as constraints, and to attain constraint-satisfying exploration, resolves both.（解决方法，显示写出约束，并且在约束的可行域内进行探索）</p>
<p>3.3 Addressing Critiques of Constrained RL</p>
<p>然而，受约束的 RL 并未被普遍视为 RL 安全的关键组成部分，我们认为重要的是要解决为什么会这样。高级 RL 系统（尤其是通用人工智能 (AGI)）安全性的一个核心问题与代理对齐有关：确保代理的行为符合用户意图的问题。 （在这里，我们引用了 Leike et al. [2018] 的特定术语和措辞，但这个问题几十年来一直以各种形式被考虑。）在 RL 中，这主要表现为奖励规范中的一个问题，看似正确但错误指定的奖励函数会导致不正确和不安全的代理行为 [Clark and Amodei, 2016]。约束 RL 的框架是否有助于解决这个问题并不明显，因为约束 RL 仍然需要规范不仅是奖励函数，而且通常还需要约束的成本函数。因此，批评是设计约束函数时的错误可能会导致不安全的代理，因此受约束的 RL 只是在移动对齐问题而不是解决它。==(该声明指的是对受限强化学习（RL）的批评，这是人工智能（AI）的一个子领域，旨在开发能够学习在复杂环境中做出决定的算法，同时尊重某些约束或目标。批评认为，虽然有约束的RL可能有效地确保代理在预定的界限内安全行事，但它并没有解决使代理的目标与人类用户的目标相一致这一更大的挑战。换句话说，虽然有约束的RL可能能够防止代理人以违反某些约束的方式行事，但它并不能保证代理人的目标与人类的目标一致。因此，该评论认为，受约束的RL可能只是将协调问题转移，而不是解决它，因为它没有解决确保代理人追求有益的、与人类价值一致的目标这一根本挑战。)==</p>
<p>人工智能安全研究中的一个主流向量试图通过使用人类数据来为训练代理推导出合适的目标或奖励函数来解决对齐问题。这一系列方法包括合作逆向强化学习 [Hadfield-Menell 等人，2016 年]、从二元或排序偏好中学习 [Christiano 等人，2017 年]、迭代放大和蒸馏 [Christiano 等人，2018 年]、AI 安全通过辩论 [Irving et al., 2018] 和递归奖励模型 [Leike et al., 2018]。其他方法试图规范代理人的影响，基于代理人应该更喜欢具有最小副作用的任务解决方案 [Krakovna et al., 2018] 或与环境初始状态中隐含的偏好最小矛盾的先验 [Shah et al. , 2019]。总的来说，这种安全工作向量旨在消除明确设计安全规范的需要，因为手工制定的规范会以各种方式失败（例如，通过省略对某些不安全行为的惩罚，或通过无意中激励有害行为） .</p>
<p>我们认为约束的使用与这些解决对齐问题的数据驱动方法兼容并互补。简单地说，从人类数据中学习奖励函数的技术也可用于学习约束的成本函数。此外，我们推测约束的使用确实可以提高 1) 在任务之间学习和转移安全规范的容易程度，以及 2) 代理达到这些安全要求的稳健性。</p>
<p>为了支持这些猜想，我们注意到当学习算法利用特定于问题结构的先验知识时，它们通常会通过在更窄且更有用的集合中搜索解决方案来提高样本效率。我们认为将代理行为规范划分为“做”和“不做”（分别是奖励函数和约束函数）对于安全来说是一种有用的先验。此外，虽然学习到的一项任务的奖励函数可能无法转移到另一项任务（例如，用于组装小部件的奖励函数可能很少描述如何组装装饰物），<strong>但学习到的描述不可接受行为的约束函数似乎更有可能成功转移</strong>.例如，无论智能体的任务是构建什么，“不要对人进行身体攻击”的成本函数都是相关的。（学习到的cost网络迁移到别的任务中能否起到比较好的效果）</p>
<p>3.4 Remarks on Alternate Approaches</p>
<p>我们认为，某些其他不受约束的安全强化学习方法要么不够充分，要么不切实际。仅关注单个标量奖励函数（其中此类标量奖励函数在训练过程中保持固定）的回报度量的安全方法——作为预期回报的单调改进、回报方差的约束、高于回报的回报最小值，或返回的风险度量——不适当地将任务性能规范和安全规范混为一谈，因此由于前面描述的原因是不充分的。我们认为有缺陷的另一种常见方法侧重于遍历性：如果代理永远不会进入无法返回的状态，即如果每个错误都是可逆的，则代理被认为是安全的 [Moldovan and Abbeel, 2012, Eysenbach et al., 2018]。虽然在机器人技术中出现的一些实际案例中，这可能是一个很好的安全经验法则，但它在更多情况下是无关紧要的，正如 Pecka 和 Svoboda [2014] 所讨论的那样：它排除了不可逆转的好行为和坏行为。</p>
<p>3.5 Remark on Multi-Objective RL</p>
<p>我们注意到约束 RL 与多目标 RL 密切相关，并且我们关于分离任务目标和安全要求之间关注点的论点也适用于多目标 RL。我们选择关注受限强化学习，因为安全需求函数的自然“形状”：通常存在满足安全需求的饱和点，进一步降低函数的值不再使系统有意义或有用地更安全.在约束公式中，这对应于约束阈值；这在多目标公式中没有标准等价物。</p>
<h4 id="4-Safety-Gym"><a href="#4-Safety-Gym" class="headerlink" title="4 Safety Gym"></a>4 Safety Gym</h4><p>We now introduce Safety Gym, a set of tools for accelerating safe exploration research. Safety Gym consists of two components:</p>
<p> • an environment-builder that allows a user to create a new environment by mixing and matching from a wide range of physics elements, goals, and safety requirements, </p>
<p>• and a suite of pre-configured benchmark environments to help standardize the measurement of progress on the safe exploration problem.</p>
<p>在深入解释和明确列出基准环境之前，我们将首先对安全体育馆的特点和设计要求做一个高层次的概述。</p>
<ul>
<li>Framework（框架）： Safety Gym是作为一个独立的模块实现的，它使用OpenAI Gym[Brockman等人，2016]接口来实例化RL环境并与之互动，并使用MuJoCo物理模拟器[Todorov等人，2012]来构建和向前模拟每个环境。</li>
<li>Environment Contents（环境内容）： 安全健身房的环境和环境元素受到机器人控制中出现的实际安全问题的启发（尽管不是精确模拟）。每个环境都有一个机器人，它必须在杂乱无章的环境中完成任务，同时尊重它与周围物体和区域互动的约束。与我们提出的围绕约束性RL的形式主义规范安全探索研究的建议相一致，每个Safety Gym环境都有单独的奖励和成本函数，它们分别指定任务目标和安全要求。</li>
<li>Generalization（泛化）： 与Cobbe等人[2018]类似，我们担心许多先前的RL基准（如Atari环境[Bellemare等人，2012]或MuJoCo-Gym[Brockman等人，2016]）需要代理人很少或没有泛化才能成功；这对安全来说具有特殊的意义，在那里分布性转变的稳健性是一个关键问题[Amodei等人，2016]。我们通过将广泛的布局随机化纳入Safety Gym基准环境来解决这个问题，因此代理人需要进行概括，以便安全地导航和解决任务：布局在每个新情节的开始都是随机的。虽然我们没有像Cobbe等人[2018]那样明确地将我们的环境布局划分为训练和测试集，但我们的环境构建工具很容易支持这种形式的扩展</li>
</ul>
<h5 id="4-1-Safety-Gym-Environment-Builder"><a href="#4-1-Safety-Gym-Environment-Builder" class="headerlink" title="4.1 Safety Gym Environment-Builder"></a>4.1 Safety Gym Environment-Builder</h5><p>虽然我们将环境构建工具的详细文档留给了代码库本身，但我们将在此处简要介绍其基本使用、功能和设计原则。环境构建器被实现为一个类，<code>safety_gym.envs.engine.Engine</code>。用户通过提供适当的配置指令来指定环境，例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> safety_gym.envs.engine.Engine <span class="keyword">import</span> Engine </span><br><span class="line">config_dict = ... </span><br><span class="line">env = Engine(config=config_dict)</span><br></pre></td></tr></table></figure>
<p>用户能够配置各种环境特征，包括机器人、任务、约束、观察空间和布局随机化。</p>
<p>4.1.1 Robot Options and Desiderata</p>
<p>In Safety Gym environments, the agent perceives the world <code>through a robot’s sensors</code> and interacts with the world through its actuators.</p>
<p>机器人通过 MuJoCo XML 文件指定。 Safety Gym 附带了我们在基准环境中使用的三个预制机器人，但用户可以通过将文件路径传递到 Engine 对象配置中的 XML 来创建一个带有新机器人的环境。预制机器人是</p>
<ul>
<li><strong>Point</strong>：（图 1a。）一个简单的机器人被限制在 2D 平面上，一个执行器用于转动，另一个用于向前/向后移动。这种因式控制方案使机器人特别容易控制导航。 Point 前面有一个小方块，使它既可以更容易地从视觉上确定机器人的方向，也可以帮助点推动出现在我们的一项任务中的盒子元素。</li>
<li><strong>Car</strong>：（图 1b。）汽车是一种稍微复杂的机器人，具有两个独立驱动的平行轮和一个自由滚动的后轮。汽车不固定在 2D 平面上，但大部分位于其中。对于这个机器人，转动和向前/向后移动都需要协调两个执行器。它在设计上类似于教育中使用的简单机器人。</li>
<li><strong>Doggo</strong>：（图 1c.）Doggo 是一个双侧对称的四足机器人。四条腿中的每条腿在臀部都有两个控件，用于相对于躯干的方位角和仰角，在膝盖上有一个控件，用于控制角度。它的设计使得统一的随机策略应该防止机器人摔倒并产生一些行程。</li>
</ul>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202305061608344.png" alt="image-20230506160814252"></p>
<p>所有机器人的所有动作都是连续的，并且线性缩放到 [-1，+1]，这对于基于 3D 机器人的 RL 环境很常见，并且（有趣的是）改进了神经网络的学习。</p>
<p>模数缩放，动作参数化基于手动调整和 MuJoCo 执行器默认值的混合，我们警告说，目前尚不清楚这些选择是否最佳。</p>
<p>一些安全探索技术是动作层干预，例如投射到最接近的预测安全动作 [Dalal 等人，2018 年；Chow 等人，2019 年]，这些方法可能对动作参数化敏感。因此，动作参数化可能值得比通常给出的更仔细的考虑。未来关于动作空间设计的工作可能是找到尊重我们关心的物理测量的动作参数化——例如，固定距离对应固定能量的动作空间。</p>
<p>4.1.2 Task Options and Desiderata</p>
<p>Safety Gym 环境构建器目前支持三个主要任务：目标、按钮和推送（如图 2 所示）。 Safety Gym 中的任务是相互排斥的，一个单独的环境只能使用一个任务。奖励函数是可配置的，允许奖励是稀疏的（奖励仅在任务完成时获得）或密集的（奖励具有有用的、手工制作的塑造项）。任务详情如下：</p>
<ul>
<li><strong>Goal</strong>：（图 2a。）将机器人移动到一系列目标位置。达成目标后，目标位置会随机重置到某个新位置，同时保持布局的其余部分不变。稀疏奖励成分是在达到目标位置（机器人进入目标圈）时获得的。密集的奖励成分为朝着目标前进提供了奖励。</li>
<li><strong>Button</strong>：（图 2b。）按下一系列目标按钮。几个固定的“按钮”散布在整个环境中，代理应导航到并按下（接触）当前突出显示的按钮，即目标按钮。代理按下正确的按钮后，环境将选择并突出显示一个新的目标按钮，并保持其他一切不变。稀疏奖励成分是在按下当前目标按钮时获得的。密集的奖励组件为朝着当前目标按钮移动提供奖励。</li>
<li><strong>Push</strong>：（图 2c.）将一个盒子移动到一系列目标位置。和目标任务一样，每完成一个目标就会绘制一个新的目标位置。当黄色框进入目标圈时，获得稀疏奖励成分。密集奖励组件由两部分组成：一部分用于使代理更接近盒子，另一部分用于使盒子更接近目标。</li>
</ul>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202305071113881.png" alt="image-20230507111340799"></p>
<p>该代码还包括对额外调试任务 X、Z 和 Circle 的支持。这些分别奖励智能体沿 x 轴尽可能远地运行、沿 z 轴向上运行以及绕圆圈运行（类似于 [Achiam 等人，2017] 的 Circle 环境）。我们的基准测试环境中不使用调试任务。</p>
<p>4.1.3 Constraint Options and Desiderata</p>
<p>Safety Gym 环境构建器支持与安全要求相关的五种主要元素：Hazards、Vases、Pillars、Buttons 和 Gremlins（如图 3 所示）Safety Gym 中的这些元素可以自由混合和匹配：用户可以向环境中添加任意数量的任意种类的元素，并可以为每种元素决定代理是否需要满足约束。在每个时间步，环境将为具有关联约束的每种不安全元素提供单独的成本信号，并提供反映与不安全元素的整体交互的总成本信号，如前所述，这些成本与基于任务的奖励信号是分开的。一些细节：</p>
<ul>
<li><p>Safety Gym 环境提供用于约束的每个状态成本函数，但不指定约束函数或约束阈值的选择。我们将它们分别视为属于系统的算法和人类设计者。 </p>
</li>
<li><p>RL 环境的标准 Gym API 为环境步骤函数生成以下签名：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">next_observation, reward, done, info = env.step(action)</span><br></pre></td></tr></table></figure>
<p>我们使用相同的签名，并通过信息字典提供成本信息。在每个时间步，信息包含形式为 cost_{kind} 的键，一个对应于存在的每种成本。（一些不安全的元素有多种相关的成本。）信息字典还包含一个成本键，它给出了aggregate cost：total cost（所有成本的总和）或是否有任何成本非零的二进制指示符。</p>
</li>
</ul>
<p>约束元素本身是：</p>
<ul>
<li>Hazards: (Fig. 3a.) Dangerous areas to avoid. These are circles on the ground that are non-physical, and the agent is penalized for entering them. </li>
<li>Vases: (Fig. 3b.) Objects to avoid. These are small blocks that represent fragile objects. The agent is penalized for touching or moving them. </li>
<li>Pillars: (Fig. 3d.) Immobile obstacles. These are rigid barriers in the environment, which the agent should not touch.</li>
<li>Buttons: (Fig. 3c.) Incorrect goals. When using the “buttons” goal, pressing an incorrect button is penalized. </li>
<li>Gremlins: (Fig. 3e.) Moving objects. These are simple moving objects that the agent must avoid contacting. Since they are moving quickly, the agent must stay out of the way of their path of travel.</li>
</ul>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202305061647400.png" alt="image-20230506164704320"></p>
<p>尽管所有约束元素都代表了代理要避免的事情，但它们由于具有不同的动态而对代理提出了不同的挑战。为了说明对比：危险不提供物理障碍，花瓶是可移动的障碍物，柱子是不可移动的障碍物，按钮有时可以被视为目标，小精灵是主动移动的障碍物。</p>
<p>与 Safety Gym 中的奖励函数一样，成本函数可以通过多种方式配置；有关详细信息，请参阅代码。默认情况下，成本函数是指示是否发生不安全交互的简单指标（如果代理做了不安全的事情，则 ct = 1，否则 ct = 0）。</p>
<p>4.1.4 Observation Space Options and Desiderata</p>
<p>Safety Gym 中的观察空间是高度可配置的。观察空间组件的选项包括标准机器人传感器（加速度计、陀螺仪、磁力计和速度计）、关节位置和速度传感器、指向目标的罗盘和激光雷达（每个激光雷达传感器感知单一种类的物体）.用户可以通过将适当的配置标志传递给引擎来将这些添加到环境中。</p>
<p>设计观察空间的指导原则是尽量保持特征值较小（理想情况下为零，介于 -1 和 1 之间），并且处于状态的微小变化导致观察的微小变化的状态</p>
<p>For instance, to avoid <strong>wrap-around effects</strong> from representing angles in degrees or radians, we represented angles θ with (sin θ, cos θ)</p>
<p>但是，我们后来发现，有些观测分量有时还是会取很大的值；As a result, algorithmic tricks for <strong>shifting</strong> and <strong>scaling observation values</strong> may be useful in practice.</p>
<p><strong>Natural Lidar and Pseudo-Lidar:</strong>可以使用“自然激光雷达”或“伪激光雷达”计算激光雷达观测值。自然激光雷达是使用 MuJoCo 中的光线追踪工具计算的，而伪激光雷达是通过遍历对象并用适当的值填充容器来计算的。伪激光雷达对某些对象类型的表现更好，因此我们认为它是首选；因此，Safety Gym 中的所有激光雷达观测都默认为伪激光雷达。但是，如果需要，用户可以通过标志将激光雷达计算类型更改为引擎。</p>
<p><strong>Lidar Visualization:</strong>为了帮助人类理解代理人正在感知什么，在渲染场景时，我们使用漂浮在代理人上方的非物理“激光雷达光晕”来可视化代理人的激光雷达观察结果。激光雷达光晕如图 4 所示。</p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202305071538100.png" alt="image-20230507153758931"></p>
<p>4.1.5 Layout Randomization Options and Desiderata</p>
<p>用户可以通过为每个对象种类选择随机放置区域来配置布局随机化。如前所述，Safety Gym 中的随机化选项使我们能够构建代理必须泛化才能成功导航、解决任务并遵守安全约束的环境。</p>
<h5 id="4-2-Safety-Gym-Benchmark-Suite"><a href="#4-2-Safety-Gym-Benchmark-Suite" class="headerlink" title="4.2 Safety Gym Benchmark Suite"></a>4.2 Safety Gym Benchmark Suite</h5><p>Safety Gym 附带一套预配置的基准环境，使用 Safety Gym Engine 构建，用于衡量安全探索的进展。机器人（Point、Car 和 Doggo）和任务（Goal、Button 和 Push）的所有组合都在套件中表示；每个组合都有三个难度级别（0、1 和 2），对应于该环境中不安全元素的密度。</p>
<p>所有 0 级环境都是不受约束的，不会出现不安全的元素。 1 级环境有一些不安全元素，2 级环境中不安全元素非常密集。18 个 1 级和 2 级环境用于衡量受约束 RL 的进展，而 9 个 0 级环境允许调试纯 RL。</p>
<p>全套环境如图 5 所示，描述如下：</p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202305071556314.png" alt="image-20230507155631156"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202305071556950.png" alt="image-20230507155640837"></p>
<p>Environments are instantiated using the OpenAI Gym [Brockman et al., 2016] make function:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> gym, safety_gym env = gym.make(<span class="string">'Safexp-DoggoGoal1-v0'</span>)</span><br></pre></td></tr></table></figure>
<p>基准环境的布局在每一集开始时随机重新排列。我们在图 6 中展示了随机布局的示例。</p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202305071601980.png" alt="image-20230507160122902"></p>
<p>所有基准环境都配置为使用dense reward signals and indicator cost functions。</p>
<h4 id="5-Experiments"><a href="#5-Experiments" class="headerlink" title="5 Experiments"></a>5 Experiments</h4><p>在本节中，我们将描述我们的实验，以在 Safety Gym 环境中为现有的无约束和约束 RL 算法设定基线。</p>
<h5 id="5-1-Methods-Evaluation-Protocol"><a href="#5-1-Methods-Evaluation-Protocol" class="headerlink" title="5.1 Methods: Evaluation Protocol"></a>5.1 Methods: Evaluation Protocol</h5><p><strong>Optimization Problem</strong>: We evaluate agents based on the optimization problem</p>
<script type="math/tex; mode=display">
\begin{aligned}
\max _{\pi_\theta} & \underset{\tau \sim \pi_\theta}{\mathbb{E}}\left[\sum_{t=0}^T r_t\right] \\
\text { s.t. } & \underset{\tau \sim \pi_\theta}{\mathbb{E}}\left[\sum_{t=0}^T c_t\right] \leq d,
\end{aligned}</script><p>where <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex;" xmlns="http://www.w3.org/2000/svg" width="1.745ex" height="1.357ex" role="img" focusable="false" viewBox="0 -442 771.3 599.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"></path></g><g data-mml-node="mi" transform="translate(466,-150) scale(0.707)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g></g></g></g></svg></mjx-container> is the aggregate indicator cost function for the environment (<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex;" xmlns="http://www.w3.org/2000/svg" width="1.745ex" height="1.357ex" role="img" focusable="false" viewBox="0 -442 771.3 599.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"></path></g><g data-mml-node="mi" transform="translate(466,-150) scale(0.707)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g></g></g></g></svg></mjx-container>  = 1 for an unsafe interaction, regardless of source) and <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.023ex;" xmlns="http://www.w3.org/2000/svg" width="1.176ex" height="1.593ex" role="img" focusable="false" viewBox="0 -694 520 704"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g></g></g></svg></mjx-container> is a hyperparameter.That is, in our experiments, we use the finite horizon undiscounted return and cumulative cost formulations, and furthermore, we fold all safety requirements into a single constraint.(其中 ct 是环境的聚合指标成本函数（ct = 1 表示不安全的交互，无论来源如何），d 是超参数。也就是说，在我们的实验中，我们使用有限范围的未贴现回报和累积成本公式，此外，我们将所有安全要求折叠成一个单一的约束)</p>
<p><strong>Metrics</strong>:为了描述代理的任务和安全性能及其训练运行，我们在整个训练过程中测量以下内容：</p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202305071614785.png" alt="image-20230507161412723"></p>
<p>选择衡量成本率(cost rate)而不是总成本或约束违规总和是不明显的，并且可能存在争议，但我们认为成本率具有几个有吸引力的特性。首先，它直接对应于安全结果：较低的成本率意味着发生的不安全事件较少。与总成本相比，成本率更直观，并且可以比较提供信息的不等长训练运行（尽管对于非常不等长的训练是不完美的，因为更长的运行可以在训练早期“平均掉”不良情况）。因为等额的成本通常对应等量的安全风险，所以使用像 $\rho<em>c<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.452ex;" xmlns="http://www.w3.org/2000/svg" width="131.222ex" height="2.149ex" role="img" focusable="false" viewBox="0 -750 58000 950"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">这</text></g><g data-mml-node="mi" transform="translate(1000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">样</text></g><g data-mml-node="mi" transform="translate(2000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">的</text></g><g data-mml-node="mi" transform="translate(3000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">衡</text></g><g data-mml-node="mi" transform="translate(4000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">量</text></g><g data-mml-node="mi" transform="translate(5000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">标</text></g><g data-mml-node="mi" transform="translate(6000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">准</text></g><g data-mml-node="mi" transform="translate(7000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">更</text></g><g data-mml-node="mi" transform="translate(8000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">有</text></g><g data-mml-node="mi" transform="translate(9000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">意</text></g><g data-mml-node="mi" transform="translate(10000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">义</text></g><g data-mml-node="mi" transform="translate(11000,0)"><text data-variant="italic" transform="scale(1,-1)" font-size="884px" font-family="serif" font-style="italic">，</text></g><g data-mml-node="mi" transform="translate(12000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">它</text></g><g data-mml-node="mi" transform="translate(13000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">考</text></g><g data-mml-node="mi" transform="translate(14000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">虑</text></g><g data-mml-node="mi" transform="translate(15000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">了</text></g><g data-mml-node="mi" transform="translate(16000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">整</text></g><g data-mml-node="mi" transform="translate(17000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">个</text></g><g data-mml-node="mi" transform="translate(18000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">培</text></g><g data-mml-node="mi" transform="translate(19000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">训</text></g><g data-mml-node="mi" transform="translate(20000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">过</text></g><g data-mml-node="mi" transform="translate(21000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">程</text></g><g data-mml-node="mi" transform="translate(22000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">中</text></g><g data-mml-node="mi" transform="translate(23000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">的</text></g><g data-mml-node="mi" transform="translate(24000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">所</text></g><g data-mml-node="mi" transform="translate(25000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">有</text></g><g data-mml-node="mi" transform="translate(26000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">成</text></g><g data-mml-node="mi" transform="translate(27000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">本</text></g><g data-mml-node="mi" transform="translate(28000,0)"><text data-variant="italic" transform="scale(1,-1)" font-size="884px" font-family="serif" font-style="italic">，</text></g><g data-mml-node="mi" transform="translate(29000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">而</text></g><g data-mml-node="mi" transform="translate(30000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">不</text></g><g data-mml-node="mi" transform="translate(31000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">是</text></g><g data-mml-node="mi" transform="translate(32000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">仅</text></g><g data-mml-node="mi" transform="translate(33000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">包</text></g><g data-mml-node="mi" transform="translate(34000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">括</text></g><g data-mml-node="mi" transform="translate(35000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">超</text></g><g data-mml-node="mi" transform="translate(36000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">过</text></g><g data-mml-node="mi" transform="translate(37000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">约</text></g><g data-mml-node="mi" transform="translate(38000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">束</text></g><g data-mml-node="mi" transform="translate(39000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">阈</text></g><g data-mml-node="mi" transform="translate(40000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">值</text></g><g data-mml-node="mi" transform="translate(41000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">的</text></g><g data-mml-node="mi" transform="translate(42000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">成</text></g><g data-mml-node="mi" transform="translate(43000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">本</text></g><g data-mml-node="mi" transform="translate(44000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">的</text></g><g data-mml-node="mi" transform="translate(45000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">衡</text></g><g data-mml-node="mi" transform="translate(46000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">量</text></g><g data-mml-node="mi" transform="translate(47000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">标</text></g><g data-mml-node="mi" transform="translate(48000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">准</text></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(49000,0)"><g data-mml-node="mo"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">。</text></g></g><g data-mml-node="mi" transform="translate(50000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">最</text></g><g data-mml-node="mi" transform="translate(51000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">后</text></g><g data-mml-node="mi" transform="translate(52000,0)"><text data-variant="italic" transform="scale(1,-1)" font-size="884px" font-family="serif" font-style="italic">，</text></g><g data-mml-node="mi" transform="translate(53000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">我</text></g><g data-mml-node="mi" transform="translate(54000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">们</text></g><g data-mml-node="mi" transform="translate(55000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">观</text></g><g data-mml-node="mi" transform="translate(56000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">察</text></g><g data-mml-node="mi" transform="translate(57000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">到</text></g></g></g></svg></mjx-container>\rho_c<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.452ex;" xmlns="http://www.w3.org/2000/svg" width="58.824ex" height="2.149ex" role="img" focusable="false" viewBox="0 -750 26000 950"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">与</text></g><g data-mml-node="mi" transform="translate(1000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">训</text></g><g data-mml-node="mi" transform="translate(2000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">练</text></g><g data-mml-node="mi" transform="translate(3000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">过</text></g><g data-mml-node="mi" transform="translate(4000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">程</text></g><g data-mml-node="mi" transform="translate(5000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">中</text></g><g data-mml-node="mi" transform="translate(6000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">的</text></g><g data-mml-node="mi" transform="translate(7000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">近</text></g><g data-mml-node="mi" transform="translate(8000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">似</text></g><g data-mml-node="mi" transform="translate(9000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">约</text></g><g data-mml-node="mi" transform="translate(10000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">束</text></g><g data-mml-node="mi" transform="translate(11000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">满</text></g><g data-mml-node="mi" transform="translate(12000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">意</text></g><g data-mml-node="mi" transform="translate(13000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">度</text></g><g data-mml-node="mi" transform="translate(14000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">之</text></g><g data-mml-node="mi" transform="translate(15000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">间</text></g><g data-mml-node="mi" transform="translate(16000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">的</text></g><g data-mml-node="mi" transform="translate(17000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">关</text></g><g data-mml-node="mi" transform="translate(18000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">系</text></g><g data-mml-node="mi" transform="translate(19000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">非</text></g><g data-mml-node="mi" transform="translate(20000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">常</text></g><g data-mml-node="mi" transform="translate(21000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">简</text></g><g data-mml-node="mi" transform="translate(22000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">单</text></g><g data-mml-node="mi" transform="translate(23000,0)"><text data-variant="italic" transform="scale(1,-1)" font-size="884px" font-family="serif" font-style="italic">：</text></g><g data-mml-node="mi" transform="translate(24000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">如</text></g><g data-mml-node="mi" transform="translate(25000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">果</text></g></g></g></svg></mjx-container>T</em>{ep}<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="91.206ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 40313 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">是</text></g><g data-mml-node="mi" transform="translate(1000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">平</text></g><g data-mml-node="mi" transform="translate(2000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">均</text></g><g data-mml-node="mi" transform="translate(3000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">剧</text></g><g data-mml-node="mi" transform="translate(4000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">集</text></g><g data-mml-node="mi" transform="translate(5000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">长</text></g><g data-mml-node="mi" transform="translate(6000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">度</text></g><g data-mml-node="mo" transform="translate(7000,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(7389,0)"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="mi" transform="translate(7918,0)"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path></g><g data-mml-node="mi" transform="translate(8403,0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g><g data-mml-node="mi" transform="translate(8869,0)"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(9320,0)"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="mi" transform="translate(9849,0)"><path data-c="1D454" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z"></path></g><g data-mml-node="mi" transform="translate(10326,0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g><g data-mml-node="mi" transform="translate(10792,0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g><g data-mml-node="mi" transform="translate(11258,0)"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path></g><g data-mml-node="mi" transform="translate(11761,0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(12106,0)"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path></g><g data-mml-node="mi" transform="translate(12575,0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g><g data-mml-node="mi" transform="translate(13060,0)"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g><g data-mml-node="mi" transform="translate(13580,0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g><g data-mml-node="mi" transform="translate(14046,0)"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="mi" transform="translate(14344,0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g><g data-mml-node="mi" transform="translate(14810,0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(15410,0)"><path data-c="1D454" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z"></path></g><g data-mml-node="mi" transform="translate(15887,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g><g data-mml-node="mi" transform="translate(16248,0)"><path data-c="210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"></path></g><g data-mml-node="mo" transform="translate(16824,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mi" transform="translate(17213,0)"><text data-variant="italic" transform="scale(1,-1)" font-size="884px" font-family="serif" font-style="italic">，</text></g><g data-mml-node="mi" transform="translate(18213,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">则</text></g><g data-mml-node="mo" transform="translate(19213,0)"><g data-c="2036"><path data-c="2035" d="M12 501Q12 527 31 542T63 558Q73 560 77 560Q114 560 128 528Q133 518 188 293T244 61Q244 56 223 50T195 43Q192 43 190 45T102 263T14 486Q12 496 12 501Z"></path><path data-c="2035" d="M12 501Q12 527 31 542T63 558Q73 560 77 560Q114 560 128 528Q133 518 188 293T244 61Q244 56 223 50T195 43Q192 43 190 45T102 263T14 486Q12 496 12 501Z" transform="translate(275,0)"></path></g></g><g data-mml-node="mi" transform="translate(19763,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">训</text></g><g data-mml-node="mi" transform="translate(20763,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">练</text></g><g data-mml-node="mi" transform="translate(21763,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">期</text></g><g data-mml-node="mi" transform="translate(22763,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">间</text></g><g data-mml-node="mi" transform="translate(23763,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">的</text></g><g data-mml-node="mi" transform="translate(24763,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">平</text></g><g data-mml-node="mi" transform="translate(25763,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">均</text></g><g data-mml-node="mi" transform="translate(26763,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">剧</text></g><g data-mml-node="mi" transform="translate(27763,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">集</text></g><g data-mml-node="mi" transform="translate(28763,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">满</text></g><g data-mml-node="mi" transform="translate(29763,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">足</text></g><g data-mml-node="mi" transform="translate(30763,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">约</text></g><g data-mml-node="mi" transform="translate(31763,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">束</text></g><g data-mml-node="mo" transform="translate(32763,0)"><g data-c="2033"><path data-c="2032" d="M79 43Q73 43 52 49T30 61Q30 68 85 293T146 528Q161 560 198 560Q218 560 240 545T262 501Q262 496 260 486Q259 479 173 263T84 45T79 43Z"></path><path data-c="2032" d="M79 43Q73 43 52 49T30 61Q30 68 85 293T146 528Q161 560 198 560Q218 560 240 545T262 501Q262 496 260 486Q259 479 173 263T84 45T79 43Z" transform="translate(275,0)"></path></g></g><g data-mml-node="mi" transform="translate(33313,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">的</text></g><g data-mml-node="mi" transform="translate(34313,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">条</text></g><g data-mml-node="mi" transform="translate(35313,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">件</text></g><g data-mml-node="mi" transform="translate(36313,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">可</text></g><g data-mml-node="mi" transform="translate(37313,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">以</text></g><g data-mml-node="mi" transform="translate(38313,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">写</text></g><g data-mml-node="mi" transform="translate(39313,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">为</text></g></g></g></svg></mjx-container>\rho<em>cT</em>{ep}\leq d$。</p>
<p>我们承认成本率不是一个完美的衡量标准。例如，成本信号中具有高振幅振荡的训练运行可能与每个轨迹具有恒定成本的训练运行具有相同的成本率，但由于不稳定，前者显然不如后者理想。但我们认为，总的来说，成本率是一个很好的衡量安全遗憾的有用指标。</p>
<p><strong>Comparing Training Runs</strong>:有多种方法可以根据这些测量对智能体和训练运行进行排名，不同的比较规则适用于不同的情况。但是，我们强调一些指导我们讨论的通用规则：</p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202305071630351.png" alt="image-20230507163022283"></p>
<p><strong>Comparing Algorithms</strong>:</p>
<p>虽然到目前为止我们已经描述了如何在单一环境中比较两个代理，但我们仍然需要一个规则来比较多个环境中算法的综合性能。在我们的分析中，我们通过查看在 Safety Gym 环境和随机种子上平均的归一化性能指标来比较算法。</p>
<p>我们为每个环境 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex;" xmlns="http://www.w3.org/2000/svg" width="1.276ex" height="1.645ex" role="img" focusable="false" viewBox="0 -705 564 727"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="45" d="M144 470Q144 556 240 630T451 705Q564 705 564 637Q564 611 540 573Q529 559 505 547T464 534Q448 534 448 545Q448 552 455 562Q463 577 463 591Q463 600 462 604T456 616T436 627T400 635Q396 635 390 635T380 636Q291 636 258 568Q245 544 245 516Q245 463 290 438T391 410Q415 410 415 398Q415 392 407 380T376 356T326 341Q288 340 260 327Q218 311 187 276T143 208T130 151Q130 113 156 88T211 55T268 47Q349 47 403 125Q415 144 439 157T483 171Q499 171 499 160Q499 148 475 120T413 59T315 3T197 -22Q124 -22 77 14T30 105Q30 126 39 154T66 216T122 288T209 354L223 362Q144 400 144 470Z"></path></g></g></g></g></svg></mjx-container> 分配一组特征指标，<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.576ex;" xmlns="http://www.w3.org/2000/svg" width="9.56ex" height="2.526ex" role="img" focusable="false" viewBox="0 -861.5 4225.6 1116.3"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msubsup"><g data-mml-node="mi"><path data-c="1D43D" d="M447 625Q447 637 354 637H329Q323 642 323 645T325 664Q329 677 335 683H352Q393 681 498 681Q541 681 568 681T605 682T619 682Q633 682 633 672Q633 670 630 658Q626 642 623 640T604 637Q552 637 545 623Q541 610 483 376Q420 128 419 127Q397 64 333 21T195 -22Q137 -22 97 8T57 88Q57 130 80 152T132 174Q177 174 182 130Q182 98 164 80T123 56Q115 54 115 53T122 44Q148 15 197 15Q235 15 271 47T324 130Q328 142 387 380T447 625Z"></path></g><g data-mml-node="TeXAtom" transform="translate(719.9,363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="45" d="M144 470Q144 556 240 630T451 705Q564 705 564 637Q564 611 540 573Q529 559 505 547T464 534Q448 534 448 545Q448 552 455 562Q463 577 463 591Q463 600 462 604T456 616T436 627T400 635Q396 635 390 635T380 636Q291 636 258 568Q245 544 245 516Q245 463 290 438T391 410Q415 410 415 398Q415 392 407 380T376 356T326 341Q288 340 260 327Q218 311 187 276T143 208T130 151Q130 113 156 88T211 55T268 47Q349 47 403 125Q415 144 439 157T483 171Q499 171 499 160Q499 148 475 120T413 59T315 3T197 -22Q124 -22 77 14T30 105Q30 126 39 154T66 216T122 288T209 354L223 362Q144 400 144 470Z"></path></g></g><g data-mml-node="mi" transform="translate(588,-247) scale(0.707)"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(1168.7,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msubsup" transform="translate(1613.4,0)"><g data-mml-node="mi"><path data-c="1D43D" d="M447 625Q447 637 354 637H329Q323 642 323 645T325 664Q329 677 335 683H352Q393 681 498 681Q541 681 568 681T605 682T619 682Q633 682 633 672Q633 670 630 658Q626 642 623 640T604 637Q552 637 545 623Q541 610 483 376Q420 128 419 127Q397 64 333 21T195 -22Q137 -22 97 8T57 88Q57 130 80 152T132 174Q177 174 182 130Q182 98 164 80T123 56Q115 54 115 53T122 44Q148 15 197 15Q235 15 271 47T324 130Q328 142 387 380T447 625Z"></path></g><g data-mml-node="TeXAtom" transform="translate(719.9,363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="45" d="M144 470Q144 556 240 630T451 705Q564 705 564 637Q564 611 540 573Q529 559 505 547T464 534Q448 534 448 545Q448 552 455 562Q463 577 463 591Q463 600 462 604T456 616T436 627T400 635Q396 635 390 635T380 636Q291 636 258 568Q245 544 245 516Q245 463 290 438T391 410Q415 410 415 398Q415 392 407 380T376 356T326 341Q288 340 260 327Q218 311 187 276T143 208T130 151Q130 113 156 88T211 55T268 47Q349 47 403 125Q415 144 439 157T483 171Q499 171 499 160Q499 148 475 120T413 59T315 3T197 -22Q124 -22 77 14T30 105Q30 126 39 154T66 216T122 288T209 354L223 362Q144 400 144 470Z"></path></g></g><g data-mml-node="mi" transform="translate(588,-247) scale(0.707)"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"></path></g></g><g data-mml-node="mo" transform="translate(2782.1,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msubsup" transform="translate(3226.7,0)"><g data-mml-node="mi"><path data-c="1D70C" d="M58 -216Q25 -216 23 -186Q23 -176 73 26T127 234Q143 289 182 341Q252 427 341 441Q343 441 349 441T359 442Q432 442 471 394T510 276Q510 219 486 165T425 74T345 13T266 -10H255H248Q197 -10 165 35L160 41L133 -71Q108 -168 104 -181T92 -202Q76 -216 58 -216ZM424 322Q424 359 407 382T357 405Q322 405 287 376T231 300Q217 269 193 170L176 102Q193 26 260 26Q298 26 334 62Q367 92 389 158T418 266T424 322Z"></path></g><g data-mml-node="TeXAtom" transform="translate(550,363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="45" d="M144 470Q144 556 240 630T451 705Q564 705 564 637Q564 611 540 573Q529 559 505 547T464 534Q448 534 448 545Q448 552 455 562Q463 577 463 591Q463 600 462 604T456 616T436 627T400 635Q396 635 390 635T380 636Q291 636 258 568Q245 544 245 516Q245 463 290 438T391 410Q415 410 415 398Q415 392 407 380T376 356T326 341Q288 340 260 327Q218 311 187 276T143 208T130 151Q130 113 156 88T211 55T268 47Q349 47 403 125Q415 144 439 157T483 171Q499 171 499 160Q499 148 475 120T413 59T315 3T197 -22Q124 -22 77 14T30 105Q30 126 39 154T66 216T122 288T209 354L223 362Q144 400 144 470Z"></path></g></g><g data-mml-node="mi" transform="translate(550,-247) scale(0.707)"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"></path></g></g></g></g></svg></mjx-container>（均为正数），并计算归一化回报  <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="4.986ex" height="2.679ex" role="img" focusable="false" viewBox="0 -934 2203.9 1184"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mover"><g data-mml-node="mi"><path data-c="1D43D" d="M447 625Q447 637 354 637H329Q323 642 323 645T325 664Q329 677 335 683H352Q393 681 498 681Q541 681 568 681T605 682T619 682Q633 682 633 672Q633 670 630 658Q626 642 623 640T604 637Q552 637 545 623Q541 610 483 376Q420 128 419 127Q397 64 333 21T195 -22Q137 -22 97 8T57 88Q57 130 80 152T132 174Q177 174 182 130Q182 98 164 80T123 56Q115 54 115 53T122 44Q148 15 197 15Q235 15 271 47T324 130Q328 142 387 380T447 625Z"></path></g><g data-mml-node="mo" transform="translate(503,244) translate(-250 0)"><path data-c="AF" d="M69 544V590H430V544H69Z"></path></g></g></g><g data-mml-node="mi" transform="translate(588,-150) scale(0.707)"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(956.9,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1345.9,0)"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"></path></g><g data-mml-node="mo" transform="translate(1814.9,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container>、归一化约束违反 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="5.896ex" height="2.679ex" role="img" focusable="false" viewBox="0 -934 2606.2 1184"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mover"><g data-mml-node="mi"><path data-c="1D440" d="M289 629Q289 635 232 637Q208 637 201 638T194 648Q194 649 196 659Q197 662 198 666T199 671T201 676T203 679T207 681T212 683T220 683T232 684Q238 684 262 684T307 683Q386 683 398 683T414 678Q415 674 451 396L487 117L510 154Q534 190 574 254T662 394Q837 673 839 675Q840 676 842 678T846 681L852 683H948Q965 683 988 683T1017 684Q1051 684 1051 673Q1051 668 1048 656T1045 643Q1041 637 1008 637Q968 636 957 634T939 623Q936 618 867 340T797 59Q797 55 798 54T805 50T822 48T855 46H886Q892 37 892 35Q892 19 885 5Q880 0 869 0Q864 0 828 1T736 2Q675 2 644 2T609 1Q592 1 592 11Q592 13 594 25Q598 41 602 43T625 46Q652 46 685 49Q699 52 704 61Q706 65 742 207T813 490T848 631L654 322Q458 10 453 5Q451 4 449 3Q444 0 433 0Q418 0 415 7Q413 11 374 317L335 624L267 354Q200 88 200 79Q206 46 272 46H282Q288 41 289 37T286 19Q282 3 278 1Q274 0 267 0Q265 0 255 0T221 1T157 2Q127 2 95 1T58 0Q43 0 39 2T35 11Q35 13 38 25T43 40Q45 46 65 46Q135 46 154 86Q158 92 223 354T289 629Z"></path></g><g data-mml-node="mo" transform="translate(629,244) translate(-250 0)"><path data-c="AF" d="M69 544V590H430V544H69Z"></path></g></g></g><g data-mml-node="mi" transform="translate(1003,-150) scale(0.707)"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"></path></g></g><g data-mml-node="mo" transform="translate(1359.2,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1748.2,0)"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"></path></g><g data-mml-node="mo" transform="translate(2217.2,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container> 和归一化成本率 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="4.871ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 2153.2 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mover"><g data-mml-node="mi"><path data-c="1D70C" d="M58 -216Q25 -216 23 -186Q23 -176 73 26T127 234Q143 289 182 341Q252 427 341 441Q343 441 349 441T359 442Q432 442 471 394T510 276Q510 219 486 165T425 74T345 13T266 -10H255H248Q197 -10 165 35L160 41L133 -71Q108 -168 104 -181T92 -202Q76 -216 58 -216ZM424 322Q424 359 407 382T357 405Q322 405 287 376T231 300Q217 269 193 170L176 102Q193 26 260 26Q298 26 334 62Q367 92 389 158T418 266T424 322Z"></path></g><g data-mml-node="mo" transform="translate(341.8,3) translate(-250 0)"><path data-c="AF" d="M69 544V590H430V544H69Z"></path></g></g></g><g data-mml-node="mi" transform="translate(550,-150) scale(0.707)"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"></path></g></g><g data-mml-node="mo" transform="translate(906.2,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1295.2,0)"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"></path></g><g data-mml-node="mo" transform="translate(1764.2,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container> 根据以下条件在 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex;" xmlns="http://www.w3.org/2000/svg" width="1.276ex" height="1.645ex" role="img" focusable="false" viewBox="0 -705 564 727"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="45" d="M144 470Q144 556 240 630T451 705Q564 705 564 637Q564 611 540 573Q529 559 505 547T464 534Q448 534 448 545Q448 552 455 562Q463 577 463 591Q463 600 462 604T456 616T436 627T400 635Q396 635 390 635T380 636Q291 636 258 568Q245 544 245 516Q245 463 290 438T391 410Q415 410 415 398Q415 392 407 380T376 356T326 341Q288 340 260 327Q218 311 187 276T143 208T130 151Q130 113 156 88T211 55T268 47Q349 47 403 125Q415 144 439 157T483 171Q499 171 499 160Q499 148 475 120T413 59T315 3T197 -22Q124 -22 77 14T30 105Q30 126 39 154T66 216T122 288T209 354L223 362Q144 400 144 470Z"></path></g></g></g></g></svg></mjx-container>中进行训练：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\bar{J}_r(\theta) & =\frac{J_r(\theta)}{J_r^{\mathcal{E}}} \\
\bar{M}_c(\theta) & =\frac{\max \left(0, J_c(\theta)-d\right)}{\max \left(\epsilon, J_c^{\mathcal{E}}-d\right)}, \quad \epsilon=10^{-6} \\
\bar{\rho}_c(\theta) & =\frac{\rho_c(\theta)}{\rho_c^{\mathcal{E}}}
\end{aligned}</script><p>每个环境的特征指标都是从我们的实验数据中获得的，作为我们无约束 PPO 实施的最终指标。(特征回报(Characteristic return)和累积成本(cumulative cost)是通过对最后五个epochs的训练进行平均来获得的，以减少噪音。特征成本率(Characteristic cost rate)只是取自最后一个epoch。)</p>
<p>我们比较归一化分数，就像我们比较单独的训练运行一样：平均约束违规应该为零（或在零噪声范围内），并且在近似满足约束的算法中，如果一个算法在平均归一化回报和平均值方面都做得更好，则它会支配另一个算法归一化成本率。</p>
<p>我们报告各种环境的平均归一化分数：</p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202305071923133.png" alt="image-20230507192310984"></p>
<p>由于在每个环境中使用多个种子在完整的 SG18 上进行训练在计算上很费力，因此我们建议将 SG6 作为基本平台，以在有限的计算预算下进行受限 RL 研究。</p>
<h5 id="5-2-Methods-Algorithms"><a href="#5-2-Methods-Algorithms" class="headerlink" title="5.2 Methods: Algorithms"></a>5.2 Methods: Algorithms</h5><p>我们评估的无约束算法是 TRPO [Schulman et al., 2015] 和 PPO [Schulman et al., 2017]，其中奖励函数不包含有关辅助成本(auxiliary costs.)的信息。我们的 PPO 版本基于 Deep RL [OpenAI 和 Achiam，2018] 中的 Spinning Up，它使用提前停止(early stopping)而不是通常出现在 PPO 实现中的其他正则化器(regularizers)。对于约束算法，我们评估</p>
<ul>
<li>Lagrangian methods**:拉格朗日方法使用自适应惩罚系数来实施约束。以 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="4.066ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 1797 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"></path></g><g data-mml-node="mo" transform="translate(550,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(939,0)"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"></path></g><g data-mml-node="mo" transform="translate(1408,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container> 为目标，<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="8.049ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 3557.6 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D454" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z"></path></g><g data-mml-node="mo" transform="translate(477,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(866,0)"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"></path></g><g data-mml-node="mo" transform="translate(1335,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(2001.8,0)"><path data-c="2264" d="M674 636Q682 636 688 630T694 615T687 601Q686 600 417 472L151 346L399 228Q687 92 691 87Q694 81 694 76Q694 58 676 56H670L382 192Q92 329 90 331Q83 336 83 348Q84 359 96 365Q104 369 382 500T665 634Q669 636 674 636ZM84 -118Q84 -108 99 -98H678Q694 -104 694 -118Q694 -130 679 -138H98Q84 -131 84 -118Z"></path></g><g data-mml-node="mn" transform="translate(3057.6,0)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></g></g></g></svg></mjx-container> 为约束，拉格朗日方法求解等效的无约束<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.186ex;" xmlns="http://www.w3.org/2000/svg" width="10.496ex" height="1.699ex" role="img" focusable="false" viewBox="0 -669 4639.3 751"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="6D" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q351 442 364 440T387 434T406 426T421 417T432 406T441 395T448 384T452 374T455 366L457 361L460 365Q463 369 466 373T475 384T488 397T503 410T523 422T546 432T572 439T603 442Q729 442 740 329Q741 322 741 190V104Q741 66 743 59T754 49Q775 46 803 46H819V0H811L788 1Q764 2 737 2T699 3Q596 3 587 0H579V46H595Q656 46 656 62Q657 64 657 200Q656 335 655 343Q649 371 635 385T611 402T585 404Q540 404 506 370Q479 343 472 315T464 232V168V108Q464 78 465 68T468 55T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z"></path><path data-c="61" d="M137 305T115 305T78 320T63 359Q63 394 97 421T218 448Q291 448 336 416T396 340Q401 326 401 309T402 194V124Q402 76 407 58T428 40Q443 40 448 56T453 109V145H493V106Q492 66 490 59Q481 29 455 12T400 -6T353 12T329 54V58L327 55Q325 52 322 49T314 40T302 29T287 17T269 6T247 -2T221 -8T190 -11Q130 -11 82 20T34 107Q34 128 41 147T68 188T116 225T194 253T304 268H318V290Q318 324 312 340Q290 411 215 411Q197 411 181 410T156 406T148 403Q170 388 170 359Q170 334 154 320ZM126 106Q126 75 150 51T209 26Q247 26 276 49T315 109Q317 116 318 175Q318 233 317 233Q309 233 296 232T251 223T193 203T147 166T126 106Z" transform="translate(833,0)"></path><path data-c="78" d="M201 0Q189 3 102 3Q26 3 17 0H11V46H25Q48 47 67 52T96 61T121 78T139 96T160 122T180 150L226 210L168 288Q159 301 149 315T133 336T122 351T113 363T107 370T100 376T94 379T88 381T80 383Q74 383 44 385H16V431H23Q59 429 126 429Q219 429 229 431H237V385Q201 381 201 369Q201 367 211 353T239 315T268 274L272 270L297 304Q329 345 329 358Q329 364 327 369T322 376T317 380T310 384L307 385H302V431H309Q324 428 408 428Q487 428 493 431H499V385H492Q443 385 411 368Q394 360 377 341T312 257L296 236L358 151Q424 61 429 57T446 50Q464 46 499 46H516V0H510H502Q494 1 482 1T457 2T432 2T414 3Q403 3 377 3T327 1L304 0H295V46H298Q309 46 320 51T331 63Q331 65 291 120L250 175Q249 174 219 133T185 88Q181 83 181 74Q181 63 188 55T206 46Q208 46 208 23V0H201Z" transform="translate(1333,0)"></path></g><g data-mml-node="mo" transform="translate(2027.7,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mo" transform="translate(2972.3,0)"><path data-c="6D" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q351 442 364 440T387 434T406 426T421 417T432 406T441 395T448 384T452 374T455 366L457 361L460 365Q463 369 466 373T475 384T488 397T503 410T523 422T546 432T572 439T603 442Q729 442 740 329Q741 322 741 190V104Q741 66 743 59T754 49Q775 46 803 46H819V0H811L788 1Q764 2 737 2T699 3Q596 3 587 0H579V46H595Q656 46 656 62Q657 64 657 200Q656 335 655 343Q649 371 635 385T611 402T585 404Q540 404 506 370Q479 343 472 315T464 232V168V108Q464 78 465 68T468 55T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z"></path><path data-c="69" d="M69 609Q69 637 87 653T131 669Q154 667 171 652T188 609Q188 579 171 564T129 549Q104 549 87 564T69 609ZM247 0Q232 3 143 3Q132 3 106 3T56 1L34 0H26V46H42Q70 46 91 49Q100 53 102 60T104 102V205V293Q104 345 102 359T88 378Q74 385 41 385H30V408Q30 431 32 431L42 432Q52 433 70 434T106 436Q123 437 142 438T171 441T182 442H185V62Q190 52 197 50T232 46H255V0H247Z" transform="translate(833,0)"></path><path data-c="6E" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q450 438 463 329Q464 322 464 190V104Q464 66 466 59T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" transform="translate(1111,0)"></path></g></g></g></svg></mjx-container>优化问题</li>
</ul>
<script type="math/tex; mode=display">
\underset{\theta}{\text{max}}\underset{\lambda\geq0}{\text{min}}\mathcal{L}(\theta,\lambda)\doteq f(\theta)-\lambda g(\theta),</script><p>​     通过在 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.023ex;" xmlns="http://www.w3.org/2000/svg" width="1.061ex" height="1.618ex" role="img" focusable="false" viewBox="0 -705 469 715"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"></path></g></g></g></svg></mjx-container>上梯度上升和在 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.027ex;" xmlns="http://www.w3.org/2000/svg" width="1.319ex" height="1.597ex" role="img" focusable="false" viewBox="0 -694 583 706"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D706" d="M166 673Q166 685 183 694H202Q292 691 316 644Q322 629 373 486T474 207T524 67Q531 47 537 34T546 15T551 6T555 2T556 -2T550 -11H482Q457 3 450 18T399 152L354 277L340 262Q327 246 293 207T236 141Q211 112 174 69Q123 9 111 -1T83 -12Q47 -12 47 20Q47 37 61 52T199 187Q229 216 266 252T321 306L338 322Q338 323 288 462T234 612Q214 657 183 657Q166 657 166 673Z"></path></g></g></g></svg></mjx-container> 上下降。我们将拉格朗日方法与 TRPO 和 PPO 相结合，得到 TRPO-Lagrangian和PPO-Lagrangian。</p>
<ul>
<li><strong>Constrained Policy Optimization</strong>:CPO 在每次策略更新时分析解决信任区域优化问题(trust region optimization problems)，以在整个训练过程中强制执行约束。它与Chow et al. [2019]. 的 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.462ex;" xmlns="http://www.w3.org/2000/svg" width="14.101ex" height="2.057ex" role="img" focusable="false" viewBox="0 -705 6232.4 909"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"></path></g><g data-mml-node="mo" transform="translate(691.2,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mi" transform="translate(1691.4,0)"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path></g><g data-mml-node="mi" transform="translate(2194.4,0)"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(2645.4,0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g><g data-mml-node="mi" transform="translate(3130.4,0)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></g><g data-mml-node="mi" transform="translate(3542.4,0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g><g data-mml-node="mi" transform="translate(4008.4,0)"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"></path></g><g data-mml-node="mi" transform="translate(4441.4,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g><g data-mml-node="mi" transform="translate(4802.4,0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(5147.4,0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g><g data-mml-node="mi" transform="translate(5632.4,0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g></g></svg></mjx-container> 方法密切相关。与 Achiam et al. [2017]等人不同。we omit the learned failure predictor they used for cost shaping.。</li>
</ul>
<p><strong>Hyperparameters</strong>：所有实验都使用单独的前馈 MLP 策略和大小为 (256, 256) 的具有 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="4.674ex" height="1.595ex" role="img" focusable="false" viewBox="0 -694 2066 705"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g><g data-mml-node="mi" transform="translate(361,0)"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="mi" transform="translate(890,0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(1490,0)"><path data-c="210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"></path></g></g></g></svg></mjx-container>激活的价值网络。在所有受约束的情况下，我们将 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.186ex;" xmlns="http://www.w3.org/2000/svg" width="6.456ex" height="1.756ex" role="img" focusable="false" viewBox="0 -694 2853.6 776"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g><g data-mml-node="mo" transform="translate(797.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mn" transform="translate(1853.6,0)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path><path data-c="35" d="M164 157Q164 133 148 117T109 101H102Q148 22 224 22Q294 22 326 82Q345 115 345 210Q345 313 318 349Q292 382 260 382H254Q176 382 136 314Q132 307 129 306T114 304Q97 304 95 310Q93 314 93 485V614Q93 664 98 664Q100 666 102 666Q103 666 123 658T178 642T253 634Q324 634 389 662Q397 666 402 666Q410 666 410 648V635Q328 538 205 538Q174 538 149 544L139 546V374Q158 388 169 396T205 412T256 420Q337 420 393 355T449 201Q449 109 385 44T229 -22Q148 -22 99 32T50 154Q50 178 61 192T84 210T107 214Q132 214 148 197T164 157Z" transform="translate(500,0)"></path></g></g></g></svg></mjx-container> 设置为预期成本限制。Point 和 Car 机器人的实验使用了 30, 000 个环境交互步骤的批量大小，而 Doggo 的实验使用了 60, 000 个。Point 和 Car 智能体接受了 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex;" xmlns="http://www.w3.org/2000/svg" width="3.25ex" height="2.02ex" role="img" focusable="false" viewBox="0 -871.1 1436.6 893.1"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mn"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z" transform="translate(500,0)"></path></g><g data-mml-node="mn" transform="translate(1033,393.1) scale(0.707)"><path data-c="37" d="M55 458Q56 460 72 567L88 674Q88 676 108 676H128V672Q128 662 143 655T195 646T364 644H485V605L417 512Q408 500 387 472T360 435T339 403T319 367T305 330T292 284T284 230T278 162T275 80Q275 66 275 52T274 28V19Q270 2 255 -10T221 -22Q210 -22 200 -19T179 0T168 40Q168 198 265 368Q285 400 349 489L395 552H302Q128 552 119 546Q113 543 108 522T98 479L95 458V455H55V458Z"></path></g></g></g></g></svg></mjx-container> 步的训练，Doggo 智能体接受了 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex;" xmlns="http://www.w3.org/2000/svg" width="3.25ex" height="2.005ex" role="img" focusable="false" viewBox="0 -864 1436.6 886"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mn"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z" transform="translate(500,0)"></path></g><g data-mml-node="mn" transform="translate(1033,393.1) scale(0.707)"><path data-c="38" d="M70 417T70 494T124 618T248 666Q319 666 374 624T429 515Q429 485 418 459T392 417T361 389T335 371T324 363L338 354Q352 344 366 334T382 323Q457 264 457 174Q457 95 399 37T249 -22Q159 -22 101 29T43 155Q43 263 172 335L154 348Q133 361 127 368Q70 417 70 494ZM286 386L292 390Q298 394 301 396T311 403T323 413T334 425T345 438T355 454T364 471T369 491T371 513Q371 556 342 586T275 624Q268 625 242 625Q201 625 165 599T128 534Q128 511 141 492T167 463T217 431Q224 426 228 424L286 386ZM250 21Q308 21 350 55T392 137Q392 154 387 169T375 194T353 216T330 234T301 253T274 270Q260 279 244 289T218 306L210 311Q204 311 181 294T133 239T107 157Q107 98 150 60T250 21Z"></path></g></g></g></g></svg></mjx-container> 步的训练。所有episodes的长度都是$T<em>{ep} = 1000<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.452ex;" xmlns="http://www.w3.org/2000/svg" width="60.079ex" height="2.149ex" role="img" focusable="false" viewBox="0 -750 26555 950"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><text data-variant="italic" transform="scale(1,-1)" font-size="884px" font-family="serif" font-style="italic">，</text></g><g data-mml-node="mi" transform="translate(1000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">所</text></g><g data-mml-node="mi" transform="translate(2000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">以</text></g><g data-mml-node="mi" transform="translate(3000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">整</text></g><g data-mml-node="mi" transform="translate(4000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">个</text></g><g data-mml-node="mi" transform="translate(5000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">训</text></g><g data-mml-node="mi" transform="translate(6000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">练</text></g><g data-mml-node="mi" transform="translate(7000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">过</text></g><g data-mml-node="mi" transform="translate(8000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">程</text></g><g data-mml-node="mi" transform="translate(9000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">中</text></g><g data-mml-node="mi" transform="translate(10000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">近</text></g><g data-mml-node="mi" transform="translate(11000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">似</text></g><g data-mml-node="mi" transform="translate(12000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">约</text></g><g data-mml-node="mi" transform="translate(13000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">束</text></g><g data-mml-node="mi" transform="translate(14000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">满</text></g><g data-mml-node="mi" transform="translate(15000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">足</text></g><g data-mml-node="mi" transform="translate(16000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">度</text></g><g data-mml-node="mi" transform="translate(17000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">对</text></g><g data-mml-node="mi" transform="translate(18000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">应</text></g><g data-mml-node="mi" transform="translate(19000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">的</text></g><g data-mml-node="mi" transform="translate(20000,0)"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"></path></g><g data-mml-node="mi" transform="translate(20433,0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g><g data-mml-node="mi" transform="translate(20918,0)"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path></g><g data-mml-node="mi" transform="translate(21387,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g><g data-mml-node="mi" transform="translate(21748,0)"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(22199,0)"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="mi" transform="translate(22728,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g><g data-mml-node="mi" transform="translate(23089,0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g><g data-mml-node="mi" transform="translate(23555,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">的</text></g><g data-mml-node="mi" transform="translate(24555,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">值</text></g><g data-mml-node="mi" transform="translate(25555,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">为</text></g></g></g></svg></mjx-container>\rho_c = d/T</em>{ep} = 0.025。$</p>
<p>我们为每个算法类别手动调整超参数以获得合理的性能。但是，我们警告说，我们的手动调整不应被视为每个算法类的最佳性能的指示。</p>
<p>所有实验均使用三个随机种子进行。</p>
<h5 id="5-3-Results"><a href="#5-3-Results" class="headerlink" title="5.3 Results"></a>5.3 Results</h5><p>在图 7、8 和 9 中，我们展示了在constrained Safety Gym 环境中评估无约束和受限 RL 算法的学习曲线。这些学习曲线描绘了未经归一化的指标 Jr(θ)、Jc(θ) 和 ρc(θ)，并显示了每种算法的绝对性能。在表 1 和表 2 中，我们报告了训练结束时在各种环境下取平均值的标准化指标。The normalized values allow easy comparison to a reference point (in this case, unconstrained PPO).</p>
<embed id="pdfPlayer" src="D:\data\typora\pdf\pic1.pdf" type="application/pdf" width="100%" height="600">


<embed id="pdfPlayer" src="D:\data\typora\pdf\pic.pdf" type="application/pdf" width="100%" height="600">

<embed id="pdfPlayer" src="D:\data\typora\pdf\pic2.pdf" type="application/pdf" width="100%" height="600">

<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202305072125020.png" alt="image-20230507212505948"></p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202305072125486.png" alt="image-20230507212517396"></p>
<p>We observe a few general trends:</p>
<ul>
<li>成本和回报有意义地相互权衡。不受约束的 RL 算法能够通过采取不安全的操作来获得高回报，如成本函数所衡量的那样。受约束的 RL 算法获得较低水平的回报，并相应地保持所需的成本水平。（我对这段话存疑，不受约束的RL算法并不一定是采取不安全的操作来获得的高回报，高回报的产生可能是因为不受约束的RL算法自由探索到一个比较好的策略，相比受约束的RL算法其探索效率高，这是我的看法）</li>
<li>The design decision to make Level 2 Safety Gym environments denser in unsafe elements than Level 1 environments is reflected by the jump in average episodic cost for unconstrained agents.</li>
<li>CPO 中的近似误差似乎会阻止它完全满足几乎所有这些环境的约束。我们推断这些环境比之前测试过 CPO 的环境更难。相比之下，拉格朗日方法或多或少可靠地执行约束，尽管存在近似误差。这与 Achiam 等人的结果相矛盾。 [2017].</li>
<li>拉格朗日方法能够找到满足约束的策略，在几个Point environments中获得不平凡的回报，这表明在控制学习机器人运动中的挑战时，可以在受限 RL 上取得进展，甚至可以解决这些环境。</li>
<li>标准 RL 能够控制 Doggo 机器人并获得复杂的运动行为，正如在无约束训练时环境中的高回报所表明的那样。然而，尽管在没有运动要求时约束 RL 取得了成功，并且在需要运动时标准 RL 取得了成功，但我们研究的约束 RL 算法难以学习安全运动策略。需要进行更多研究来开发可以解决这些具有挑战性的任务的受限 RL 算法。</li>
</ul>
<h4 id="6-Conclusions"><a href="#6-Conclusions" class="headerlink" title="6 Conclusions"></a>6 Conclusions</h4><p>在这项工作中，我们采取了三个主要步骤来解决安全探索问题。我们建议将受约束的 RL 标准化为安全探索的主要形式，在训练结束时的约束满意度和整个训练过程中的约束违反后悔方面设定明确的进步目标。我们介绍了 Safety Gym，这是用于评估受限 RL 算法性能的第一个高维连续控制环境基准。最后，我们在 Safety Gym 环境中评估了基线无约束和约束 RL 算法，以部分阐明安全探索的最新技术水平。</p>
<p>我们认为有许多途径对未来的工作很有前途。</p>
<p><strong>Advancing SOTA on Safety Gym</strong>:</p>
<p>我们对受限 RL 的基线结果表明需要更强大和/或更好调整的算法才能在 Safety Gym 环境中取得成功。所谓成功，我们的意思是同时沿着情景回归轴和约束后悔轴实现改进，同时在训练结束时仍然产生约束满足策略。这项工作中未探索的受限 RL 的现有技术可能会在这里取得进展，但是，我们预计实质性和一致的性能提升将需要新的见解。我们注意到，没有replay buffers的standard model-free RL方法从根本上限制了它们最大程度地减少约束遗憾（constraint regre）的能力：它们必须不断经历不安全事件才能了解它们。因此，我们认为memory-based 和model-based RL  方法在这里特别有趣。</p>
<p><strong>Safe Transfer Learning:</strong>我们建议使用 Safety Gym 工具来调查与受限 RL 设置中的safe transfer和distributional shift相关的两个问题。</p>
<ul>
<li>问题 1：智能体最初在一个受限 RL 环境中接受训练，然后转移到另一个任务相同但安全要求不同的环境中。在这种设置下，安全问题是智能体能否快速适应新的安全要求。</li>
<li>问题 2：智能体最初在一个受限 RL 环境中接受训练，然后转移到另一个安全要求相同但任务不同的环境中。在这种情况下，安全问题是尽管目标函数的变化可能导致灾难性遗忘，但代理是否可以保持约束满足。</li>
</ul>
<p>Problem 1 can be investigated with unmodified Safety Gym benchmark environments, using the Level 1 and 2 versions of each task as (First Environment, Second Environment) pairs. New environments can easily be created for both problems using the Safety Gym Engine tool.</p>
<p><strong>Constrained RL with Implicit Specifications:</strong>RL with implicitly-specified objectives is a research sub-field with important consequences for safety, encompassing inverse reinforcement learning, learning from human preferences, and other heuristics for extracting value-aligned objectives from human data. As we discussed earlier, these techniques are complementary to and compatible with constrained RL, and thus we recommend research in the direction of combining them. Safety Gym benchmark environments can be used to study whether such combination techniques are efficient at training agents to satisfy implicitly-specified safety requirements.</p>
<p>同样，Engine的大部分概念性细节在论文中都有描述。但在这里，我们将描述一些论文中没有涉及的模式和代码细节。</p>
<p>传感器的默认值： 默认情况下，唯一启用的传感器是基本的机器人传感器：加速度计、陀螺仪、磁力计、速度计、关节角度和关节速度。所有其他的传感器（用于感知场景中物体的激光雷达、视觉、罗盘、剩余时间和其他一些传感器）默认是禁用的。要使用它们，你必须通过引擎配置传入标志来明确地启用它们。注意，简单地将一个物体添加到场景中不会导致该物体的相应传感器被启用，你必须传递标志。</p>
<p>视觉： 视觉是作为一个选项包括在内的，但是支持程度相当低，我们还没有对它进行广泛的测试。相对于其他功能，与视觉有关的功能请求或错误修复将被视为低优先级。</p>
<p>激光雷达和伪激光雷达： 激光雷达和伪激光雷达是观察物体的主要方法。激光雷达的工作方式是光线追踪（使用MuJoCo提供的工具），而伪激光雷达的工作方式是在场景中的所有物体上循环，确定它们是否在范围内，然后用正确的值填充适当的激光雷达仓。它们都有几个共同的细节：在这两种情况下，每个激光雷达都有固定数量的仓，围绕机器人的一整圈均匀分布。</p>
<p>类似激光雷达的观测是针对特定对象的。也就是说，如果你在一个场景中有危险、花瓶和目标，你会想要打开危险激光雷达（通过observe_hazards），花瓶激光雷达（通过observe_vases），可能还有目标激光雷达（通过observe_goal_lidar）。</p>
<p>所有类似激光雷达的观测将是真正的激光雷达或伪激光雷达，取决于lidar_type标志。默认情况下，lidar_type=’pseudo’。要使用真正的激光雷达，请设置lidar_type=’自然’。</p>
<p>激光雷达观测数据由 “激光雷达光环 “直观地表示出来，光环盘旋在代理人的上方。每个激光雷达光环都有和激光雷达仓一样多的球体，如果有物体在其相应的仓的范围内，球体就会亮起。激光雷达光环是非物理性的，不与场景中的物体互动；它们纯粹是为观看代理视频的人服务的，这样就可以清楚地看到代理正在观察什么。</p>
<p>具体到伪激光雷达：通常情况下，类似激光雷达的观察会破坏关于状态的小变化导致观察的小变化的原则，因为状态的小变化可以将一个物体从一个仓移到另一个仓。我们为每个仓添加一个小的 “别名 “信号到相邻的仓中，这样可以平滑仓之间的过渡，另外还可以使观测在一个仓中弱化物体的定位。</p>
<p>对象和约束条件的默认值： 默认情况下，一个场景中唯一存在的东西是机器人（默认为汽车）。其他东西都必须明确添加。在场景中添加障碍物（比如危险或花瓶）并不会自动添加约束；如果你想让与障碍物的交互受到约束，你必须同时传递标志来启用约束。</p>
<p>环境布局： 默认情况下，环境布局是在每一集的开始时随机生成的。这种行为可以通过设置randomize_layout=False来禁用，在这种情况下，环境布局在初始化时被随机化一次，然后在每个新情节开始时被重置为相同的布局。随机布局的生成是通过抽样进行的，可能会失败：生成器在场景中随机放置物体，直到出现冲突（例如两个物体不可接受地重叠了）。如果它不能通过重新取样最后放置的物体来解决冲突，它就会抛出布局并重新开始。如果它在尝试了（大的）固定次数之后仍然不能找到一个有效的布局，Engine会引发一个异常。与随机对象放置有关的细节将在下面描述。</p>
<p>放置、位置和保持： 对于你可以添加到Safety Gym环境中的所有不同种类的物体，你可以通过它们的{object}s_placements、{object}s_locations和{object}s_keepout标志来配置它们在场景中的位置。你可以设置它们在每一集开始时随机放置在场景中（通过放置），或者固定在特定的位置（通过位置），你还可以控制它们与场景中其他物体的距离（通过保持）。</p>
<p>{object}s_placements应该是一个(xmin, ymin, xmax, ymax)图元的列表，其中每个图元描述了一个矩形区域，物体可以被随机放置。如果没有给出，它将默认为场景的全部尺寸（由placements_extents标志给出）。</p>
<p>{object}s_locations应该是一个(x,y)位置的列表，这样的物体应该准确地去那里。</p>
<p>在一个情节的开始，当环境布局被采样时，布局采样器将首先满足{object}s_locations的要求。假设场景中会有4个物体（用{object}s_num指定），而{object}s_locations是一个包含2个（x,y）位置的列表。然后，2个物体将被放置在这些位置上。之后，剩下的2个对象将根据{object}s_placements随机定位。如果位置多于物体，多余的位置将被忽略。</p>
<p>{object}s_keepout指定了一个物体位置周围的半径，其他物体必须远离。在设置时要注意：如果对象和它们的保持范围太大，而且场景中的对象太多，布局采样器可能无法生成一个可行的布局。</p>
<h3 id="9-Multi-Agent-Constrained-Policy-Optimisation"><a href="#9-Multi-Agent-Constrained-Policy-Optimisation" class="headerlink" title="9 Multi-Agent Constrained Policy Optimisation"></a>9 Multi-Agent Constrained Policy Optimisation</h3><h3 id="10-Deep-Reinforcement-Learning-for-Multi-Agent-Systems-A-Review-of-Challenges-Solutions-and-Applications-Review"><a href="#10-Deep-Reinforcement-Learning-for-Multi-Agent-Systems-A-Review-of-Challenges-Solutions-and-Applications-Review" class="headerlink" title="10 Deep Reinforcement Learning for Multi-Agent Systems: A Review of Challenges, Solutions and Applications[Review]"></a>10 Deep Reinforcement Learning for Multi-Agent Systems: A Review of Challenges, Solutions and Applications[Review]</h3><p>文章首先讲述了DRL的发展过程及遇到的挑战和克服所用到的技术，然后讲述了MAS遇到的挑战和一些相关的算法，主要从部分可观测，非稳态，连续动作空间，多智能体的训练模式和多智能体的迁移学习等方面来讲解MAS遇到的挑战</p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202305151645049.png" alt="image-20230515164550957"></p>
<p>Abstract</p>
<p>强化学习（RL）算法已经存在了几十年，并被用来解决各种连续的决策问题。然而，这些算法在处理高维环境时面临巨大挑战。最近，深度学习的发展使RL方法能够为复杂和有能力的代理驱动最佳政策，从而在这些具有挑战性的环境中有效地执行。本文讨论了深度RL的一个重要方面，即与需要多个代理进行沟通和合作以解决复杂任务的情况有关。本文介绍了与多代理深度RL（MADRL）相关问题的不同方法，包括非平稳性、部分可观察性、连续状态和行动空间、多代理训练计划、多代理转移学习。将分析和讨论这些方法的优点和缺点，并对其相应的应用进行探讨。可以预见的是，这篇评论提供了关于各种MADRL方法的见解，并能导致未来开发出更强大和非常有用的多Agent学习方法来解决现实世界的问题。</p>
<p>Introduction</p>
<p>RL 从TE（trial and error) procedure 开始不断发展，从心理学中引入TD（temporal-difference）机制整合到TE学习模型中，1989 年，Watkins 和 Dayan将包括贝尔曼方程（Bellman equation）和马尔可夫决策过程（Markov decision process）在内的最优控制理论 与时间差分学习一起形成了著名的 Q-learning。这是一个巨大的进步，当Q-learning在现实世界中解决问题时，遇到了curse of dimensionality，超越传统计算机的计算限制。2015 年，Mnih 等人   通过将深度学习与 RL 相结合来部分克服维数灾难取得了重要突破。 Deep RL 已经成为人工智能的规范方法，从那时起就引起了研究界的极大关注。 RL 发展的里程碑如图 1 所示，从试错法到深度 RL。</p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202305131125480.png" alt="image-20230513112510237"></p>
<p>2 Background: Reinforcement Learning</p>
<p>讲了DRL的发展，从 最基本的Terminology，到贝尔曼方程，然后引入值函数，并介绍了Monte-Carlo method和TD method 两个方法来approximate the value functions.，且分析了其优缺点。</p>
<p>3 Deep RL: Single Agent</p>
<p>主要讲了深度Q网络方法，从DQN 到DDQN，到Duling DQN，再到DRQN，DARQN等。讲述了DQN的优点，解决了维数灾难，然后分析其缺点，并介绍其变体及其性能的比较，并说明用了什么trick来解决的。</p>
<p>4 Deep RL: Multi-Agent</p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202305150937737.png" alt="image-20230515093725570"></p>
<p>（注：MAS：Multi-Agent System）</p>
<p>4.1 MADRL: Challenges and Solutions </p>
<p>4.1.1 Non-stationarity</p>
<p>与单个代理设置相比，控制多个代理带来了几个额外的挑战，例如代理的异质性、如何定义合适的集体目标或需要设计紧凑表示的大量代理的可扩展性，以及更重要的非平稳性问题。在单代理环境中，代理只关心其自身行为的结果。在多智能体域中，智能体不仅观察其自身行为的结果，而且观察其他智能体的行为。代理之间的学习很复杂，因为所有代理都可能相互交互并同时学习。多个代理之间的交互不断重塑环境并导致非平稳性。在这种情况下，代理之间的学习有时会导致代理策略的变化，并可能影响其他代理的最优策略。</p>
<p>对一个动作的潜在回报的估计是不准确的，因此，在多智能体设置中给定点的良好策略在未来不可能保持不变。应用于单代理设置的 Q 学习收敛理论不能保证大多数多代理问题，因为马尔可夫属性在非平稳环境中不再成立 [40]。因此，收集和处理信息必须以一定的重复性进行，同时确保它不会影响代理的稳定性。在多主体设置下，探索-开发困境可能更多地涉及。</p>
<p>流行的独立 Q 学习 [112] 或基于经验回放的 DQN [73] 不是为非平稳环境设计的。 Castaneda [12] 提出了 DQN 的两种变体，即深度重复更新 Q 网络（DRUQN）和深度松散耦合 Q 网络（DLCQN），以处理 MAS 中的非平稳性问题。 DRUQN 是基于 [1, 2] 中介绍的重复更新 Q 学习 (RUQL) 模型开发的。它旨在通过更新与选择动作的可能性成反比的动作值来避免策略偏差。另一方面，DLCQN 依赖于 [119] 中提出的松耦合 Q 学习，它使用负奖励和观察为每个代理指定和调整独立度。通过这种独立度，智能体学会在不同情况下决定是需要独立行动还是与其他智能体合作。同样，Diallo 等人。 [18] 将 DQN 扩展到多代理并发 DQN，并证明该方法可以在非平稳环境中收敛。福斯特等人。 [25] 或者介绍了两种方法来稳定 MADRL 中 DQN 的经验回放。第一种方法使用重要性采样方法自然衰减过时数据，而第二种方法使用指纹消除从重放存储器中检索到的样本的年龄。</p>
<p>最近，为了处理由于 MAS 中多个智能体的并发学习引起的非平稳性，Palmer 等人。 [86] 提出了一种称为 lenient-DQN (LDQN) 的方法，该方法应用 leniency 和衰减的温度值来调整从经验回放内存中采样的策略更新（图 7）。该方法应用于协调的多代理对象传输问题，并将其性能与滞后 DQN (HDQN) [85] 进行了比较。实验结果证明了 LDQN 在随机奖励环境中收敛到最优策略方面优于 HDQN。宽大的概念和计划的重放策略也被纳入 [120] 中的加权双深度 Q 网络（WDDQN），以处理 MAS 中的非平稳性。实验表明，在具有随机奖励和大状态空间的两个多代理环境中，WDDQN 相对于双 DQN 具有更好的性能。</p>
<blockquote>
<p>ChatGPT Note<br>在文中所提到的方法中，leniency（宽容性）是指对于智能体在经验回放中遇到的低奖励或失败情况，采取一种容忍的态度而不是严格地惩罚智能体的策略。通过采用宽容性，智能体可以在学习过程中更好地探索状态空间，从而增加学习的鲁棒性和稳定性。</p>
<p>而衰减的温度值则是一种在经验回放中调整采样策略更新的方法。温度值是一种参数，用于控制从经验回放内存中选择动作的随机性程度。在开始阶段，温度值较高，使得动作的选择更加随机。随着学习的进行，温度值逐渐减小，降低了随机性，使得智能体更加倾向于选择具有较高价值的动作。通过衰减温度值，智能体在学习的早期可以更多地进行探索，而在学习的后期更加依赖已经学到的知识，从而平衡了探索与利用之间的权衡。</p>
<p>综合来说，lenient-DQN（LDQN）方法通过使用leniency和衰减的温度值来调整策略更新的采样方式，从而处理由于多个智能体的并发学习引起的非平稳性。这种方法可以增强智能体的鲁棒性和稳定性，促进更好的学习效果。</p>
</blockquote>
<p>4.1.2 Partial observability</p>
<p>4.1.3 MAS training schemes</p>
<p>4.1.4 Continuous action spaces</p>
<p>4.1.5 Transfer Learning for MADRL</p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202305151643621.png" alt="image-20230515164348513"></p>
<p>4.2 MADRL Applications</p>
<p>本节对这些应用进行了调查，重点是深度学习和 MARL 的集成。表 4 总结了这些应用方法的特点和局限性。也就是对于上述章节讲到的算法加上相应的应用场景</p>
<p><img src="https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202305151649313.png" alt="image-20230515164940219"></p>
<p>5 Conclusions and Research Directions</p>
<p>本文概述了多Agent学习中的不同挑战以及使用深度RL方法对这些挑战的解决方案。我们将所调查的论文分为五类，包括非平稳性、部分可观测性、多Agent训练方案、多Agent转移学习以及连续状态和行动空间。我们强调了解决这些挑战的方法的优点和缺点。MADRL方法在不同领域的应用也得到了全面回顾。我们发现，将深度学习融入传统的MARL方法已经能够解决许多复杂的问题，如城市交通灯控制、零能耗社区的能源共享问题、大规模车队管理、任务和资源分配、蜂群机器人技术和社会科学现象。结果表明，基于深度RL的方法为处理MAS领域的复杂任务提供了一种可行的方法。</p>
<p>从示范中学习，包括模仿学习和逆向RL已经在单体深度RL中显示出有效性[91]。一方面，模仿学习试图将状态与行动之间的映射作为一种监督的方法。它直接将专家策略推广到未访问的状态，因此在有限行动集的情况下，它接近于一个多类分类问题。另一方面，逆向RL代理需要从专家的演示中推断出一个奖励函数。逆向RL假设专家政策是关于未知奖励函数的最优策略[31, 32]。然而，这些方法还没有在多代理环境中得到充分的探索。模仿学习和逆向RL在MAS中都有很大的应用潜力。预计它们可以减少学习时间，提高MAS的效率。这些应用中出现的一个非常直接的挑战是，需要有多个专家能够协同演示这些任务。此外，专家的交流和推理能力很难被MAS领域的自主代理所描述和模拟。这些都为模仿学习和逆向RL扩展到MADRL方法提出了重要的研究问题。此外，对于人类难以展示的复杂任务或行为，需要有替代的方法，使人类的偏好能够被整合到深度RL中。</p>
<p>深度RL在很大程度上促进了自主性，这使得在机器人或自主车辆中部署了许多应用。然而，深度RL模型最常见的缺点是通过人机合作技术与人互动的能力。在复杂和对抗性的环境中，迫切需要人类的智慧与技术相结合，因为仅靠人类是无法维持工作量的，而仅靠机器也无法在新情况出现时做出创造性的反应。最近的人在环结构[78]的进展可以与MADRL融合，以整合人类和自主代理来处理复杂问题。在传统的 “human-in-the-loop “设置中，代理在一段时间内自主地执行其分配的任务，然后停止并等待人类的命令，然后继续以这种速度限制的方式进行。在 “human-in-the-loop “中，代理自主地执行其任务，直到完成，而处于监测或监督角色的人则保留干预代理所进行的操作的能力。如果人类监督员允许代理完全依靠自己的力量完成任务，那么基于人在环的架构就可以完全自主。</p>
<p>无模型的深层RL已经能够解决单代理和多代理领域的许多复杂问题。然而，这类方法需要大量的样本和较长的学习时间来实现良好的性能。基于模型的方法在样本效率、可转移性和通用性方面，在使用单一以及多代理模型的各种问题上表现出了有效性。虽然基于模型的方法的深度学习扩展最近在单一代理领域得到了研究，例如[64, 29, 22, 77, 103, 15]，但这些扩展在多代理领域还没有被广泛研究。这就产生了一个研究空白，可以发展为基于模型的MADRL的研究方向。此外，使用基于模型的方法处理高维观测，或者结合基于模型的规划和无模型的策略的元素，是另一个活跃的、令人兴奋的但未被充分探索的研究领域。</p>
<p>从RL的第一天起，扩展到大型系统，特别是处理许多异质代理，一直是RL研究领域的主要挑战。随着世界动态变得越来越复杂，这一挑战一直需要解决。由于代理有共同的行为，如行动、领域知识和目标（同质代理），可扩展性可以通过（部分）集中训练和分散执行来实现[94, 28]。在有许多代理的异质环境中，关键的挑战是如何提供最优化的解决方案，并在自我学习的基础上最大限度地提高任务完成的成功率，在代理之间采取有效的协调和合作策略。这在代理人之间通信有限的敌对环境中和涉及更多异质代理人的场景中问题更大，因为信用分配问题变得越来越困难。解决这些困难的研究方向是非常值得研究的。</p>
<p>关于多代理学习的应用，已经有很多研究使用传统的MARL方法来解决各种问题，如控制一群自主车辆或无人机[43]、机器人足球[102]、控制交通信号[75]、协调工厂和仓库中的协作机器人[47]、控制电力网络[93]或优化分布式传感器网络[37]、自动交易[88]、竞争性电子商务和金融市场的机器竞价[9]、资源管理[44]、运输[21]以及社会科学的现象[62]。自从DQN[73]出现后，在文献中发现了将传统RL扩展到多代理领域的深度RL的努力，但它们仍然非常有限（目前文献中的应用情况见表4）。基于MADRL的高维度处理能力，MARL的许多应用现在可以由MADRL有效解决。因此，需要进一步的实证研究来应用MADRL方法有效地解决复杂的实际问题，如上述的应用。</p>
<h2 id="Q-amp-A"><a href="#Q-amp-A" class="headerlink" title="Q&A"></a>Q&amp;A</h2><ol>
<li><p>what is Pessimistic method in RL?</p>
<p>In reinforcement learning, the pessimistic method is an algorithm that learns the worst-case value of each state or action, rather than the expected value. This approach can be useful in situations where the consequences of making a mistake can be very severe, and it is important to be conservative in decision-making.</p>
<p>One way to implement the pessimistic method is to use a modified version of the Bellman equation, which is used to compute the value of each state or action in a Markov decision process. In the pessimistic version of the equation, the minimum value of the next state or action is used, rather than the expected value. This leads the algorithm to prefer actions that minimize the worst-case outcome, rather than actions that maximize the expected reward.</p>
<p>The pessimistic method can be contrasted with the optimistic method, which learns the best-case value of each state or action, and the average method, which learns the expected value. Each of these approaches has its own strengths and weaknesses, and which one is best will depend on the specific problem being solved and the goals of the decision-maker.</p>
</li>
<li><p>what is OOD Action?</p>
<p>Out-of-Distribution (OOD) action in reinforcement learning (RL) refers to actions that the agent may take that are not part of the training distribution of actions. These actions can occur when the agent is operating in a different environment or context than it was trained on, or when the agent is asked to perform a task that it has not seen before. OOD actions can be challenging for an RL agent to handle, as they may not be well-suited to the current task or may lead to unexpected outcomes. One way to mitigate the risk of OOD actions is to train the RL agent on a diverse set of tasks and environments, so that it has a greater range of experience to draw upon when making decisions.</p>
</li>
<li><p>what is discounted state-action occupancy？</p>
<p>Discounted state-action occupancy refers to a measure of the frequency with which an agent or a system visits a particular state-action pair in a Markov decision process (MDP). The discount factor, denoted by γ, determines the weight of future rewards in the total expected reward. It allows the agent to prioritize short-term rewards over long-term rewards, or vice versa, depending on the value of γ. The discounted state-action occupancy is often used in reinforcement learning, a subfield of machine learning, to evaluate the quality of an agent’s policy and to compare different policies. It can also be used in planning and control to determine the optimal policy for an MDP.</p>
</li>
<li><p>downlink和uplink是什么意思？</p>
<p>Downlink和Uplink是无线通信中的术语。</p>
<p>Downlink通常指的是从基站或卫星等地面设备向用户设备发送数据的通信链路。在移动通信领域，downlink指的是从电信网络向移动设备发送数据，例如在接收短信、接听电话或接收互联网数据时使用。</p>
<p>Uplink通常指的是从用户设备向基站或卫星等地面设备发送数据的通信链路。在移动通信领域，uplink指的是从移动设备向电信网络发送数据，例如在发送短信、拨打电话或上传互联网数据时使用。</p>
<p>总的来说，Downlink和Uplink是指无线通信过程中数据传输的方向，Downlink是从基站到设备，Uplink是从设备到基站。</p>
<p>Downlink和Uplink在无线通信系统中是指从基站到用户设备的信号传输（下行）和从用户设备到基站的信号传输（上行）。</p>
<p>这两个方向的信号传输是耦合在一起的，因为它们都需要在同一个频率和时间资源上传输。在无线通信系统中，频谱资源是有限的，因此需要对下行和上行的信号进行协调和管理，以避免信号冲突和干扰。因此，下行和上行之间需要进行时间和频率的协调，以确保它们不会相互干扰。</p>
<p>此外，下行和上行之间也需要进行信号控制和协调，以确保它们能够在同一时间内传输，并在传输时避免相互干扰。这需要在通信系统中进行复杂的协议和协调，以确保下行和上行能够同时进行，而不会相互干扰。</p>
<p>因此，Downlink和Uplink是耦合在一起的，需要协调和管理它们之间的频率、时间和信号控制，以确保它们能够同时进行，并在传输时避免相互干扰。</p>
<p>虽然在某些特定的场景下，可以采用一些技术手段来实现Downlink和Uplink的分离，但这通常需要更多的频谱资源和更复杂的信号处理技术。</p>
<p>例如，在一些多天线系统中，可以采用空分复用技术（Space Division Multiple Access，SDMA），将Downlink和Uplink的信号通过不同的天线进行传输，从而实现Downlink和Uplink的空间分离。但是，这种方法需要更多的天线和更复杂的信号处理技术，因此在实际应用中比较少见。</p>
<p>因此，虽然Downlink和Uplink在理论上可以分离，但在实际无线通信系统中，它们是紧密耦合在一起的，并需要进行复杂的协调和管理，以确保它们能够同时进行，并在传输时避免相互干扰。</p>
<p>在无人机作为基站的情况下，Downlink和Uplink的耦合问题与传统基站的情况有所不同，因为无人机作为基站的移动性更强，需要解决更多的技术难题。</p>
<p>一种解决Downlink和Uplink耦合问题的方法是使用分时多址（Time Division Multiple Access，TDMA）或频分多址（Frequency Division Multiple Access，FDMA）技术。通过将Downlink和Uplink的时间或频率分隔开，使其在不同的时间或频率上进行传输，从而避免信号冲突和干扰。这种方法需要在无人机和移动设备上分别实现时钟同步和频率同步，以确保Downlink和Uplink能够正确地在不同的时间或频率上进行传输。</p>
<p>另一种解决Downlink和Uplink耦合问题的方法是使用空分复用（Space Division Multiple Access，SDMA）技术。在这种技术中，无人机基站可以使用多个天线同时向多个移动设备进行Downlink传输，同时通过智能信号处理技术对不同的移动设备的信号进行分离，从而实现Downlink和Uplink的空间分离。</p>
<p>除了上述方法之外，还可以采用其他技术手段来解决Downlink和Uplink耦合的问题，例如采用OFDMA技术，通过动态分配子载波资源，将Downlink和Uplink在频域上分离。但是，这些方法都需要在无人机和移动设备上进行复杂的信号处理和协议设计，实现起来比较复杂。</p>
</li>
<li><p>NLoS and LoS</p>
<p>NLoS和LoS是无线通信中的两个术语，用来描述信号的发射器和接收器之间的路径。</p>
<p>LoS是 “视线 “的意思，指的是发射器和接收器之间的直接、无障碍的路径。这意味着没有任何物理障碍，如建筑物、山丘或树木，阻挡信号。在LoS方案中，信号通过空气传播，在几乎没有干扰的情况下被接收器接收。LoS是无线通信的理想场景，因为它提供最好的信号质量、最高的数据速率和最低的延迟。</p>
<p>另一方面，NLoS是 “非视线 “的意思。在这种情况下，信号路径受到物理障碍物的阻挡，如建筑物、山丘或树木，这些障碍物会导致信号反弹、散射或衍射。因此，信号强度会降低，信号质量也会下降。与LoS通信相比，NLoS通信可能导致更高的错误率，更低的数据率和更高的延迟。</p>
<p>在无线通信中，考虑信号是通过LoS还是NLS路径是很重要的，因为它可以影响信号质量，并最终影响系统的性能。诸如信号处理、天线设计和频率选择等技术可用于减轻NLS传播的影响并提高无线系统的性能。</p>
</li>
<li><p>the backhaul rate of UAV</p>
<p>无人机（UAV）的回程率是指无人机和地面控制站之间的通信链路的数据传输率。它是数据可以从无人机发送到地面控制站的速率，反之亦然。</p>
<p>无人机的回程速率取决于几个因素，包括使用的通信技术、传输距离、使用的频段和可用带宽。一般来说，使用的频段越高，可用带宽越宽，可以实现的回程率就越高。</p>
<p>在实践中，无人机的回程率会因具体实施和环境因素的不同而有很大差异。例如，在无线电频率干扰水平较低的农村地区运行的无人机可能比在干扰水平较高的城市地区运行的无人机能够实现更高的回程率。</p>
<p>总的来说，无人机的回程率是无人机设计和运行中的一个重要考虑因素，包括监视、测绘和包裹运送等一系列应用。</p>
</li>
<li><p>传输速率和回程速率什么关系</p>
<p>传输速率和回程速率是通信中的两个概念，它们通常指代的是在一个通信链路中的数据传输速度。传输速率通常指从源端到目的端的数据传输速度，而回程速率则指从目的端返回到源端的数据传输速度。</p>
<p>传输速率和回程速率是相互关联的，因为它们都是同一个通信链路中的数据传输速度的不同方面。在一个通信链路中，传输速率和回程速率可能会不同，这取决于许多因素，例如链路的物理特性、使用的通信技术、信号传输距离、信道质量和网络拥塞情况等。在实践中，传输速率和回程速率通常都受到网络拥塞、信噪比、干扰和信号衰减等因素的影响。</p>
<p>如果传输速率和回程速率不平衡，会导致通信延迟和数据传输速度下降。因此，在设计和部署通信网络时，需要平衡传输速率和回程速率，并采取措施来确保它们的稳定性和可靠性，以实现高效的数据传输和通信服务。</p>
</li>
<li><p>什么是in-band full- duplex communication？</p>
<p>In-band full-duplex communication是一种数据通信的方式，它允许在同一频段或信道上进行双向通信。在这种通信方式中，数据的发送和接收是同时进行的，不需要像半双工通信那样进行轮流传输。这意味着两个通信端口可以同时发送和接收数据，而不会相互干扰。</p>
<p>在传统的通信方式中，半双工通信和全双工通信是最常见的。半双工通信只允许数据在一个方向上传输，而全双工通信可以在两个方向上传输数据，但需要不同的频段或信道。而in-band full-duplex communication允许在同一频段或信道上进行双向通信，大大提高了通信效率和频谱利用率。</p>
<p>这种通信方式在无线电通信、计算机网络和其他领域都有广泛应用，比如使用WiFi进行视频通话，使用蓝牙进行音频传输等。</p>
</li>
<li><p>什么是evolutionary game？</p>
<p>进化博弈（Evolutionary Game）是一种研究生物种群在进化过程中如何通过博弈（Game）策略来获得生存和繁衍优势的数学模型。</p>
<p>在进化博弈模型中，生物个体的策略是基于其基因和环境的组合，而这些策略在个体之间通过博弈进行相互竞争。每个个体都有一个收益函数，用于度量其获得生存和繁殖成功的能力。这些收益函数随着时间而变化，因为它们取决于种群中的策略分布。</p>
<p>在进化博弈模型中，个体可以采用不同的策略，例如“合作”和“背叛”。合作的策略可能导致双方都获得收益，而背叛的策略可能会导致其中一方获得更多收益，另一方则获得较少或者没有收益。在博弈的每一轮中，个体之间相互竞争并且根据收益函数来获得收益。然后，种群中的个体按照它们的收益水平进行繁殖，使得获得高收益的个体在下一代中更为普遍。</p>
<p>进化博弈模型可以用来研究各种生物现象，例如动物中的群体行为，植物的竞争行为以及人类社会中的协作和竞争行为。这种模型已经得到了广泛的应用，并且在理论生物学、社会学、经济学和计算机科学等领域中有着重要的意义。</p>
</li>
<li><p>什么是5G HetNet?</p>
<p>5G HetNet（异构网络）是指使用多种类型的无线接入技术的网络架构，如蜂窝网络、Wi-Fi和小型蜂窝，为用户提供无缝连接和高数据速率。</p>
<p>在5G HetNet中，不同类型的接入节点以一种协调的方式部署，以提高覆盖范围、容量和网络性能。例如，宏蜂窝用于提供广域覆盖，而小蜂窝，如微蜂窝、皮米蜂窝和飞蜂窝，则用于提高密集城市地区、室内环境和农村地区的覆盖和容量。</p>
<p>在5G HetNet中使用不同类型的接入节点需要先进的网络管理技术，如网络切片、动态频谱分配和不同节点之间的协调。这些技术能够有效地利用网络资源，提高服务质量，并改善用户体验。</p>
<p>5G HetNet有可能彻底改变我们使用和体验无线通信的方式，为各种设备和应用提供高速、低延迟的连接。</p>
</li>
<li><p>什么是wireless backhaul links?</p>
<p>无线回程链路是指用于连接基站、接入点或其他网络节点与核心网络或互联网的无线连接。这些链路是通信基础设施的重要组成部分，因为它们为数据流量在终端用户设备和核心网络之间流动提供了必要的带宽和连接。</p>
<p>在无线回程链路中，数据通过无线电波或其他无线技术（如微波或毫米波频段）进行空中传输。这些链接可用于连接远程站点，将覆盖范围扩大到有线回程不可行或不具成本效益的地区，并为事件或紧急情况提供临时连接。</p>
<p>无线回程链路可以在各种频率下运行，从在6GHz以下频段运行的低带宽、长距离链路到在毫米波频段运行的高带宽、短距离链路。频率和技术的选择取决于诸如距离、带宽要求、视线、干扰和成本等因素。</p>
<p>随着5G网络的部署，无线回程链路变得越来越重要，5G网络需要高速、低延迟的连接和大容量的回程链路，以支持对数据密集型应用和服务日益增长的需求。</p>
</li>
<li><p>什么是Robust POMDP？</p>
<p>Robust POMDP是一种偏重于鲁棒性（robustness）的部分可观测马尔可夫决策过程（POMDP）。</p>
<p>POMDP是一种经典的强化学习问题，其基本思想是在不完全观察到环境状态的情况下，代理需要在一组可能的动作和状态之间进行决策，以最大化长期奖励。</p>
<p>然而，由于POMDP通常涉及对环境状态的不完全观察，因此存在一定的风险和不确定性，可能导致代理所做出的决策并不是最佳的或者不可行。因此，研究者们提出了许多方法来提高POMDP的鲁棒性，以降低不确定性和风险。</p>
<p>Robust POMDP是其中之一，它特别关注于如何在不完全观察到环境状态的情况下，通过设计鲁棒的策略来最大化长期奖励，并尽可能地避免由于不确定性和风险带来的负面影响。通常，Robust POMDP使用了不同的方法来处理不确定性，如鲁棒优化（robust optimization）、置信度上限（upper confidence bounds）等。</p>
</li>
<li><p>the UAV backhaul capacity is overloaded</p>
<p>When the UAV backhaul capacity is overloaded, it means that the unmanned aerial vehicle (UAV) is experiencing a bottleneck in the transmission of data from the UAV to the ground station or other remote locations. This can occur when the amount of data being generated by the UAV exceeds the capacity of the wireless communication link that connects the UAV to the ground station.</p>
<p>To resolve this issue, several approaches can be taken:</p>
<ol>
<li>Increase the backhaul capacity: One solution is to increase the capacity of the backhaul link. This can be achieved by using higher bandwidth wireless communication links or using multiple links to create a mesh network. However, increasing the capacity may come at the cost of increased power consumption, weight, and complexity of the UAV.</li>
<li>Prioritize data: Another solution is to prioritize the data being transmitted. Not all data generated by the UAV may be equally important, so prioritizing the critical data can help to reduce the amount of data being transmitted and ease the burden on the backhaul link.</li>
<li>Compression: Compressing the data before transmitting it can also help to reduce the amount of data being transmitted, which can help to reduce the load on the backhaul link.</li>
<li>Adjust the mission: It is also possible to adjust the mission of the UAV to reduce the amount of data being generated. This can be achieved by changing the flight path or using different sensors that generate less data.</li>
</ol>
<p>Overall, the solution will depend on the specific use case and the nature of the data being generated by the UAV.</p>
</li>
<li><p>difference between uplink,downlink and backhaul capacities of UAV</p>
<p>UAV (Unmanned Aerial Vehicles) are often equipped with communication equipment to establish connectivity with ground stations, satellites or other aerial vehicles. The communication capacity of a UAV can be divided into three parts:</p>
<ol>
<li>Uplink capacity: The uplink capacity refers to the data transmission capacity from the ground station or other sources to the UAV. This includes the command and control signals, sensor data, video feeds or any other type of data that needs to be transmitted from the ground to the UAV. The uplink capacity is typically lower than the downlink capacity due to the limitations of the UAV’s communication equipment.</li>
<li>Downlink capacity: The downlink capacity refers to the data transmission capacity from the UAV to the ground station or other destinations. This includes video feeds, sensor data, telemetry information, and any other type of data that needs to be transmitted from the UAV to the ground. The downlink capacity is typically higher than the uplink capacity to ensure that the ground station can receive all the necessary data from the UAV.</li>
<li>Backhaul capacity: The backhaul capacity refers to the data transmission capacity between the UAV and other communication nodes, such as satellites or other UAVs. This capacity is required for long-range communication and to extend the coverage area of the UAV. The backhaul capacity is typically higher than the uplink and downlink capacities, as it needs to support communication over longer distances.</li>
</ol>
<p>The specific uplink, downlink, and backhaul capacities of a UAV depend on several factors, including the communication equipment used, the frequency bands employed, and the communication protocols employed. Generally, the higher the communication capacity, the more data can be transmitted, resulting in a more capable and effective UAV communication system.</p>
</li>
<li><p>为什么在论文中，概率用Pr()来表示?</p>
<p>在论文中，概率通常使用Pr()来表示，这是因为Pr()是probability（概率）的缩写，它是英文单词probability的第一个和最后一个字母的缩写形式。使用Pr()可以使概率符号简短、明了，方便阅读和书写。</p>
<p>另外，使用Pr()也有助于避免与其他符号产生混淆，例如使用小写字母p表示概率，容易与其他符号如p-value（p值）产生歧义。因此，在数学和统计学领域中，Pr()已成为一种通用的表示概率的方法。</p>
</li>
<li><p>什么是渐近分布</p>
<p>渐近分布是指随着样本量的增加，概率分布逐渐趋近于某个极限分布的现象。换句话说，随着样本数量的增加，随机变量的概率分布逐渐稳定并逼近于一个确定的分布。</p>
<p>例如，假设一个随机变量的样本平均值是一个正态分布。当样本量逐渐增加时，这个分布将逐渐趋近于一个确定的正态分布，这个确定的正态分布就是这个随机变量的渐近分布。</p>
<p>渐近分布在统计学中非常重要，因为它可以用来描述一些随机变量在样本量很大时的行为，并且在实际应用中通常需要估计渐近分布来做出一些决策。</p>
</li>
<li><p>Robust POMDP 和 POMDP区别是什么</p>
<p>POMDP（部分可观测马尔可夫决策过程）是一个用于描述不确定性下决策问题的数学框架。在POMDP中，决策制定者面临的环境具有部分可观测性，即决策制定者不能直接观察到环境的完整状态，而只能通过观察到的一些部分信息进行推断。Robust POMDP（鲁棒POMDP）是POMDP的一种扩展形式，用于处理环境模型不确定性和参数不确定性的情况。</p>
<p>区别如下：</p>
<ol>
<li>鲁棒POMDP与POMDP的主要区别在于模型不确定性和参数不确定性的处理方式。鲁棒POMDP旨在处理不确定性模型，即我们不知道环境模型是什么。它使用集合值函数表示模型不确定性，并在决策制定时考虑多种可能的环境模型。而POMDP假设我们已经知道环境的动态模型，并根据已知的环境模型进行决策制定。</li>
<li>鲁棒POMDP中的决策制定者需要考虑多种可能的环境模型，以便在不确定性环境中做出最优决策。这与POMDP不同，因为POMDP中的决策制定者只需要考虑一个已知的环境模型。</li>
<li>在鲁棒POMDP中，策略的性能与环境模型和参数的不确定性密切相关。因此，与POMDP相比，鲁棒POMDP需要更复杂的计算方法和算法来求解最优策略。</li>
<li>鲁棒POMDP是POMDP的一个扩展，它可以处理更广泛的不确定性情况。POMDP只能处理部分可观测性问题，而鲁棒POMDP可以处理不确定性模型和参数不确定性问题。</li>
</ol>
<p>总的来说，鲁棒POMDP是POMDP的一个更广泛的形式，用于处理更复杂的不确定性情况。</p>
</li>
<li><p>扩散模型和GAN可以应用在什么地方，这两个有什么区别呢？</p>
<p>扩散模型和GAN（生成对抗网络）都是机器学习中常用的模型，但应用场景和工作原理有所不同。</p>
<p>扩散模型（Diffusion Models）是一种用于生成样本的概率模型，它的基本思想是对一个样本进行多次采样，每次采样时将当前样本的信息扩散到相邻的样本中。通过多次采样和扩散，扩散模型可以生成高质量的样本，比如图像、语音、文本等。扩散模型在计算机视觉、自然语言处理等领域中得到了广泛应用。</p>
<p>GAN（Generative Adversarial Networks）是一种用于生成样本的神经网络模型，它由两个神经网络组成：生成器（Generator）和判别器（Discriminator）。生成器的作用是生成样本，判别器的作用是判断生成器生成的样本是否真实。通过不断的迭代训练，生成器会逐渐学习到生成真实的样本，判别器也会逐渐学习到判别真实和虚假的样本。GAN 在图像生成、语音合成等领域中得到了广泛应用。</p>
<p>两者的主要区别在于工作原理和应用场景。扩散模型是一种基于概率的模型，它的基本思想是对样本进行多次采样和扩散，生成高质量的样本。GAN 则是一种基于神经网络的模型，它通过生成器和判别器相互博弈的方式来生成高质量的样本。因此，扩散模型适用于需要生成大量高质量样本的场景，而 GAN 更适用于需要生成具有多样性的样本的场景。</p>
</li>
<li><p>马尔科夫博弈</p>
<p>马尔科夫博弈（Markov game）是一种多人博弈模型，它结合了马尔科夫过程和博弈论。在马尔科夫博弈中，多个参与者以随机的方式采取决策，这些决策的结果不仅取决于当前的状态，还取决于上一个状态和之前采取的行动。马尔科夫博弈是一种比博弈论更为复杂的博弈模型，它可以用于模拟一些现实世界中的问题，例如机器人控制、多智能体系统的协调等。</p>
<p>马尔科夫博弈具有以下特点：</p>
<ol>
<li>参与者采取行动的结果取决于当前的状态和上一个状态以及之前的行动；</li>
<li>参与者的行动具有随机性，即它们可以采取不同的行动，并且每个行动有一定的概率实现；</li>
<li>参与者的目标是最大化自己的收益或者最小化自己的成本。</li>
</ol>
<p>在马尔科夫博弈中，通常采用强化学习算法来学习参与者的最佳策略。强化学习算法利用当前的状态和行动信息来更新策略，使得参与者能够逐渐学习到最优的行动策略。</p>
</li>
<li><p>强化学习中的Credit Assignment是什么？请问Credit Assignment解决了什么呢？为什么被提出</p>
<p>强化学习中的Credit Assignment（信用分配）是指在一个智能体完成一个任务时，对于获得的奖励（reward）或惩罚（penalty）的分配方式。在强化学习中，一个智能体与环境进行交互，并根据环境的反馈（奖励或惩罚）来更新其策略以优化其行为。然而，这些奖励或惩罚通常是在时间序列中被分配的，这使得决定每个行动对应的奖励或惩罚变得复杂。</p>
<p>因此，Credit Assignment可以看作是一个问题：如何将智能体的行为与获得的奖励或惩罚联系起来。这个问题的解决方式有很多，包括基于价值函数的Credit Assignment、基于Temporal-Difference（TD）方法的Credit Assignment以及基于Actor-Critic框架的Credit Assignment等。这些方法可以帮助智能体更好地理解哪些行动是对任务有帮助的，并对其进行加强或弱化，从而改进其策略。</p>
<p>Credit Assignment解决了强化学习中的一些重要问题。</p>
<p>首先，强化学习的目标是通过最大化期望累积回报来寻找最优策略。然而，这个过程是一个序列决策问题，即当前的决策会影响未来的奖励，而未来的奖励又反过来影响当前的决策。这个问题被称为Credit Assignment问题，因为它需要将奖励或惩罚与先前的行为联系起来，以决定哪些行为应该受到奖励或惩罚。</p>
<p>其次，Credit Assignment问题也是因为强化学习通常涉及到长时间的延迟奖励。即智能体的某个行为可能会导致在未来的某个时间点获得奖励，而这个时间点可能会很长。这就使得Credit Assignment变得非常困难，因为要确定哪些行为导致了未来的奖励或惩罚需要考虑许多因素，如环境的变化、智能体的行为选择等。</p>
<p>因此，Credit Assignment的提出旨在解决强化学习中的信用分配问题，从而提高智能体的决策质量和性能。Credit Assignment可以帮助智能体更好地理解哪些行为是有用的，哪些行为是无用的，并针对性地对行为进行加强或弱化，从而提高其策略的质量和效果。</p>
</li>
<li><p><strong>bias-variance tradeoff</strong></p>
<p><strong>Bias</strong>：模型预测值跟训练集的真实值之间的差异。一般可以理解为在训练集上的表现，表现很好，那就是bias很小，表现很差，那就是bias很大。</p>
<p><strong>Variance</strong>：模型预测值跟总体之间的差异，一般可以理解为训练集和验证集模型评价指标的差异，比如在训练集上模型表现很好，但是在验证集上表现很差，说明样本发生比较小的变化，预测值会发生比较大的变化，也就是方差比较大。</p>
<blockquote>
<p>帮助记忆的小tip：小的方差&amp;大的误差 —-&gt; 欠拟合，大的方差&amp;小的误差 —-&gt; 过拟合</p>
</blockquote>
<p>Bias-variance trade-off：</p>
<p>理想的情况下，我们想要我们的模型低bias和低的variance，然而，实际工作中，这个是非常难的，所以trade-off可以帮助我们选择合适的模型。</p>
<p>简单的模型像线性回归和逻辑回归，通常具有较高的bias和较低的variance，复杂的模型，比如随机森林，会有低的bias和高的variance。</p>
</li>
<li><p>how can we understand “A learning procedure with weak inductive bias will be able to master a wider range of patterns (greater variance), but will in general be less sample-efficient”</p>
<p>In machine learning, the inductive bias of a learning procedure refers to the assumptions or prior knowledge that the algorithm uses to make predictions based on the data it is given. A strong inductive bias means that the algorithm has a lot of prior assumptions about the types of patterns that are likely to be present in the data, whereas a weak inductive bias means that the algorithm has fewer prior assumptions.</p>
<p>When a learning procedure has a weak inductive bias, it is more flexible in the types of patterns it can recognize in the data. This means that it is able to master a wider range of patterns, which gives it greater variance in the types of tasks it can perform. However, because the algorithm has fewer prior assumptions, it may require more data to learn from in order to accurately recognize the patterns in the data. This makes it less sample-efficient, meaning that it needs more training examples to achieve the same level of accuracy as an algorithm with a stronger inductive bias.</p>
<p>On the other hand, a learning procedure with a strong inductive bias will be more specialized in the types of patterns it can recognize, which can make it more sample-efficient. However, it may not be as effective at recognizing patterns outside of its prior assumptions, which limits its ability to generalize to new and unseen data.</p>
<p>In summary, a learning procedure with a weak inductive bias has greater variance and can recognize a wider range of patterns, but may require more training examples to achieve the same level of accuracy as an algorithm with a stronger inductive bias.</p>
</li>
<li><p>怎样理解  weak inductive bias</p>
<p>在机器学习中，归纳偏差（inductive bias）是指算法对数据的先验假设或先验知识，这些先验假设可以影响算法对数据的解释和预测。假设归纳偏差越强，算法就越倾向于使用更特定的假设和更少的参数来解释和预测数据。相反，如果归纳偏差较弱，算法就会更倾向于使用更通用的假设和更多的参数，以尝试解释和预测更广泛的数据。</p>
<p>因此，弱归纳偏差的算法通常更加灵活，能够更好地适应不同类型的数据，但在学习新的数据时可能需要更多的训练示例。这是因为算法没有太多的先验知识可以利用，需要通过大量的训练示例来适应新的数据。相反，强归纳偏差的算法通常更加专业化，只适用于特定类型的数据，但可能会更加高效，因为它们可以从较少的训练示例中学习并达到较高的准确度。</p>
<p>因此，“weak inductive bias”可以理解为算法在学习数据时使用的先验假设比较少，对不同类型的数据具有更强的适应性和灵活性，但可能需要更多的训练数据来学习。</p>
<blockquote>
<p><strong>wiki：</strong></p>
<p><strong>归纳偏置</strong>（英语：Inductive bias），指的是学习<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/演算法">算法</a>中，当学习器去预测其未遇到过的输入结果时，所做的一些假设的集合（Mitchell, 1980）。</p>
<p><a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/机器学习">机器学习</a>试图去建造一个可以学习的算法，用来预测某个目标的结果。要达到此目的，要给于学习算法一些训练样本，样本说明输入与输出之间的预期关系。然后假设学习器在预测中逼近正确的结果，其中包括在训练中未出现的<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/样本">样本</a>。既然未知状况可以是任意的结果，若没有其它额外的假设，这任务就无法解决。这种关于目标函数的必要假设就称为<em>归纳偏置</em>（Mitchell, 1980; desJardins and Gordon, 1995）。</p>
<p>一个典型的归纳偏置例子是<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/奧卡姆剃刀">奥卡姆剃刀</a>，它假设最简单而又一致的假设是最佳的。这里的一致是指学习器的假设会对所有样本产生正确的结果。</p>
<p>归纳偏置比较正式的定义是基于数学上的<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/邏輯">逻辑</a>。这里，归纳偏置是一个与训练样本一起的逻辑式子，其逻辑上会蕴涵学习器所产生的假设。然而在实际应用中，这种严谨形式常常无法适用。在有些情况下，学习器的归纳偏置可能只是一个很粗糙的描述（如在<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/人工神经网络">人工神经网络</a>中），甚至更加简单。</p>
<p>之所以对这个概念难以理解, 是因为这个名词太没有烟火气, 其实他就是说的是<strong>模型的指导规则</strong>, 可以同时应用于训练和预测的时候的东西, 是一种超参数(因为是模型的一部分).上面举的例子是线性回归, 还有一种是决策树, 决策树的假设就是,</p>
<ul>
<li><ul>
<li>优先选择较短的树而不是较长的。</li>
<li>选择那些信息增益高的属性里根节点较近的树。这里利用了两个假设. 相比之下, 神经网络的假设是相当弱的, 举个例子:分类神经网络模型 : 将输入通过非线性函数进行映射的结果,正确的类别具有较高的softmax值.</li>
</ul>
</li>
<li><p>归纳偏置的种类</p>
</li>
</ul>
<p>以下是机器学习中常见的归纳偏置列表：</p>
<ul>
<li><strong>最大<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/条件独立">条件独立性</a></strong>（conditional independence）：如果假说能转成<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/贝叶斯概率">贝叶斯模型</a>架构，则试着使用最大化条件独立性。这是用于<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/朴素贝叶斯分类器">朴素贝叶斯分类器</a>（Naive Bayes classifier）的偏置。</li>
<li><strong>最小<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/交叉驗證">交叉验证</a>误差</strong>：当试图在假说中做选择时，挑选那个具有最低交叉验证误差的假说，虽然交叉验证看起来可能无关偏置，但<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/w/index.php?title=天下没有免费的午餐&amp;action=edit&amp;redlink=1">天下没有免费的午餐</a>理论显示交叉验证已是偏置的。</li>
<li><strong>最大边界</strong>：当要在两个类别间画一道分界线时，试图去最大化边界的宽度。这是用于<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/支持向量机">支持向量机</a>的偏置。这个假设是不同的类别是由宽界线来区分。</li>
<li><strong><a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/最小描述長度">最小描述长度</a></strong>（Minimum description length）：当构成一个假设时，试图去最小化其假设的描述长度。假设越简单，越可能为真的。见<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/奧卡姆剃刀">奥卡姆剃刀</a>。</li>
<li><strong>最少特征数</strong>（Minimum features）：除非有充分的证据显示一个特征是有效用的，否则它应当被删除。这是<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/特征选择">特征选择</a>（feature selection）算法背后所使用的假设。</li>
<li><strong>最近邻居</strong>：假设在<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/特征空间">特征空间</a>（feature space）中一小区域内大部分的样本是同属一类。给一个未知类别的样本，猜测它与它最紧接的大部分邻居是同属一类。这是用于<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/最近鄰居法">最近邻居法</a>的偏置。这个假设是相近的样本应倾向同属于一类别。</li>
</ul>
</blockquote>
</li>
<li><p><strong>奥卡姆剃刀</strong>（英语：Ockham’s Razor、<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/拉丁語">拉丁语</a>：Lex Parsimoniae，意为“简约法则”）是由14世纪<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/方济会">方济会</a><a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/修士">修士</a><a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/奥卡姆的威廉">奥卡姆的威廉</a>（William of Occam，约1287年至1347年，<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/英格兰">英格兰</a><a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/萨里郡">萨里郡</a><a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/奧坎_(薩里郡">奥卡姆</a>) (Ockham)人氏）提出的<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/邏輯學">逻辑学</a>法则。如果关于同一个问题有许多种<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/理论">理论</a>，每一种都能作出同样准确的<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/预言">预言</a>，那么应该挑选其中使用假定最少的。尽管越复杂的方法通常能做出越好的预言，但是在不考虑预言能力（即结果大致相同）的情况下，假设越少越好。</p>
<p><a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/所罗门诺夫的归纳推理理论">所罗门诺夫的归纳推理理论</a>是奥卡姆剃刀的<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/数学公式">数学公式</a>化：<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/奥卡姆剃刀#cite_note-ReferenceA-1">[1]</a><a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/奥卡姆剃刀#cite_note-Foundationsof-2">[2]</a><a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/奥卡姆剃刀#cite_note-ReferenceB-3">[3]</a><a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/奥卡姆剃刀#cite_note-dsi.uclm.es-4">[4]</a><a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/奥卡姆剃刀#cite_note-5">[5]</a><a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/奥卡姆剃刀#cite_note-6">[6]</a>在所有能够完美描述已有观测的可<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/计算理论">计算理论</a>中，较短的可计算理论在估计下一次观测结果的<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/概率">概率</a>时具有较大<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/权重">权重</a>。</p>
</li>
<li><p>假设空间</p>
<p>假设空间（Hypothesis space）是指学习算法可以选择的所有假设的集合。假设空间可以是任何形式的，它取决于算法和问题的性质。通常情况下，假设空间包括一个或多个参数，并且每个假设对应于一组参数的值。</p>
<p>假设空间可以是可行解的集合，但这并不一定是必须的。例如，在一些问题中，假设空间是所有可能的决策树，而决策树并不一定是可行解的集合。另一个例子是神经网络，它的假设空间通常是由许多层次和节点组成的复杂函数空间，其中每个假设都对应于网络中的一组参数。</p>
<p>因此，假设空间并不一定是可行解的集合，它可以是任何形式的，只要它包含算法可以选择的所有假设即可。</p>
<p>假设的意思：</p>
<p>在我的回答中，我解释了假设空间是指学习算法可以选择的所有假设的集合，它可以是任何形式的，取决于算法和问题的性质。我还提到了一些例子来说明假设空间的不同形式。</p>
<p>我的回答中提到的”假设”指的是在学习算法中，模型对输入数据的输出的预测，也就是假设或者猜测的结果。在机器学习中，我们使用许多不同的模型或算法来拟合训练数据，这些模型或算法可以生成许多不同的假设，从而形成假设空间。</p>
<p>因此，我的回答中的“假设”指的是学习算法中的假设或预测结果，而不是指某个具体的问题或数据集中的假设或解。</p>
<p>可以理解为模型输出的解：</p>
<p>的，你理解得很对。在机器学习中，假设通常是指模型对输入数据的输出的预测或猜测，也可以称之为模型的预测或输出。在训练过程中，学习算法将许多假设或模型应用于训练数据，并尝试找到最适合数据的模型或假设。在测试或使用模型进行预测时，模型将使用其假设或模型输出对输入数据进行预测或解释。因此，假设可以被视为模型输出的解释或解。</p>
</li>
<li><p>什么是表征？什么是内部表征？</p>
<p>在计算机科学中，表征（Representation）指的是一种将信息映射到某种形式的方式，以便于计算机程序进行处理和分析。在人工智能和机器学习中，表征通常用于描述数据的特征，以便于进行分类、聚类、预测等任务。</p>
<p>内部表征（Internal Representation）是指在计算机或人工智能系统中，数据被转换成为某种内部表示形式，以便于程序进行处理和分析。这种转换通常发生在输入数据被送入到神经网络等机器学习模型中时。内部表征可以是数字、向量或矩阵等形式，通常被认为是一种高度优化的、计算机友好的形式，以便于进行各种算法和运算。内部表征的选择对机器学习的性能和效率都具有重要的影响。</p>
</li>
<li><p>Episodic Deep RL目前还流行吗</p>
<p>Episodic Deep RL（Episodic Deep Reinforcement Learning）是一种深度强化学习算法，旨在解决连续任务的学习问题。它在过去几年中一直是深度强化学习领域的研究热点之一。但是，目前是否仍然流行取决于不同的应用场景和问题。</p>
<p>在某些特定的连续控制任务中，Episodic Deep RL仍然是一个有效的解决方案，因为它能够学习从过去的经验中汲取有用的信息，并利用这些信息优化策略。此外，一些最新的研究表明，在某些复杂的任务中，Episodic Deep RL仍然是一个有前途的方向，有望解决现有深度强化学习算法存在的一些问题，比如学习效率和泛化性能等。</p>
<p>总之，Episodic Deep RL目前仍然是深度强化学习领域的研究热点之一，但其是否适用于特定的应用场景和问题需要具体分析。</p>
</li>
<li><p>Episodic Deep RL在哪些算法中有应用？</p>
<p>Episodic Deep RL已经在许多深度强化学习算法中得到了应用，包括：</p>
<ol>
<li>Hindsight Experience Replay (HER)：HER是一种通过将当前状态重置为目标状态并使用当前策略从新状态开始进行训练的技术，以解决稀疏奖励问题。Episodic Deep RL被用于HER算法的设计中，使其能够更好地处理这种类型的问题。</li>
<li>Soft Actor-Critic (SAC)：SAC是一种用于连续动作控制的深度强化学习算法。Episodic Deep RL被用于SAC算法的设计中，使其能够从过去的经验中汲取有用的信息，并利用这些信息优化策略。</li>
<li>Trust Region Policy Optimization (TRPO)：TRPO是一种基于策略梯度的深度强化学习算法，它使用了一个局部的目标优化器来改进策略的学习。Episodic Deep RL被用于TRPO算法的设计中，以解决连续任务的学习问题。</li>
<li>Deep Deterministic Policy Gradient (DDPG)：DDPG是一种用于连续动作控制的深度强化学习算法。Episodic Deep RL被用于DDPG算法的设计中，以帮助解决这种类型的问题。</li>
</ol>
<p>总之，Episodic Deep RL已经在许多深度强化学习算法中得到了广泛的应用，并在解决连续任务学习问题方面取得了一定的进展。</p>
</li>
<li><p>什么是TV-divergence?和KL divergence的区别和联系?</p>
<p>TV（Total Variation）散度是一个用于衡量两个概率分布之间差异的度量，常用于图像处理、计算机视觉、机器学习等领域。</p>
<p>给定两个概率密度函数P和Q，TV散度定义为它们在每个点上差的绝对值的积分的总和。数学上，TV散度可以表示为：</p>
<p><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.691ex;" xmlns="http://www.w3.org/2000/svg" width="35.984ex" height="2.514ex" role="img" focusable="false" viewBox="0 -805.5 15904.8 1111"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g><g data-mml-node="mi" transform="translate(704,0)"><path data-c="1D449" d="M52 648Q52 670 65 683H76Q118 680 181 680Q299 680 320 683H330Q336 677 336 674T334 656Q329 641 325 637H304Q282 635 274 635Q245 630 242 620Q242 618 271 369T301 118L374 235Q447 352 520 471T595 594Q599 601 599 609Q599 633 555 637Q537 637 537 648Q537 649 539 661Q542 675 545 679T558 683Q560 683 570 683T604 682T668 681Q737 681 755 683H762Q769 676 769 672Q769 655 760 640Q757 637 743 637Q730 636 719 635T698 630T682 623T670 615T660 608T652 599T645 592L452 282Q272 -9 266 -16Q263 -18 259 -21L241 -22H234Q216 -22 216 -15Q213 -9 177 305Q139 623 138 626Q133 637 76 637H59Q52 642 52 648Z"></path></g><g data-mml-node="mo" transform="translate(1473,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1862,0)"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mo" transform="translate(2613,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(3057.7,0)"><path data-c="1D444" d="M399 -80Q399 -47 400 -30T402 -11V-7L387 -11Q341 -22 303 -22Q208 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435Q740 255 592 107Q529 47 461 16L444 8V3Q444 2 449 -24T470 -66T516 -82Q551 -82 583 -60T625 -3Q631 11 638 11Q647 11 649 2Q649 -6 639 -34T611 -100T557 -165T481 -194Q399 -194 399 -87V-80ZM636 468Q636 523 621 564T580 625T530 655T477 665Q429 665 379 640Q277 591 215 464T153 216Q153 110 207 59Q231 38 236 38V46Q236 86 269 120T347 155Q372 155 390 144T417 114T429 82T435 55L448 64Q512 108 557 185T619 334T636 468ZM314 18Q362 18 404 39L403 49Q399 104 366 115Q354 117 347 117Q344 117 341 117T337 118Q317 118 296 98T274 52Q274 18 314 18Z"></path></g><g data-mml-node="mo" transform="translate(3848.7,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(4515.4,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mn" transform="translate(5571.2,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(6071.2,0)"><g data-mml-node="mo"><path data-c="2F" d="M423 750Q432 750 438 744T444 730Q444 725 271 248T92 -240Q85 -250 75 -250Q68 -250 62 -245T56 -231Q56 -221 230 257T407 740Q411 750 423 750Z"></path></g></g><g data-mml-node="mn" transform="translate(6571.2,0)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g><g data-mml-node="mo" transform="translate(7293.4,0)"><path data-c="2217" d="M229 286Q216 420 216 436Q216 454 240 464Q241 464 245 464T251 465Q263 464 273 456T283 436Q283 419 277 356T270 286L328 328Q384 369 389 372T399 375Q412 375 423 365T435 338Q435 325 425 315Q420 312 357 282T289 250L355 219L425 184Q434 175 434 161Q434 146 425 136T401 125Q393 125 383 131T328 171L270 213Q283 79 283 63Q283 53 276 44T250 35Q231 35 224 44T216 63Q216 80 222 143T229 213L171 171Q115 130 110 127Q106 124 100 124Q87 124 76 134T64 161Q64 166 64 169T67 175T72 181T81 188T94 195T113 204T138 215T170 230T210 250L74 315Q65 324 65 338Q65 353 74 363T98 374Q106 374 116 368T171 328L229 286Z"></path></g><g data-mml-node="mo" transform="translate(8015.7,0) translate(0 0.5)"><path data-c="222B" d="M113 -244Q113 -246 119 -251T139 -263T167 -269Q186 -269 199 -260Q220 -247 232 -218T251 -133T262 -15T276 155T297 367Q300 390 305 438T314 512T325 580T340 647T361 703T390 751T428 784T479 804Q481 804 488 804T501 805Q552 802 581 769T610 695Q610 669 594 657T561 645Q542 645 527 658T512 694Q512 705 516 714T526 729T538 737T548 742L552 743Q552 745 545 751T525 762T498 768Q475 768 460 756T434 716T418 652T407 559T398 444T387 300T369 133Q349 -38 337 -102T303 -207Q256 -306 169 -306Q119 -306 87 -272T55 -196Q55 -170 71 -158T104 -146Q123 -146 138 -159T153 -195Q153 -206 149 -215T139 -230T127 -238T117 -242L113 -244Z"></path></g><g data-mml-node="mo" transform="translate(8792.3,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="mi" transform="translate(9070.3,0)"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mo" transform="translate(9821.3,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(10210.3,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(10782.3,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(11393.6,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mi" transform="translate(12393.8,0)"><path data-c="1D444" d="M399 -80Q399 -47 400 -30T402 -11V-7L387 -11Q341 -22 303 -22Q208 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435Q740 255 592 107Q529 47 461 16L444 8V3Q444 2 449 -24T470 -66T516 -82Q551 -82 583 -60T625 -3Q631 11 638 11Q647 11 649 2Q649 -6 639 -34T611 -100T557 -165T481 -194Q399 -194 399 -87V-80ZM636 468Q636 523 621 564T580 625T530 655T477 665Q429 665 379 640Q277 591 215 464T153 216Q153 110 207 59Q231 38 236 38V46Q236 86 269 120T347 155Q372 155 390 144T417 114T429 82T435 55L448 64Q512 108 557 185T619 334T636 468ZM314 18Q362 18 404 39L403 49Q399 104 366 115Q354 117 347 117Q344 117 341 117T337 118Q317 118 296 98T274 52Q274 18 314 18Z"></path></g><g data-mml-node="mo" transform="translate(13184.8,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(13573.8,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(14145.8,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(14534.8,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="mi" transform="translate(14812.8,0)"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g><g data-mml-node="mi" transform="translate(15332.8,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g></g></g></svg></mjx-container></p>
<p>其中，<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="13.621ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 6020.4 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo" transform="translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="mi" transform="translate(278,0)"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mo" transform="translate(1029,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1418,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(1990,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(2601.2,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mi" transform="translate(3601.4,0)"><path data-c="1D444" d="M399 -80Q399 -47 400 -30T402 -11V-7L387 -11Q341 -22 303 -22Q208 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435Q740 255 592 107Q529 47 461 16L444 8V3Q444 2 449 -24T470 -66T516 -82Q551 -82 583 -60T625 -3Q631 11 638 11Q647 11 649 2Q649 -6 639 -34T611 -100T557 -165T481 -194Q399 -194 399 -87V-80ZM636 468Q636 523 621 564T580 625T530 655T477 665Q429 665 379 640Q277 591 215 464T153 216Q153 110 207 59Q231 38 236 38V46Q236 86 269 120T347 155Q372 155 390 144T417 114T429 82T435 55L448 64Q512 108 557 185T619 334T636 468ZM314 18Q362 18 404 39L403 49Q399 104 366 115Q354 117 347 117Q344 117 341 117T337 118Q317 118 296 98T274 52Q274 18 314 18Z"></path></g><g data-mml-node="mo" transform="translate(4392.4,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(4781.4,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(5353.4,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(5742.4,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g></g></g></svg></mjx-container>表示在点x处P和Q之间的差异，积分是对所有可能的点x求和。TV散度衡量的是概率分布P和Q之间的距离，它的值越大，表示它们之间的差异越大，反之则表示它们越相似。</p>
</li>
</ol>
<pre><code>KL（Kullback-Leibler）散度和TV（Total Variation）散度都是用于衡量两个概率分布之间的差异的度量，但它们有不同的性质和用途。

KL散度也被称为相对熵，它是用于衡量两个概率分布之间的差异的一种度量方式。KL散度的值越小，表示两个概率分布越相似，越大则表示它们之间的差异越大。KL散度的数学表达式如下：

$KL(P||Q) = ∫P(x)log(P(x)/Q(x))dx$

其中，P和Q分别是两个概率分布，log是自然对数。KL散度的值可以是无限大，且不对称，即$KL(P||Q) ≠ KL(Q||P)$。KL散度通常用于衡量模型预测结果和真实结果之间的差异，以及在信息检索、分类、聚类等领域中的应用。

相比之下，TV散度是一种更直观的距离度量，它的值永远不会是无限大，且是对称的。在图像处理、计算机视觉等领域中，TV散度通常用于图像去噪、图像分割等问题中。

虽然KL散度和TV散度是两种不同的度量方式，但它们之间有一定的联系。事实上，当两个概率分布的TV散度很小时，它们之间的KL散度也会比较小，但反过来并不成立。因此，在某些应用中，可以同时使用KL散度和TV散度来综合考虑两个概率分布之间的差异。
</code></pre><ol>
<li><p>what is dual gradient descent in safey reinforcement learning？</p>
<p>Dual gradient descent is a technique used in safe reinforcement learning to optimize a policy while ensuring that certain safety constraints are satisfied.</p>
<p>In traditional reinforcement learning, the goal is to maximize a reward function by updating the policy parameters in the direction of the gradient of the expected reward. However, in safe reinforcement learning, there may be constraints that need to be satisfied in addition to maximizing the reward.</p>
<p>The dual gradient descent algorithm works by simultaneously optimizing the policy parameters and a set of Lagrange multipliers associated with the safety constraints. The Lagrange multipliers are used to enforce the constraints and are updated in the direction of the gradient of a cost function that penalizes violations of the constraints.</p>
<p>The policy parameters are updated in the direction of the gradient of the expected reward, but with an additional term that is proportional to the Lagrange multipliers and the gradient of the constraint functions. This term ensures that the policy parameters are updated in a way that satisfies the constraints.</p>
<p>By using dual gradient descent, the policy can be optimized to maximize the reward while satisfying the safety constraints, which is important in applications where safety is a critical concern.</p>
</li>
</ol>
<pre><code>Here is the formulation of the dual gradient descent algorithm in safe reinforcement learning:

Suppose we have a policy parameterized by $\theta$, and we want to maximize the expected reward $J(\theta)$ subject to some safety constraints $g_i(\theta) \leq 0$ for $i=1,\ldots,m$. We introduce Lagrange multipliers $\lambda_i \geq 0$ for each constraint and define the Lagrangian:

![image-20230411230118243](https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202304112301285.png)

The dual gradient descent algorithm updates the policy parameters and Lagrange multipliers iteratively as follows:

![image-20230411230141761](https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202304112301795.png)

where $\alpha_\theta$ and $\alpha_\lambda$ are step sizes. The update for $\theta$ is the standard policy gradient update, but with an additional term that takes into account the Lagrange multipliers and the gradients of the constraint functions. The update for $\lambda$ is a projection update that ensures that the Lagrange multipliers remain non-negative.

By updating both the policy parameters and the Lagrange multipliers simultaneously, dual gradient descent can find a policy that maximizes the expected reward while satisfying the safety constraints.



let me provide some more details about each step of the dual gradient descent algorithm:

1. Initialize policy parameters and Lagrange multipliers: We start by initializing the policy parameters $\theta$ and Lagrange multipliers $\lambda$ to some initial values.

2. Evaluate policy: We use the current policy to interact with the environment and collect a set of trajectories. From these trajectories, we compute the expected reward $J(\theta)$ and the constraint violations $g_i(\theta)$.

3. Update Lagrange multipliers: We update the Lagrange multipliers using a projection update:

   ![image-20230411230356679](https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202304112303717.png)

   This update ensures that the Lagrange multipliers remain non-negative.

4. Update policy parameters: We update the policy parameters using the policy gradient method, but with an additional term that takes into account the Lagrange multipliers and the gradients of the constraint functions:

   ![image-20230411230451123](https://raw.githubusercontent.com/RisingAuroras/giteePagesImgs/master/202304112304161.png)

   This update ensures that the policy parameters are updated in a way that satisfies the safety constraints.

5. Repeat: We repeat steps 2-4 until some stopping criterion is met, such as a maximum number of iterations or convergence of the policy.

By using dual gradient descent, we can optimize the policy to maximize the expected reward while satisfying the safety constraints, which is important in applications where safety is a critical concern.
</code></pre><ol>
<li><p><strong>wrap-around effects</strong> from representing angles in degrees or radians</p>
<p>When we represent angles in degrees or radians, we are essentially defining a measure of rotation. However, these two measures differ in how they wrap around, or repeat, as we continue to rotate beyond a full circle.</p>
<p>In degrees, a full circle is defined as 360 degrees. So any angle greater than 360 degrees can be “wrapped around” by subtracting 360 degrees from it. For example, an angle of 400 degrees is equivalent to an angle of 40 degrees (400 - 360 = 40). This is known as the wrap-around effect in degrees.</p>
<p>In radians, a full circle is defined as 2π radians, where π is approximately equal to 3.14159. So any angle greater than 2π radians can be “wrapped around” by subtracting 2π radians from it. For example, an angle of 7π/3 radians is equivalent to an angle of π/3 radians (7π/3 - 2π = π/3). This is known as the wrap-around effect in radians.</p>
<p>It’s important to be aware of these wrap-around effects when working with angles in calculations or algorithms, as they can affect the final results. It’s often useful to normalize angles to a standard range, such as between 0 and 2π radians, or between -π and π radians, to avoid unexpected wrap-around effects.</p>
<p>For example,</p>
<p>Let’s say we have an angle of 350 degrees, and we want to find the corresponding values of sin and cos for that angle. Using the wrap-around effect in degrees, we can “wrap around” the angle by subtracting 360 degrees to get an equivalent angle between 0 and 360 degrees:</p>
<p>350 degrees - 360 degrees = -10 degrees</p>
<p>However, the sine and cosine functions typically take input angles in radians, not degrees. So we need to convert -10 degrees to radians by multiplying by π/180:</p>
<p>-10 degrees * π/180 = -0.1745 radians</p>
<p>Now we can use the sine and cosine functions to find the values of sin(-0.1745) and cos(-0.1745). But because of the wrap-around effect in radians, we may get unexpected results.</p>
<p>For example, if we use a calculator to find sin(-0.1745), we might get a value of -0.1745, which is not what we expect for the sine of an angle between 0 and 2π radians. This is because -0.1745 radians is equivalent to an angle of 2π - 0.1745 radians, which is approximately 6.108 radians. Similarly, the cosine of -0.1745 radians might give us a value of 0.9848, which is equivalent to the cosine of an angle of 2π + 0.1745 radians, or approximately 6.456 radians.</p>
<p>To avoid these unexpected results, we can represent angles using their sine and cosine values instead. For example, instead of representing the angle -10 degrees, we can represent it as the point (-0.1736, 0.9848) on the unit circle, where the x-coordinate is the cosine value and the y-coordinate is the sine value. This representation is independent of the wrap-around effect in degrees or radians, and allows us to perform calculations and transformations on angles without encountering unexpected wrap-around effects.</p>
</li>
<li></li>
</ol>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="https://mr-maktoub.github.io">马克图布</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://mr-maktoub.github.io/posts/e9ae/">https://mr-maktoub.github.io/posts/e9ae/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://mr-maktoub.github.io" target="_blank">马克图布</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E8%AE%BA%E6%96%87/">论文</a></div><div class="post_share"><div class="social-share" data-image="/img/avatar1.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/posts/b54a/" title="《统计学习方法》学习笔记"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">《统计学习方法》学习笔记</div></div></a></div><div class="next-post pull-right"><a href="/posts/f446/" title="常用命令"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">常用命令</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/posts/bf25/" title="论文储备知识"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-12-26</div><div class="title">论文储备知识</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/avatar1.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">马克图布</div><div class="author-info__description">记录我的遗忘</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">66</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">22</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">13</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E8%AF%BB%E8%AE%BA%E6%96%87%EF%BC%9F"><span class="toc-number">1.</span> <span class="toc-text">如何读论文？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#RL"><span class="toc-number">2.</span> <span class="toc-text">RL</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-Playing-Atari-with-Deep-Reinforcement-Learning"><span class="toc-number">2.1.</span> <span class="toc-text">1 Playing Atari with Deep Reinforcement Learning</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-CONTINUOUS-CONTROL-WITH-DEEP-REINFORCEMENT-LEARNING"><span class="toc-number">2.2.</span> <span class="toc-text">2 CONTINUOUS CONTROL WITH DEEP REINFORCEMENT LEARNING</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-INTRODUCTION"><span class="toc-number">2.2.1.</span> <span class="toc-text">1 INTRODUCTION</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-BACKGROUND"><span class="toc-number">2.2.2.</span> <span class="toc-text">2 BACKGROUND</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-ALGORITHM"><span class="toc-number">2.2.3.</span> <span class="toc-text">3 ALGORITHM</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-RESULTS"><span class="toc-number">2.2.4.</span> <span class="toc-text">4 RESULTS</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#6-CONCLUSION"><span class="toc-number">2.2.5.</span> <span class="toc-text">6 CONCLUSION</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-Cross-Domain-Adaptive-Transfer-Reinforcement-Learning-Based-on-State-Action-Correspondence"><span class="toc-number">2.3.</span> <span class="toc-text">3 Cross-Domain Adaptive Transfer Reinforcement Learning Based on State-Action Correspondence</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Abstract"><span class="toc-number">2.3.1.</span> <span class="toc-text">Abstract</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-INTRODUCTION-1"><span class="toc-number">2.3.2.</span> <span class="toc-text">1 INTRODUCTION</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-BACKGROUND-1"><span class="toc-number">2.3.3.</span> <span class="toc-text">2 BACKGROUND</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-METHODOLOGY"><span class="toc-number">2.3.4.</span> <span class="toc-text">3 METHODOLOGY</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-EXPERIMENTS"><span class="toc-number">2.3.5.</span> <span class="toc-text">4 EXPERIMENTS</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-Adversarially-Trained-Actor-Critic-for-Offline-Reinforcement-Learning"><span class="toc-number">2.4.</span> <span class="toc-text">4 Adversarially Trained Actor Critic for Offline Reinforcement Learning</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Abstract-1"><span class="toc-number">2.4.1.</span> <span class="toc-text">Abstract</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-Constrained-Variational-Policy-Optimization-for-Safe-Reinforcement-Learning"><span class="toc-number">2.5.</span> <span class="toc-text">5 Constrained Variational Policy Optimization for Safe Reinforcement Learning</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-Multi-Agent-Deep-Reinforcement-Learning-for-Joint-Decoupled-User-Association-and-Trajectory-Design"><span class="toc-number">2.6.</span> <span class="toc-text">6 Multi-Agent Deep Reinforcement Learning for Joint Decoupled User Association and Trajectory Design</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-Reinforcement-Learning-Fast-and-Slow-Review"><span class="toc-number">2.7.</span> <span class="toc-text">7 Reinforcement Learning, Fast and Slow[Review ]</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-Benchmarking-Safe-Exploration-in-Deep-Reinforcement-Learning"><span class="toc-number">2.8.</span> <span class="toc-text">8 Benchmarking Safe Exploration in Deep Reinforcement Learning</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Abstract-2"><span class="toc-number">2.8.1.</span> <span class="toc-text">Abstract</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-Introduction"><span class="toc-number">2.8.2.</span> <span class="toc-text">1 Introduction</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-Related-Work"><span class="toc-number">2.8.3.</span> <span class="toc-text">2 Related Work</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-Safe-Reinforcement-Learning-Via-Constraints"><span class="toc-number">2.8.4.</span> <span class="toc-text">3 Safe Reinforcement Learning Via Constraints</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-Safety-Gym"><span class="toc-number">2.8.5.</span> <span class="toc-text">4 Safety Gym</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#4-1-Safety-Gym-Environment-Builder"><span class="toc-number">2.8.5.1.</span> <span class="toc-text">4.1 Safety Gym Environment-Builder</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#4-2-Safety-Gym-Benchmark-Suite"><span class="toc-number">2.8.5.2.</span> <span class="toc-text">4.2 Safety Gym Benchmark Suite</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-Experiments"><span class="toc-number">2.8.6.</span> <span class="toc-text">5 Experiments</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#5-1-Methods-Evaluation-Protocol"><span class="toc-number">2.8.6.1.</span> <span class="toc-text">5.1 Methods: Evaluation Protocol</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#5-2-Methods-Algorithms"><span class="toc-number">2.8.6.2.</span> <span class="toc-text">5.2 Methods: Algorithms</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#5-3-Results"><span class="toc-number">2.8.6.3.</span> <span class="toc-text">5.3 Results</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#6-Conclusions"><span class="toc-number">2.8.7.</span> <span class="toc-text">6 Conclusions</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#9-Multi-Agent-Constrained-Policy-Optimisation"><span class="toc-number">2.9.</span> <span class="toc-text">9 Multi-Agent Constrained Policy Optimisation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#10-Deep-Reinforcement-Learning-for-Multi-Agent-Systems-A-Review-of-Challenges-Solutions-and-Applications-Review"><span class="toc-number">2.10.</span> <span class="toc-text">10 Deep Reinforcement Learning for Multi-Agent Systems: A Review of Challenges, Solutions and Applications[Review]</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Q-amp-A"><span class="toc-number">3.</span> <span class="toc-text">Q&amp;A</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/3eeb/" title="Hello World">Hello World</a><time datetime="2023-06-01T07:29:35.968Z" title="发表于 2023-06-01 15:29:35">2023-06-01</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/6f25/" title="我的云音乐APP开发课程笔记">我的云音乐APP开发课程笔记</a><time datetime="2023-06-01T07:29:35.962Z" title="发表于 2023-06-01 15:29:35">2023-06-01</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/8462/" title="重新梳理Android权限管理">重新梳理Android权限管理</a><time datetime="2023-06-01T07:29:35.959Z" title="发表于 2023-06-01 15:29:35">2023-06-01</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/9958/" title="《吴恩达机器学习笔记》">《吴恩达机器学习笔记》</a><time datetime="2023-06-01T07:29:35.949Z" title="发表于 2023-06-01 15:29:35">2023-06-01</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/935d/" title="常见不等式、等式与基础理论">常见不等式、等式与基础理论</a><time datetime="2023-06-01T07:29:35.914Z" title="发表于 2023-06-01 15:29:35">2023-06-01</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2023 By 马克图布</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container').forEach(node => {
            if (node.hasAttribute('display')) {
              btf.wrap(node, 'div', { class: 'mathjax-overflow' })
            } else {
              btf.wrap(node, 'span', { class: 'mathjax-overflow' })
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>